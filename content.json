{"pages":[{"title":"404","date":"2019-07-19T08:41:10.000Z","path":"404.html","text":"你来到了没有知识的荒原 :("},{"title":"关于","date":"2019-09-20T00:53:56.238Z","path":"about/index.html","text":"Email : yp_ren@126.com GitHub：https://github.com/yapengren"},{"title":"分类","date":"2019-08-18T03:32:37.474Z","path":"categories/index.html","text":""},{"title":"标签","date":"2019-08-02T05:00:35.984Z","path":"tags/index.html","text":""}],"posts":[{"title":"记录盲盒项目SQL错误的事故","date":"2019-12-22T09:15:33.000Z","path":"wiki/记录盲盒项目SQL错误的事故/","text":"记录盲盒抽奖项目因为SQL错误 导致表中数据被批量修改的问题 上下文需要根据 is_big_prize 是否大奖字段，修改删除状态。 如果is_big_prize = ‘1’ 则修改所有大奖商品delete_status=1 ， 如果 is_big_prize 不等于 1，则修改所有小奖商品delete_status=1 错误mapper.xml&lt;update id=\"deleteBySkuId\" parameterType=\"com.jd.jr.bcs.domain.entity.eggMachine.LdEggmachineProduct\"&gt; UPDATE ld_eggmachine_product &lt;set&gt; modified_user = #&#123;modifiedUser,jdbcType=VARCHAR&#125;, modified_date = #&#123;modifiedDate,jdbcType=TIMESTAMP&#125;, &lt;if test=\"deleteStatus != null\"&gt; delete_status = #&#123;deleteStatus,jdbcType=INTEGER&#125;, &lt;/if&gt; &lt;/set&gt; where delete_status = 0 and sku_id = #&#123;skuId,jdbcType=VARCHAR&#125; and activity_id = #&#123;activityId,jdbcType=INTEGER&#125; &lt;choose&gt; &lt;when test=\"isBigPrize !=null and isBigPrize == 1\"&gt; and is_big_prize = '1' &lt;/when&gt; &lt;otherwise&gt; and is_big_prize is null or is_big_prize != '1' &lt;/otherwise&gt; &lt;/choose&gt;&lt;/update&gt; 解析错误SQLselect *from ld_eggmachine_productwhere delete_status = 1 and sku_id = &apos;5309709&apos; and activity_id = 6 and is_big_prize is null or is_big_prize != &apos;1&apos;; or 导致ld_eggmachine_product表中is_big_prize字段!=&#39;1&#39;的数据都被修改 总结is_big_prize != &#39;1&#39;查询不出为 NULL 的数据 select *from ld_eggmachine_productwhere is_big_prize != &apos;1&apos;; 查询is_big_prize不等于1的正确写法，可以查询出 != ‘1’ 和 != null 的数据 select *from ld_eggmachine_productwhere delete_status = 1 and sku_id = &apos;5309709&apos; and activity_id = 6 and ifnull(is_big_prize, &apos;&apos;) != &apos;1&apos;; MySQL IFNULL()IFNULL() 函数用于判断第一个表达式是否为 NULL，如果为 NULL 则返回第二个参数的值，如果不为 NULL 则返回第一个参数的值 语法: IFNULL(expression, alt_value) 如果第一个参数的表达式 expression 为 NULL，则返回第二个参数的备用值","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"MySQL字段类型介绍","date":"2019-12-14T09:23:12.000Z","path":"wiki/MySQL字段类型介绍/","text":"注意点： 数值类型：注意字段范围的选择。 日期和时间类型：注意 DATETIME 和 TIMESTAMP 的区别。 字符串类型：用的最多的是 CHAR 和 VARCHAR 类型，注意两者的区别，其他类型在互联网业务中不建议使用。 数值类型 类型 大小 范围（有符号） 范围（无符号） 用途 TINYINT 1 字节 (-128，127) (0，255) 小整数值 SMALLINT 2 字节 (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 字节 (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 字节 (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 字节 (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 字节 (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度浮点数值 DOUBLE 8 字节 (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M&gt;D，为M+2否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 日期和时间类型 类型 大小 范围 格式 用途 DATE 3 字节 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 字节 ‘-838:59:59’/‘838:59:59’ HH:MM:SS 时间值或持续时间 YEAR 1 字节 1901/2155 YYYY 年份值 DATETIME 8 字节 1000-01-01 00:00:00/9999-12-31 23:59:59 YYYY-MM-DD HH:MM:SS 混合日期和时间值 TIMESTAMP 4 字节 1970-01-01 00:00:00/2038结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYYMMDD HHMMSS 混合日期和时间值，时间戳 字符串类型 类型 大小 用途 CHAR 0-255字节 定长字符串 VARCHAR 0-65535 字节 变长字符串 TINYBLOB 0-255字节 不超过 255 个字符的二进制字符串 TINYTEXT 0-255字节 短文本字符串 BLOB 0-65 535字节 二进制形式的长文本数据 TEXT 0-65 535字节 长文本数据 MEDIUMBLOB 0-16 777 215字节 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215字节 中等长度文本数据 LONGBLOB 0-4 294 967 295字节 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295字节 极大文本数据 参考 https://www.yuque.com/yinjianwei/vyrvkf/ta5a63 感谢原作者","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"进制转换","date":"2019-12-13T02:49:43.000Z","path":"wiki/进制转换/","text":"二进制转十进制方法：把二进制数按权展开，相加即得十进制数。 十进制转二进制方法：十进制数除2取余法，即十进制数除2，余数为权位上的数，得到的商值继续除2，依此步骤继续向下运算直到商为0为止。 二进制转八进制方法：3位二进制数按权展开相加得到1位八进制数。（注意事项，3位二进制转成八进制是从右到左开始转换，不足时补0）。 八进制转成二进制方法：八进制数通过除2取余法，得到二进制数，对每个八进制为3个二进制，不足时在最左边补零。 二进制转十六进制方法：与二进制转八进制方法近似，八进制是取三合一，十六进制是取四合一。（注意事项，4位二进制转成十六进制是从右到左开始转换，不足时补0）。 十六进制转二进制方法：十六进制数通过除2取余法，得到二进制数，对每个十六进制为4个二进制，不足时在最左边补零。 十进制转八进制或者十六进制方法一：间接法—把十进制转成二进制，然后再由二进制转成八进制或者十六进制。 方法二：直接法—把十进制转八进制或者十六进制按照除8或者16取余，直到商为0为止。 八进制或者十六进制转成十进制方法：把八进制、十六进制数按权展开、相加即得十进制数。 参考 https://www.yuque.com/yinjianwei/vyrvkf/vidbk6 感谢原作者","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"基础","slug":"java/基础","permalink":"https://yapengren.github.io/categories/java/基础/"}]},{"title":"java线程知识点","date":"2019-12-08T06:25:11.000Z","path":"wiki/java线程知识点/","text":"线程的生命周期","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"java8总结","date":"2019-11-24T09:26:28.000Z","path":"wiki/java8总结/","text":"Java8总结/** * Java8常见操作总结 */public class ExampleEmployee &#123; private static List&lt;Employee&gt; employeeList = new ArrayList&lt;&gt;(); static &#123; employeeList.add(Employee.builder().name(\"zhangsan\").salary(3000).office(\"beijing\").build()); employeeList.add(Employee.builder().name(\"lisi\").salary(6000).office(\"shanghai\").build()); employeeList.add(Employee.builder().name(\"wangwu\").salary(20000).office(\"guangzhou\").build()); employeeList.add(Employee.builder().name(\"zhaoliu\").salary(8000).office(\"shenzhen\").build()); employeeList.add(Employee.builder().name(\"qiangqi\").salary(8000).office(\"beijing\").build()); employeeList.add(Employee.builder().name(\"renba\").salary(21000).office(\"beijing\").build()); &#125; public static void main(String[] args) &#123; // employeeList 是否包含 office=\"shanghai\" boolean isMatch = employeeList.stream() .anyMatch(employee -&gt; \"shanghai\".equals(employee.getOffice())); System.out.println(isMatch); System.out.println(\"-------------------------\"); // employeeList 的所有 salary 都大于4000 boolean matched = employeeList.stream() .allMatch(employee -&gt; employee.getSalary() &gt; 4000); System.out.println(matched); System.out.println(\"-------------------------\"); // 工资最高 Optional&lt;Employee&gt; hightSalary = employeeList.stream() .max((e1, e2) -&gt; Integer.compare(e1.getSalary(), e2.getSalary())); System.out.println(hightSalary); System.out.println(\"-------------------------\"); Optional&lt;Employee&gt; hightSalary2 = employeeList.stream() .max(Comparator.comparingInt(Employee::getSalary)); System.out.println(hightSalary2); System.out.println(\"-------------------------\"); // 返回姓名集合 List&lt;String&gt; names = employeeList.stream() .map(Employee::getName) .collect(Collectors.toList()); System.out.println(names); System.out.println(\"-------------------------\"); // List 转 Map Map&lt;String, Employee&gt; employeeMap = employeeList.stream() .collect(Collectors.toMap(Employee::getName, value -&gt; value)); employeeMap.forEach((key, value) -&gt; &#123; System.out.println(key + \" : \" + value); &#125;); System.out.println(\"-------------------------\"); // 统计 office 是 beijing 的个数 long count = employeeList.stream() .filter(employee -&gt; \"beijing\".equals(employee.getOffice())) .count(); System.out.println(count); System.out.println(\"-------------------------\"); // List 转换为 Set Set&lt;String&gt; officeSet = employeeList.stream() .map(Employee::getOffice) .collect(Collectors.toSet()); System.out.println(officeSet); System.out.println(\"-------------------------\"); // 查找 office 是 beijing 的员工 Optional&lt;Employee&gt; allMatchedEmployees = employeeList.stream() .filter(employee -&gt; \"beijing\".equals(employee.getOffice())) .findAny(); System.out.println(allMatchedEmployees); System.out.println(\"-------------------------\"); // 按照工资的降序来列出员工信息 List&lt;Employee&gt; descEmployeeList = employeeList.stream() .sorted((e1, e2) -&gt; Integer.compare(e2.getSalary(), e1.getSalary())) .collect(Collectors.toList()); System.out.println(descEmployeeList); System.out.println(\"-------------------------\"); // 按照名字的升序列出员工信息 List&lt;Employee&gt; sortEmployeeByName = employeeList.stream() .sorted(Comparator.comparing(Employee::getName)) .collect(Collectors.toList()); System.out.println(sortEmployeeByName); System.out.println(\"-------------------------\"); // 获取工资最高的前两名员工信息 List&lt;Employee&gt; top2EmployeeList = employeeList.stream() .sorted((e1, e2) -&gt; Integer.compare(e2.getSalary(), e1.getSalary())) .limit(2) .collect(Collectors.toList()); System.out.println(top2EmployeeList); System.out.println(\"-------------------------\"); // 获取平均工资 OptionalDouble averageSalary = employeeList.stream() .mapToInt(Employee::getSalary) .average(); System.out.println(averageSalary); System.out.println(\"-------------------------\"); // beijing 办公室平均工资 OptionalDouble averageSalaryByOffice = employeeList.stream() .filter(employee -&gt; \"beijing\".equals(employee.getOffice())) .mapToInt(Employee::getSalary) .average(); System.out.println(averageSalaryByOffice); System.out.println(\"-------------------------\"); // Java8 分组操作 Map&lt;String, List&lt;Employee&gt;&gt; groupMap = employeeList.stream() .collect(Collectors.groupingBy(Employee::getOffice)); System.out.println(groupMap); &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"基础","slug":"java/基础","permalink":"https://yapengren.github.io/categories/java/基础/"}]},{"title":"Spring事务","date":"2019-11-02T01:50:11.000Z","path":"wiki/Spring事务/","text":"Spring 事务管理 API 分析Spring 框架中，涉及到事务管理的 API 大约有100个左右，其中最重要的有三个：PlatformTransactionManager、TransactionDefinition、TransactionStatus。 所谓事务管理，其实就是“按照给定的事务规则来执行提交或者回滚操作”。 “按照……来执行提交或者回滚操作”便是用 PlatformTransactionManager 来表示， “给定的事务规则“就是用 TransactionDefinition 表示的， TransactionStatus 用于表示一个运行着的事务的状态。 打一个不恰当的比喻，TransactionDefinition 与 TransactionStatus 的关系就像程序和进程的关系。 PlatformTransactionManagerSpring 并不直接管理事务，而是提供了多种事务管理器，他们将事务管理的职责委托给 Hibernate 或者 JTA 等持久化机制所提供的相关平台框架的事务来实现。 Spring事务管理器的接口是： org.springframework.transaction.PlatformTransactionManager ，通过这个接口，Spring 为各个平台如 JDBC、Hibernate 等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。 PlatformTransactionManager接口代码public interface PlatformTransactionManager &#123; // 根据指定的传播行为，返回当前活动的事务或创建一个新事务 TransactionStatus getTransaction(TransactionDefinition definition) throws TransactionException; // 使用事务目前的状态提交事务 void commit(TransactionStatus status) throws TransactionException; // 对执行的事务进行回滚 void rollback(TransactionStatus status) throws TransactionException;&#125; 比如我们在使用 JDBC 或者 MyBatis 进行数据持久化操作时，xml配置文件通常如下： &lt;!-- 事务管理器 --&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;!-- 数据源 --&gt; &lt;property name=\"dataSource\" ref=\"dataSource\" /&gt;&lt;/bean&gt; TransactionDefinition事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，传参是 TransactionDefinition 类 ，该类包含与事务属性有关的方法。 TransactionDefinition 接口代码public interface TransactionDefinition &#123; // 返回事务的传播行为 int getPropagationBehavior(); // 返回事务的隔离级别，事务管理器根据它来控制另外一个事务可以看到本事务内的哪些数据 int getIsolationLevel(); // 返回事务的超时时间 int getTimeout(); // 返回是否优化为只读事务 boolean isReadOnly(); // 返回事务的名字 String getName();&#125; 为什么接口只提供了获取属性的方法，而没有提供相关设置属性的方法。其实道理很简单，事务属性的设置完全是程序员控制的，因此程序员可以自定义任何设置属性的方法，而且保存属性的字段也没有任何要求。 唯一的要求的是，Spring 进行事务操作的时候，通过调用以上接口提供的方法必须能够返回事务相关的属性取值。 事务属性包含了5个方面： 事务隔离级别隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 定义了五个表示隔离级别的常量： TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。 TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读和不可重复读，因此很少使用该隔离级别。 TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。 TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。即使在多次查询之间有新增的数据满足该查询，这些新增的记录也会被忽略。该级别可以防止脏读和不可重复读。 TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 事务传播行为事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务方法的执行行为。在 TransactionDefinition 定义中包括了如下几个表示传播行为的常量： TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。 TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。 TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。 这里需要指出的是，前面的六种事务传播行为是 Spring 从 EJB 中引入的，他们共享相同的概念。而 PROPAGATION_NESTED 是 Spring 所特有的。以 PROPAGATION_NESTED 启动的事务内嵌于外部事务中（如果存在外部事务的话），此时，内嵌事务并不是一个独立的事务，它依赖于外部事务的存在，只有通过外部的事务提交，才能引起内部事务的提交，嵌套的子事务不能单独提交。如果熟悉 JDBC 中的保存点（SavePoint）的概念，那嵌套事务就很容易理解了，其实嵌套的子事务就是保存点的一个应用，一个事务中可以包括多个保存点，每一个嵌套子事务。另外，外部事务的回滚也会导致嵌套子事务的回滚。 事务超时所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。 事务的只读属性事务的只读属性是指，对事务性资源进行只读操作或者是读写操作。所谓事务性资源就是指那些被事务管理的资源，比如数据源、 JMS 资源，以及自定义的事务性资源等等。如果确定只对事务性资源进行只读操作，那么我们可以将事务标志为只读的，以提高事务处理的性能。在 TransactionDefinition 中以 boolean 类型来表示该事务是否只读。 事务的回滚规则通常情况下，如果在事务中抛出了未检查异常（继承自 RuntimeException 的异常），则默认将回滚事务。如果没有抛出任何异常，或者抛出了已检查异常，则仍然提交事务。这通常也是大多数开发者希望的处理方式，也是 EJB 中的默认处理方式。但是，我们可以根据需要人为控制事务在抛出某些未检查异常时任然提交事务，或者在抛出某些已检查异常时回滚事务。 // 手动回滚事务TransactionAspectSupport.currentTransactionStatus().setRollbackOnly(); TransactionStatusPlatformTransactionManager.getTransaction(…) 方法返回一个 TransactionStatus 对象。返回的TransactionStatus 对象可能代表一个新的或已经存在的事务（如果在当前调用堆栈有一个符合条件的事务）。TransactionStatus 接口提供了一个简单的控制事务执行和查询事务状态的方法。 TransactionStatus 接口代码public interface TransactionStatus extends SavepointManager, Flushable &#123; // 是否是新的事务 boolean isNewTransaction(); // 是否有恢复点 boolean hasSavepoint(); // 设置为只回滚 void setRollbackOnly(); // 是否为只回滚 boolean isRollbackOnly(); // 是否已完成 boolean isCompleted();&#125; 编程式事务管理通过 Spring 提供的事务管理 API，我们可以在代码中灵活控制事务的执行。在底层，Spring 仍然将事务操作委托给底层的持久化框架来执行。 声明式事务管理基于 @Transactional 的声明式事务管理@Transactional 可以作用于接口、接口方法、类以及类方法上。当作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。 Spring 使用 BeanPostProcessor 来处理 Bean 中的标注，因此我们需要在配置文件中作如下声明来激活该后处理 Bean &lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt; transaction-manager 属性的默认值是 transactionManager，如果事务管理器 Bean 的名字即为该值，则可以省略该属性。 虽然 @Transactional 注解可以作用于接口、接口方法、类以及类方法上，但是 Spring 小组建议不要在接口或者接口方法上使用该注解，因为这只有在使用基于接口的代理时它才会生效。另外， @Transactional 注解应该只被应用到 public 方法上，这是由 Spring AOP 的本质决定的。如果你在 protected、private 或者默认可见性的方法上使用 @Transactional 注解，这将被忽略，也不会抛出任何异常。 基于 &lt;tx&gt; 和基于 @Transactional 各有优缺点：基于 &lt;tx&gt; 的方式，其优点是与切点表达式结合，功能强大。利用切点表达式，一个配置可以匹配多个方法； 而基于 @Transactional 的方式必须在每一个需要使用事务的方法或者类上用 @Transactional 标注，尽管可能大多数事务的规则是一致的，但是对 @Transactional 而言，也无法重用，必须逐个指定。另一方面，基于 @Transactional 的方式使用起来非常简单明了。 参考 https://www.ibm.com/developerworks/cn/education/opensource/os-cn-spring-trans/index.html","tags":[{"name":"spring","slug":"spring","permalink":"https://yapengren.github.io/tags/spring/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"spring","slug":"框架/spring","permalink":"https://yapengren.github.io/categories/框架/spring/"}]},{"title":"Redis过期策略","date":"2019-10-28T08:38:14.000Z","path":"wiki/Redis过期策略/","text":"Redis 过期策略Redis 过期策略是：定期删除 + 惰性删除。 所谓定期删除，指的是 Redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。 假设 Redis 里放了 10w 个 key，都设置了过期时间，你每隔几百毫秒，就检查 10w 个 key，那 Redis 基本上就死了，cpu 负载会很高的，消耗在你的检查过期 key 上了。注意，这里可不是每隔 100ms 就遍历所有的设置过期时间的 key，那样就是一场性能上的灾难。实际上 Redis 是每隔 100ms 随机抽取一些 key 来检查和删除的。 但是问题是，定期删除可能会导致很多过期 key 到了时间并没有被删除掉，那咋整呢？所以就是惰性删除了。这就是说，在你获取某个 key 的时候，Redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 但是实际上这还是有问题的，如果定期删除漏掉了很多过期 key，然后你也没及时去查，也就没走惰性删除，此时会怎么样？如果大量过期 key 堆积在内存里，导致 Redis 内存快耗尽了，怎么办？ 答案是：走内存淘汰机制。 内存淘汰机制Redis 内存淘汰机制有以下几个： noeviction 不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。 volatile-lru 尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。 volatile-lfu 尝试淘汰设置了过期时间的 key，按最近的访问频率进行淘汰。 volatile-ttl 跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl 越小越优先被淘汰。 volatile-random 跟上面一样，不过淘汰的 key 是过期 key 集合中随机的 key。 allkeys-lru 区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰（常用） allkeys-lfu 区别于 volatile-lfu，这个策略要淘汰的 key 对象是全体的 key 集合，按最近的访问频率进行淘汰（常用） allkeys-random 跟上面一样，不过淘汰的策略是随机的 key。 volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的 key 进行淘汰。 如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时不必携带过期时间。 如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。 LRU vs LFULFU 的全称是Least Frequently Used，表示按最近的访问频率进行淘汰，它比 LRU 更加精准地表示了一个 key 被访问的热度。 Frequently [/‘friːkw(ə)ntlɪ/] adv.频繁地，屡次地 如果一个 key 长时间不被访问，只是刚刚偶然被用户访问了一下，那么在使用 LRU 算法下它是不容易被淘汰的，因为 LRU 算法认为当前这个 key 是很热的。而 LFU 是需要追踪最近一段时间的访问频率，如果某个 key 只是偶然被访问一次是不足以变得很热的，它需要在近期一段时间内被访问很多次才有机会被认为很热。 LRU算法LinkedHashMap 默认的元素顺序是 put 的顺序，但是如果使用带参数的构造函数，那么 LinkedHashMap 会根据访问顺序来调整内部顺序。 LinkedHashMap 的 get() 方法除了返回元素之外还可以把被访问的元素放到链表的底端，这样一来每次顶端的元素就是 remove 的元素。 LRU代码import java.io.Serializable;import java.util.LinkedHashMap;import java.util.Map;/** * 最近最少使用算法，linkedHashMap实现，主要是针对缓存过期策略实现 * * @author renyapeng */public class LRULinkedHashMap&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; implements Serializable &#123; /** * 缓存数据容量 */ private int capacity = 16; /** * 默认装载因子 */ static final float DEFAULT_LOAD_FACTOR = 0.75F; /** * 带参数构造方法 * * @param initialCapacity 容量 */ public LRULinkedHashMap(int initialCapacity) &#123; /** * @param initialCapacity 容量 * @param DEFAULT_LOAD_FACTOR 默认装载因子 * @param accessOrder 是否使用lru算法 * true：使用（基于访问顺序，get一个元素后，这个元素被加到最后，使用了LRU最近最少被使用的调度算法） * false：不使用（基于插入顺序） */ super(initialCapacity, DEFAULT_LOAD_FACTOR, true); // 传入指定的缓存最大容量 this.capacity = initialCapacity; &#125; /** * 实现LRU的关键方法，如果map里面的元素个数大于了缓存最大容量，则删除链表的顶端元素 * * @param eldest * @return */ @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) &#123; return size() &gt; capacity; &#125;&#125; 参考 《Redis 深度历险：核心原理与应用实践》 作者：钱文品 互联网 Java 工程师进阶知识完全扫盲","tags":[{"name":"Redis","slug":"Redis","permalink":"https://yapengren.github.io/tags/Redis/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://yapengren.github.io/categories/数据库/Redis/"}]},{"title":"Redis安全防范","date":"2019-10-28T08:33:17.000Z","path":"wiki/Redis安全防范/","text":"指令安全Redis 有一些非常危险的指令，这些指令会对 Redis 的稳定以及数据安全造成非常严重的影响。比如 keys 指令会导致 Redis 卡顿，flushdb 和 flushall 会让 Redis 的所有数据全部清空。如何避免人为操作失误导致这些灾难性的后果也是运维人员特别需要注意的风险点之一。 Redis 在配置文件中提供了 rename-command 指令用于将某些危险的指令修改成特别的名称，用来避免人为误操作。比如在配置文件的 security 块增加下面的内容: rename-command keys abckeysabc 如果还想执行 keys 方法，那就不能直接敲 keys 命令了，而需要键入abckeysabc。 如果想完全封杀某条指令，可以将指令 rename 成空串，就无法通过任何字符串指令来执行这条指令了。 rename-command flushall \"\" 端口安全Redis 默认会监听 *:6379，如果当前的服务器主机有外网地址，Redis 的服务将会直接暴露在公网上，任何一个初级黑客使用适当的工具对 IP 地址进行端口扫描就可以探测出来。 Redis 的服务地址一旦可以被外网直接访问，内部的数据就彻底丧失了安全性。高级一点的黑客们可以通过 Redis 执行 Lua 脚本拿到服务器权限，恶意的竞争对手们甚至会直接清空你的 Redis 数据库。 bind 10.100.20.13 所以，运维人员务必在 Redis 的配置文件中指定监听的 IP 地址，避免这样的惨剧发生。更进一步，还可以增加 Redis 的密码访问限制，客户端必须使用 auth 指令传入正确的密码才可以访问 Redis，这样即使地址暴露出去了，普通黑客也无法对 Redis 进行任何指令操作。 requirepass yoursecurepasswordhereplease 密码控制也会影响到从库复制，从库必须在配置文件里使用 masterauth 指令配置相应的密码才可以进行复制操作。 masterauth yoursecurepasswordhereplease","tags":[],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://yapengren.github.io/categories/数据库/Redis/"}]},{"title":"java判断对象中属性值是否全为空","date":"2019-10-24T07:29:46.000Z","path":"wiki/java判断对象中属性值是否全为空/","text":"/** * 判断对象中属性值是否全为空 * * @param object * @return */public static boolean checkObjAllFieldsIsNull(Object object) throws IllegalAccessException &#123; if (null == object) &#123; return true; &#125; for (Field f : object.getClass().getDeclaredFields()) &#123; f.setAccessible(true); if (f.get(object) != null &amp;&amp; StringUtils.isNotBlank(f.get(object).toString())) &#123; return false; &#125; &#125; return true;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"基础","slug":"java/基础","permalink":"https://yapengren.github.io/categories/java/基础/"}]},{"title":"SpringBoot中间件","date":"2019-10-23T06:40:41.000Z","path":"wiki/SpringBoot中间件/","text":"Spring Boot 和 Redis 常用操作spring-boot-starter-data-redisSpring Boot 提供了对 Redis 集成的组件包：spring-boot-starter-data-redis，它依赖于 spring-data-redis 和 lettuce。Spring Boot 1.0 默认使⽤的是 Jedis 客户端，2.0 替换成了 Lettuce，但如果你从 Spring Boot 1.5.X 切换过来，⼏乎感受不⼤差异，这是因为 spring-boot-starter-data-redis 为我们隔离了其中的差异性。 Lettuce：是⼀个可伸缩线程安全的 Redis 客户端，多个线程可以共享同⼀个 RedisConnection，它利⽤优秀 Netty NIO 框架来⾼效地管理多个连接。 Spring Data：是 Spring 框架中的⼀个主要项⽬，⽬的是为了简化构建基于 Spring 框架应⽤的数据访问，包括⾮关系数据库、Map-Reduce 框架、云数据服务等，另外也包含对关系数据库的访问⽀持。 Spring Data Redis：是 Spring Data 项⽬中的⼀个主要模块，实现了对 Redis 客户端 API 的⾼度封装，使对 Redis 的操作更加便捷。 可以⽤以下⽅式来表达它们之间的关系： Lettuce ➡️ Spring Data Redis ➡️ Spring Data ➡️ spring-boot-starter-data-redis 相关配置引入依赖包&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--Lettuce 需要使⽤ commons-pool 2 创建 Redis 连接池--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; application 配置# Redis 数据库索引（默认为 0）spring.redis.database=0# Redis 服务器地址spring.redis.host=localhost# Redis 服务器连接端⼝spring.redis.port=6379# Redis 服务器连接密码（默认为空）spring.redis.password=# 连接池最⼤连接数（使⽤负值表示没有限制） 默认 8spring.redis.lettuce.pool.max-active=8# 连接池最⼤阻塞等待时间（使⽤负值表示没有限制） 默认 -1spring.redis.lettuce.pool.max-wait=-1# 连接池中的最⼤空闲连接 默认 8spring.redis.lettuce.pool.max-idle=8# 连接池中的最⼩空闲连接 默认 0spring.redis.lettuce.pool.min-idle=0 缓存配置在这⾥可以为 Redis 设置⼀些全局配置，⽐如配置主键的⽣产策略 KeyGenerator，如不配置会默认使⽤参数名作为主键。 @Configuration@EnableCaching//@EnableCaching 来开启缓存public class RedisConfig extends CachingConfigurerSupport &#123; @Override @Bean public KeyGenerator keyGenerator() &#123; return new KeyGenerator() &#123; @Override public Object generate(Object target, Method method, Object... params) &#123; StringBuilder sb = new StringBuilder(); sb.append(target.getClass().getName()); sb.append(method.getName()); for (Object obj : params) &#123; sb.append(obj.toString()); &#125; return sb.toString(); &#125; &#125;; &#125;&#125; 测试使用@SpringBootTestpublic class TestRedisTemplate &#123; @Autowired private RedisTemplate redisTemplate; @Test public void testString() &#123; redisTemplate.opsForValue().set(\"name\", \"renyapeng\"); &#125;&#125;","tags":[{"name":"springBoot","slug":"springBoot","permalink":"https://yapengren.github.io/tags/springBoot/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"springBoot","slug":"框架/springBoot","permalink":"https://yapengren.github.io/categories/框架/springBoot/"}]},{"title":"mac系统快捷键","date":"2019-10-05T02:58:45.000Z","path":"wiki/mac系统快捷键/","text":"快捷键^ A Home ^ E End ^ H 删除 ^ D Delete ^ K 删除到结尾 ^ P Previous⬆️ ^ B ^ F Backward⬅️ Forward➡️ ^ N Next⬇️ Mac 查看端口占用及杀死进程$ lsof -i:5601COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnode 29152 renyapeng 23u IPv4 0x5ff129ce84c9eb1d 0t0 TCP localhost:esmagent (LISTEN)$ kill -9 29152[2] + 29152 killed kibana","tags":[{"name":"mac","slug":"mac","permalink":"https://yapengren.github.io/tags/mac/"}],"categories":[{"name":"系统","slug":"系统","permalink":"https://yapengren.github.io/categories/系统/"},{"name":"mac","slug":"系统/mac","permalink":"https://yapengren.github.io/categories/系统/mac/"}]},{"title":"vim","date":"2019-09-22T05:57:38.000Z","path":"wiki/vim/","text":"vim工作模式 vim键盘图 vim使用案例特殊字符转义/ -&gt; \\/\\ -&gt; \\\\n 在每行行首添加相同的内容:%s/^/要添加的内容 在每行行尾添加相同的内容:%s/$/要添加的内容 删除包含指定字符的行: g/指定字符/d例如，需要删除文件中，含有字符串“content-length”的行：: g/content-length/d","tags":[{"name":"linux","slug":"linux","permalink":"https://yapengren.github.io/tags/linux/"}],"categories":[{"name":"系统","slug":"系统","permalink":"https://yapengren.github.io/categories/系统/"},{"name":"linux","slug":"系统/linux","permalink":"https://yapengren.github.io/categories/系统/linux/"}]},{"title":"linux基本命令","date":"2019-09-22T05:57:36.000Z","path":"wiki/linux基本命令/","text":"前言可以使用 man 命令来查看各个命令的使用文档，如：man cp 文件基本属性Linux中我们可以使用 ll 或者 ls –l 命令来显示一个文件的属性以及文件所属的用户和组 $ lltotal 2184-rw-r--r-- 1 renyapeng staff 846K 9 22 16:08 db.jsondrwxr-xr-x 315 renyapeng staff 9.8K 8 12 14:07 node_modulesdrwxr-xr-x 21 renyapeng staff 672B 9 22 15:28 public 每个文件的属性由左边第一部分的 10 个字符来确定（如下图） 10位字符表示： 0位：确定文件类型 1-3位：确定该文件的所有者对文件的权限 owner 4-6位：确定所有者的同组用户拥有该文件的权限 group 7-9位：确定其他用户拥有该文件的权限 others 第一个字符：代表这个文件的类型，是目录、文件，还是一个链接等等 [ d ] 目录 [ - ] 文件 [ l ] 链接文档（link file） [ b ] 可供储存的接口设备(可随机存取装置) [ c ] 串行端口设备，例如键盘、鼠标（一次性读取装置） 接下来的字符：以三个一组分成三组，用 r、w、x 三个参数的组合表示，位置不会改变 [ r ] 代表可读（read） [ w ] 代表可写（write） [ x ] 代表可执行（execute） [ - ] 没有权限 文件与目录管理mkdir 创建目录mkdir (make directory) 语法： mkdir [-p] 目录名称 参数： -m ：配置文件的权限-p ：将所需要的目录（包含上一级目录）递归创建起来 实例： $ mkdir test // 创建新目录$ mkdir test1/test2/test3/test4mkdir: test1/test2/test3: No such file or directory // 不能创建多层目录$ mkdir -p test1/test2/test3/test4 // 添加-p参数，可以创建多层目录 cp 复制文件或文件夹语法： cp [-adfilprsu] 来源档（source）目标档（destination） 参数： -a ：相当于 -pdr 的意思，至于 pdr 请参考下列说明（常用）-d ：若来源档为连结档的属性（link file），则复制连结档属性而非文件本身-f ：为强制（force）的意思，若目标文件已经存在且无法开启，则移除后再尝试一次-i ：若目标档（destination）已经存在时，在覆盖时会先询问动作的进行（常用）-l ：进行硬式连结（hard link）的连结档创建，而非复制文件本身-p ：连同文件的属性一起复制过去，而非使用默认属性（备份常用）-r ：递归持续复制，用于目录的复制行为（常用）-s ：复制成为符号连结档（symbolic link），亦即「捷径」文件-u ：若 destination 比 source 旧才升级 destination 实例： $ cp -r test1 bashrc // 复制文件夹并重命名 rm 移除文件或目录rm (remove) 语法： rm [-fir] 文件或目录 参数： -f ：就是 force 的意思，忽略不存在的文件，不会出现警告信息-i ：互动模式，在删除前会询问使用者是否动作-r ：递归删除啊！最常用在目录的删除了！这是非常危险的选项！！！ 实例： $ rm -ir test1 // 删除之前询问使用者是否动作examine files in directory test1? y mv 移动文件与目录或重命名语法： mv [-fiu] source destinationmv [options] source1 source2 source3 .... directory 参数： -f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖-i ：若目标文件（destination）已经存在时，就会询问是否覆盖-u ：若目标文件已经存在，且 source 比较新，才会升级（update） 实例： $ mv test1 test2 touch 新建文件语法： touch 实例： $ touch file // 创建一个名为“file”的新空白文件 文件内容查看Linux 系统中使用以下命令来查看文件的内容： cat 由第一行开始显示文件内容 tac 从最后一行开始显示，可以看出 tac 是 cat 的倒着写 nl 显示的时候，顺道输出行号 more 一页一页的显示文件内容 less 与 more 类似，但是可以往前翻页 head 只看头几行 tail 只看尾几行 系统管理psprocess kill语法： kill [-s &lt;信息名称或编号&gt;][程序] 或 kill [-l &lt;信息编号&gt;] 参数： 实例： $ kill 12345 // 杀死进程$ kill -KILL 123456 // 强制杀死进程$ kill -9 123456 // 彻底杀死进程 killallnohup 不挂断进程nohup (no hang up) 如果你正在运行一个进程，而且你觉得在退出帐户时该进程还不会结束，那么可以使用nohup命令。该命令可以在你退出帐户/关闭终端之后继续运行相应的进程。 在缺省情况下该作业的所有输出都被重定向到一个名为nohup.out的文件中。 实例： $ nohup java -jar ~/software/rocketmq-console-ng-1.0.1.jar &amp; // 启动 java jar$ nohup sh ~/software/rocketmq-4.6.0/bin/mqbroker -n localhost:9876 &amp; // 启动rocketmq bocker 文件管理filefind搜索指定文件|文件夹 find 路径 -name 名称 删除空文件|空文件夹 find 路径 -type d -empty|xargs -n 1 rm -rf 文档编辑grep备份压缩tar 参考 https://www.runoob.com/linux/linux-tutorial.html","tags":[{"name":"linux","slug":"linux","permalink":"https://yapengren.github.io/tags/linux/"}],"categories":[{"name":"系统","slug":"系统","permalink":"https://yapengren.github.io/categories/系统/"},{"name":"linux","slug":"系统/linux","permalink":"https://yapengren.github.io/categories/系统/linux/"}]},{"title":"java.util.ConcurrentModificationException 异常详解","date":"2019-09-20T02:11:57.000Z","path":"wiki/java-util-ConcurrentModificationException-异常详解/","text":"环境：JDK 1.8.0 使用 iterator 遍历集合的同时对集合进行修改会报错 java.util.ConcurrentModificationException 问题复现抛异常代码 @Testpublic void test02() &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 1; i &lt; 11; i++) &#123; list.add(i); &#125; Iterator&lt;Integer&gt; it = list.iterator(); while (it.hasNext()) &#123; Integer next = it.next(); if (next == 5) &#123; list.remove(next); &#125; &#125;&#125; 异常信息 java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) at java.util.ArrayList$Itr.next(ArrayList.java:859) 原因分析使用 Iterator 遍历 ArrayList， 抛出异常的是 iterator.next()，查看 ArrayList 的 Iterator 实现源码 public E next() &#123; checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i];&#125; 在 next() 方法中有一个 checkForComodification() 方法 final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125; 可以看到 checkForComodification 方法是用来判断集合的修改次数是否合法。 modCount 字段用于记录集合被修改的次数，ArrayList 增删改 (add, remove, set) 时都会自增 modCount 属性。 在创建 Iterator 的时候会将 modCount 赋值给 expectedModCount，同样记录当前集合修改的次数，初始化为集合的 modCount 值。 抛异常代码中 ArrayList 添加了 10 次所以 modCount = 10；创建 Iterator 时候 expectedModCount = 10；遍历到 next == 5 时执行了 list.remove(next)，此时 modCount = 11, expectedModCount = 10; 在执行 next 方法时，判断 modCount != expectedModCount ，导致抛出异常 java.util.ConcurrentModificationException。 为什么要这么做呢？引用一段解释 Iterator 是工作在一个独立的线程中，并且拥有一个 mutex 锁。 Iterator 被创建之后会建立一个指向原来对象的单链索引表，当原来的对象数量发生变化时，这个索引表的内容不会同步改变，所以当索引指针往后移动的时候就找不到要迭代的对象，所以按照 fail-fast 原则 Iterator 会马上抛出 java.util.ConcurrentModificationException 异常。 所以 Iterator 在工作的时候是不允许被迭代的对象被改变的。但你可以使用 Iterator 本身的方法 remove() 来删除对象， Iterator.remove() 方法会在删除当前迭代对象的同时维护索引的一致性。 解决方法使用 Iterator 本身的方法 remove() 来删除对象 正确代码 @Testpublic void test02() &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 1; i &lt; 11; i++) &#123; list.add(i); &#125; Iterator&lt;Integer&gt; it = list.iterator(); while (it.hasNext()) &#123; Integer next = it.next(); if (next == 5) &#123; // 使用 Iterator 本身的方法 remove() 来删除对象 it.remove(); &#125; &#125;&#125; 参考 https://www.cnblogs.com/xujian2014/p/5846128.html https://www.cnblogs.com/snowater/p/8024776.html https://www.iteye.com/blog/lz12366-675016","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"基础","slug":"java/基础","permalink":"https://yapengren.github.io/categories/java/基础/"}]},{"title":"Mybatis foreach 批量操作","date":"2019-09-16T07:10:48.000Z","path":"wiki/Mybatis foreach 批量操作/","text":"mybatis foreach 遍历传递进来的集合mapper 接口List&lt;Employee&gt; getEmpsByConditions(@Param(\"list\") List&lt;Integer&gt; idList); mapper.xml&lt;!-- 注意返回的数据类型是集合中保存的数据类型 Employee--&gt;&lt;select id=\"getEmpsByConditions\" resultType=\"com.jas.mybatis.bean.Employee\"&gt; SELECT * FROM t_employee WHERE id IN &lt;!-- collection：指定要遍历的集合 item:取出当前集合中元素，赋给 item 中的值 separator：遍历出的多个元素之间用什么分隔符分隔开 open：遍历集合前用什么字符进行拼接 close：遍历集合后用什么字符进行拼接 在 foreach 标签中还有一个属性 index， 遍历集合的时候 index 表示的是当前元素的索引，item 对应索引中的值 遍历 map 的时候 index 表示的是当前 map 中的 key，item 是 key 对应的 value --&gt; &lt;foreach collection=\"list\" item=\"item\" separator=\",\" open=\"(\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; foreach 批量插入数据mapper 接口Integer addEmpsByList(@Param(\"list\") List&lt;Employee&gt; list); mapper.xml&lt;insert id=\"addEmpsByList\" parameterType=\"com.mybatis.bean.Employee\"&gt; INSERT INTO t_employee(username, gender, email) VALUES &lt;foreach collection=\"list\" item=\"item\" separator=\",\"&gt; (#&#123;item.username&#125;, #&#123;item.gender&#125;, #&#123;item.email&#125;) &lt;/foreach&gt;&lt;/insert&gt; 执行多条 SQL 批量插入数据mapper.xml&lt;insert id=\"addEmpsByList\" parameterType=\"com.mybatis.bean.Employee\"&gt; &lt;!-- 每插入一条数据就执行一次 SQL，中间用\";\"分隔开 --&gt; &lt;foreach collection=\"list\" item=\"item\" separator=\";\"&gt; INSERT INTO t_employee(username, gender, email) VALUES (#&#123;item.username&#125;, #&#123;item.gender&#125;, #&#123;item.email&#125;) &lt;/foreach&gt;&lt;/insert&gt; MySql 默认的情况下是不支持使用;分隔开多条 SQL 进行执行的，程序会报错org.springframework.jdbc.BadSqlGrammarException，Mybatic 批量操作必须加上参数 &amp;allowMultiQueries=true community.jdbc.url=jdbc:mysql://172.25.28.8:3306/jr_community?createDatabaseIfNotExist=true&amp;amp;characterEncoding=utf-8&amp;amp;useUnicode=true&amp;zeroDateTimeBehavior=convertToNull&amp;allowMultiQueries=truecommunity.jdbc.username=community.jdbc.password= 参数意思是允许多查询","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://yapengren.github.io/tags/Mybatis/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"Mybatis","slug":"框架/Mybatis","permalink":"https://yapengren.github.io/categories/框架/Mybatis/"}]},{"title":"lombok使用方法","date":"2019-09-07T08:24:59.000Z","path":"wiki/lombok使用方法/","text":"@Data注解在类上，提供类所有属性的 getter 和 setter 方法，此外还提供了equals、canEqual、hashCode、toString 方法、无参构造方法； @Accessors(chain = true)注解在类上，使用链式设置属性； @Setter注解在属性上，为单个属性提供 setter 方法、无参构造方法； 注解在类上，为该类所有的属性提供 setter 方法、无参构造方法； @Getter注解在属性上，为单个属性提供 getter 方法、无参构造方法； 注解在类上，为该类所有的属性提供 getter 方法、无参构造方法； @Log4j注解在类上，为类提供一个 属性名为 log 的 log4j 日志对象，提供无参构造方法； @AllArgsConstructor注解在类上，为类提供一个全参构造方法，加了这个注解后，类中不提供无参构造方法； @NoArgsConstructor注解在类上，为类提供一个无参构造方法； @EqualsAndHashCode注解在类上，可以生成 equals、canEqual、hashCode 方法； @NonNull注解在属性上，会自动产生一个关于此参数的非空检查，如果参数为空，则抛出一个空指针异常，也会有一个无参构造方法； @ToString注解在类上，可以生成所有参数的 toString 方法，还会无参构造方法； @Value注解在类上，会生成全参构造方法，getter 方法，此外还提供了equals、hashCode、toString 方法； @Synchronized这个注解用在类方法或者实例方法上，效果和 synchronized 关键字相同，区别在于锁对象不同，对于类方法和实例方法，synchronized 关键字的锁对象分别是类的 class 对象和 this 对象，而 @Synchronized 的锁对象分别是 私有静态 final 对象 lock 和 私有 final 对象 lock，当然，也可以自己指定锁对象，此外也提供无参构造方法；","tags":[{"name":"spring","slug":"spring","permalink":"https://yapengren.github.io/tags/spring/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"spring","slug":"框架/spring","permalink":"https://yapengren.github.io/categories/框架/spring/"}]},{"title":"冒泡排序","date":"2019-09-03T06:34:04.000Z","path":"wiki/冒泡排序/","text":"基本介绍冒泡排序（Bubble Sort）需要重复的走访要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来，使值较大的元素逐渐从前移向后边。 算法描述冒泡排序算法的运作如下： 比较相邻的元素。如果第一个比第二个大，就交换 对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数 针对所有的元素重复以上的步骤，除了最后一个 持续每次对越来越少的元素重复上面的步骤 1 ～ 3，直到没有任何一对数字需要比较 代码实现private static void bubbleSort(int[] arr) &#123; // 临时变量 int temp = 0; // 标识变量，表示是否进行过交换 boolean flag = false; for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; // 如果前面的数比后面的数大，则交换 if (arr[j] &gt; arr[j + 1]) &#123; flag = true; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; // 在一趟排序中，一次交换都没有发生过 if (!flag) &#123; break; &#125; else &#123; flag = false; &#125; &#125;&#125; 参考 http://cmsblogs.com/?p=4697","tags":[{"name":"算法","slug":"算法","permalink":"https://yapengren.github.io/tags/算法/"}],"categories":[{"name":"算法","slug":"算法","permalink":"https://yapengren.github.io/categories/算法/"},{"name":"排序","slug":"算法/排序","permalink":"https://yapengren.github.io/categories/算法/排序/"}]},{"title":"稀疏数组sparsearray","date":"2019-09-01T06:37:41.000Z","path":"wiki/稀疏数组sparsearray/","text":"需求编写的五子棋程序中，有存盘退出和续上盘的功能。 分析问题因为该二维数组的很多值是默认值 0, 因此记录了很多没有意义的数据 稀疏数组基本介绍当一个数组中大部分元素为０，或者为同一个值的数组时，可以使用稀疏数组来保存该数组。 稀疏数组处理方法 记录数组一共有几行几列，有多少个不同的值 把具有不同值的元素的行列及值记录在一个小规模的数组中，从而缩小程序的规模 稀疏数组举例说明 应用实例 使用稀疏数组，来保留类似前面的二维数组(棋盘、地图等等) 把稀疏数组存盘，并且可以重新恢复原来的二维数组数 整体思路分析 原始的二维数组 –&gt; 稀疏数组的思路 遍历原始的二维数组，得到有效数据的个数 sum 根据 sum 可以创建稀疏数组 sparseArr int[sum + 1][3] 将二维数组的有效数据存入到稀疏数组 稀疏数组 –&gt; 原始的二维数组的思路 先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如 int[][] chessArr = new int[11][11] 在读取稀疏数组后几行的数据，并赋给原始的二维数组 代码实现public class SparseArray &#123; public static void main(String[] args) &#123; // 创建原始的二维数组 11 * 11 // 0表示没有棋子，1表示黑棋，2表示蓝棋 int[][] chessArr = new int[11][11]; chessArr[1][2] = 1; chessArr[2][3] = 2; // 输出原始的二维数组 System.out.println(\"原始的二维数组----------\"); for (int[] ints : chessArr) &#123; for (int data : ints) &#123; System.out.printf(\"%d\\t\", data); &#125; System.out.println(); &#125; // 原始的二维数组 --&gt; 稀疏数组的思路 // 1、遍历二维数组，得到非 0 数据的个数 int sum = 0; for (int i = 0; i &lt; 11; i++) &#123; for (int j = 0; j &lt; 11; j++) &#123; if (chessArr[i][j] != 0) &#123; sum++; &#125; &#125; &#125; // 2、创建对应的稀疏数组 int[][] sparseArr = new int[sum + 1][3]; sparseArr[0][0] = 11; sparseArr[0][1] = 11; sparseArr[0][2] = sum; // 3、遍历二维数组，将非 0 数据存入 sparseArr // count 用于记录是第几个非 0 数据 int count = 0; for (int i = 0; i &lt; 11; i++) &#123; for (int j = 0; j &lt; 11; j++) &#123; if (chessArr[i][j] != 0) &#123; count++; sparseArr[count][0] = i; sparseArr[count][1] = j; sparseArr[count][2] = chessArr[i][j]; &#125; &#125; &#125; // 输出稀疏数组 System.out.println(); System.out.println(\"稀疏数组----------\"); for (int i = 0; i &lt; sparseArr.length; i++) &#123; System.out.printf(\"%d\\t%d\\t%d\\t\\n\", sparseArr[i][0], sparseArr[i][1], sparseArr[i][2]); &#125; System.out.println(); // 稀疏数组 --&gt; 原始的二维数组的思路 // 1. 先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如 int[][] chessArr = new int[11][11] int[][] chessArr2 = new int[sparseArr[0][0]][sparseArr[0][1]]; // 2. 在读取稀疏数组后几行的数据，并赋给原始的二维数组 for (int i = 1; i &lt; sparseArr.length; i++) &#123; chessArr2[sparseArr[i][0]][sparseArr[i][1]] = sparseArr[i][2]; &#125; // 输出恢复后的二维数组 System.out.println(); System.out.println(\"恢复后的二维数组----------\"); for (int[] ints : chessArr2) &#123; for (int data : ints) &#123; System.out.printf(\"%d\\t\", data); &#125; System.out.println(); &#125; &#125;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"https://yapengren.github.io/tags/算法/"}],"categories":[{"name":"算法","slug":"算法","permalink":"https://yapengren.github.io/categories/算法/"},{"name":"数组","slug":"算法/数组","permalink":"https://yapengren.github.io/categories/算法/数组/"}]},{"title":"java基础题","date":"2019-09-01T01:38:39.000Z","path":"wiki/java基础题/","text":"自增变量@Testpublic void test01() &#123; int i = 1; i = i++; int j = i++; int k = i + ++i * i++; System.out.println(\"i=\" + i); System.out.println(\"j=\" + j); System.out.println(\"k=\" + k);&#125; 输出结果： i=4j=1k=11 i++：先使用再 +1 ++i：先 +1 再使用 初始化和实例初始化public class Father &#123; private int i = test(); private static int j = method(); static &#123; System.out.println(\"Father 静态代码块\"); &#125; public Father() &#123; System.out.println(\"Father 构造方法\"); &#125; &#123; System.out.println(\"Father 实例代码块\"); &#125; public int test() &#123; System.out.println(\"Father 普通方法\"); return 1; &#125; public static int method() &#123; System.out.println(\"Father 静态方法\"); return 1; &#125;&#125; public class Son extends Father&#123; private int i = test(); private static int j = method(); static &#123; System.out.println(\"Son 静态代码块\"); &#125; public Son() &#123; System.out.println(\"Son 构造方法\"); &#125; &#123; System.out.println(\"Son 实例代码块\"); &#125; @Override public int test() &#123; System.out.println(\"Son 普通方法\"); return 1; &#125; public static int method() &#123; System.out.println(\"Son 静态方法\"); return 1; &#125; public static void main(String[] args) &#123; Son s1 = new Son(); System.out.println(\"-----\"); Son s2 = new Son(); &#125;&#125; 输出结果： Father 静态方法Father 静态代码块Son 静态方法Son 静态代码块Son 普通方法Father 实例代码块Father 构造方法Son 普通方法Son 实例代码块Son 构造方法-----Son 普通方法Father 实例代码块Father 构造方法Son 普通方法Son 实例代码块Son 构造方法 方法的参数传递机制public class Exam4 &#123; public static void main(String[] args) &#123; int i = 1; String str = \"hello\"; Integer num = 200; int[] arr = &#123;1, 2, 3, 4, 5&#125;; MyData my = new MyData(); change(i, str, num, arr, my); System.out.println(\"i= \" + i); System.out.println(\"str= \" + str); System.out.println(\"num= \" + num); System.out.println(\"arr= \" + Arrays.toString(arr)); System.out.println(\"my.a= \" + my.a); &#125; public static void change(int j, String s, Integer n, int[] a, MyData m) &#123; j += 1; s += \"world\"; n += 1; a[0] += 1; m.a += 1; &#125;&#125;class MyData &#123; int a = 10;&#125; 输出结果： i= 1str= hellonum= 200arr= [2, 2, 3, 4, 5]my.a= 11 List 分批代码当 List 集合数据量比较大时，需要对集合分组，分批处理数据 public static void main(String[] args) &#123; // 集合 List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 100; i++) &#123; list.add(i + \"\"); &#125; // 分组长度 int groupSize = 10; int size = list.size(); int count = size % groupSize == 0 ? size / groupSize : size / groupSize + 1; for (int i = 0; i &lt; count; i++) &#123; int fromIndex = i * groupSize; int toIndex = (i + 1) * groupSize &gt; size ? size : (i + 1) * groupSize; List&lt;String&gt; subList = list.subList(fromIndex, toIndex); System.out.println(subList); &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"基础","slug":"java/基础","permalink":"https://yapengren.github.io/categories/java/基础/"}]},{"title":"数据库三范式","date":"2019-08-27T09:58:46.000Z","path":"wiki/数据库三范式/","text":"数据库三范式第一范式(1st NF－列都是不可再分)第一范式的目标是确保每列的原子性:如果每列都是不可再分的最小数据单元（也称为最小的原子单元），则满足第一范式（1NF） 第二范式(2nd NF－每个表只描述一件事情)首先满足第一范式，并且表中非主键列不存在对主键的部分依赖。 第二范式要求每个表只描述一件事情。 第三范式(3rd NF－不存在对非主键列的传递依赖)第三范式定义是，满足第二范式，并且表中的列不存在对非主键列的传递依赖。除了主键订单编号外，顾客姓名依赖于非主键顾客编号。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"数据库事务","date":"2019-08-27T09:20:07.000Z","path":"wiki/数据库事务/","text":"数据库事务概念什么是数据库事务事务（transaction）是指逻辑上对数据的一组操作， 这组操作要么全部成功，要么全部失败，是不可分割的一个工作单位。 数据库事务的 4 个基本性质（ACID）原子性（Atomicity）事务的原子性是指事务是一个不可分割的工作单位，这组操作要么全部成功，要么全部失败。 一致性（Consistency）在事务开始以前，被操作的数据的完整性处于一致性的状态，事务结束后，被操作的数据的完整性也必须处于一致性状态。 拿银行转账来说，一致性要求事务的执行不应改变A、B 两个账户的金额总和。如果没有这种一致性要求，转账过程中就会发生钱无中生有，或者不翼而飞的现象。事务应该把数据库从一个一致性状态转换到另外一个一致性状态。 隔离性（Isolation）事务隔离性要求系统必须保证事务不受其他并发执行的事务的影响，也即要达到这样一种效果：对于任何一对事务T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前已经结束，要么在 T1 完成之后才开始执行。这样，每个事务都感觉不到系统中有其他事务在并发地执行。 持久性（Durability）一个事务一旦成功提交，它对数据库的改变必须是永久的，即便是数据库发生故障也应该不回对其产生任何影响。 数据库事务的 4 种隔离级别Read uncommitted读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。 事例：老板要给程序员发工资，程序员的工资是 3.6 万/月。但是发工资时老板不小心按错了数字，按成 3.9 万/月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了 3 千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成 3.6 万再提交。 分析：实际程序员这个月的工资还是 3.6 万，但是程序员看到的是 3.9 万。他看到的是老板还没提交事务时的数据，这就是脏读。 那怎么解决脏读呢？Read committed！读提交，能解决脏读问题。 Read committed读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。 事例：程序员拿着信用卡去享受生活（卡里当然是只有 3.6 万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有 3.6 万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的… 分析：这就是读提交，若有事务对数据进行更新 UPDATE 操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。 那怎么解决可能的不可重复读问题？Repeatable read ！ Repeatable read重复读，就是在开始读取数据（事务开启）时，不再允许修改操作 事例：程序员拿着信用卡去享受生活（卡里当然是只有 3.6 万），当他埋单时（事务开启，不允许其他事务的 UPDATE 修改操作），收费系统事先检测到他的卡里有 3.6 万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。 分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即 UPDATE 操作。但是可能还会有幻读问题。因为幻读问题对应的是插入 INSERT 操作，而不是 UPDATE 操作。 什么时候会出现幻读？ 事例：程序员某一天去消费，花了 2 千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了 2 千元，就在这个时候，程序员花了 1 万买了一部电脑，即新增 INSERT 了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。 那怎么解决幻读问题？Serializable！ Serializable 序列化Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。 参考 https://blog.csdn.net/mfl0315/article/details/51981792 https://blog.csdn.net/qq_33290787/article/details/51924963","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"MySQL索引","date":"2019-08-25T03:44:17.000Z","path":"wiki/MySQL索引/","text":"索引优化索引失效案例 全值匹配 最佳左前缀法则。如果索引了多列，要遵守最左前缀法则，指的是查询从索引的最左前列开始并且不跳过索引中的列 不在索引列上做任何操作（计算、函数、「自动或者手动」类型转换），会导致索引失效而转向全表扫描 存储引擎不能使用索引中范围条件右边的列 尽量使用覆盖索引（只访问索引的查询「索引列和查询列一致」），减少 select * MySQL 在使用 != 或者 &lt;&gt; 的时候无法使用索引会导致全表扫描 is null，is not null 无法使用索引 like 以通配符开头 (‘%abc…’) MySQL 索引失效会变成全表扫描的操作 字符串不加单引号索引失效 少用 or，用它来连接时会索引失效 小总结假设 index(a, b, c) Where 语句 索引是否被使用 where a = 3 Y，使用到 a where a = 3 and b = 5 Y，使用到 a，b where a = 3 and b = 5 and c = 4 Y，使用到 a，b，c where b = 3 或者 where b = 3 and c = 4 或者 where c = 4 N where a = 3 and c = 5 使用到 a，但是 c 不可以，b 中间断了 where a = 3 and b &gt; 4 and c = 5 使用到 a 和 b，c 不能用在范围之后，b 断了 where a = 3 and b like ‘kk%’ and c = 4 Y，使用到 a，b，c where a = 3 and b like ‘%kk’ and c = 4 Y，只用到 a where a = 3 and b like ‘%kk%’ and c = 4 Y，只用到 a where a = 3 and b like ‘k%kk%’ and c = 4 Y，只用到 a，b，c 练习题# 创建表create table test03( id int primary key not null auto_increment, c1 char(10), c2 char(10), c3 char(10), c4 char(10), c5 char(10));# 插入数据insert into test03(c1,c2,c3,c4,c5)values(&apos;a1&apos;,&apos;a2&apos;,&apos;a3&apos;,&apos;a4&apos;,&apos;a5&apos;);insert into test03(c1,c2,c3,c4,c5)values(&apos;b1&apos;,&apos;b2&apos;,&apos;b3&apos;,&apos;b4&apos;,&apos;b5&apos;);insert into test03(c1,c2,c3,c4,c5)values(&apos;c1&apos;,&apos;c2&apos;,&apos;c3&apos;,&apos;c4&apos;,&apos;c5&apos;);insert into test03(c1,c2,c3,c4,c5)values(&apos;d1&apos;,&apos;d2&apos;,&apos;d3&apos;,&apos;d4&apos;,&apos;d5&apos;);insert into test03(c1,c2,c3,c4,c5)values(&apos;e1&apos;,&apos;e2&apos;,&apos;e3&apos;,&apos;e4&apos;,&apos;e5&apos;);# 创建索引create index idx_test03_c1234 on test03(c1,c2,c3,c4); 问题：我们创建了复合索引idx_test03_c1234 ，根据以下SQL分析索引使用情况？ explain select * from test03 where c1=’a1’ and c2=’a2’ and c3=’a3’ and c4=’a4’;索引全部使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c4=’a4’ and c3=’a3’;索引全部使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c3&gt;’a3’ and c4=’a4’;索引 c1c2c3 使用，c4 未使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c4&gt;’a4’ and c3=’a3’;索引全部使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c4=’a4’ order by c3;索引 c1c2 使用，c3 作用在排序而不是查找，c4 不使用 explain select * from test03 where c1=’a1’ and c2=’a2’ order by c3;索引 c1c2 使用 c3 作用在排序而不是查找，和 5 题一样 explain select * from test03 where c1=’a1’ and c2=’a2’ order by c4;索引 c1c2 使用 ，但出现 Using filesort explain select * from test03 where c1=’a1’ and c5=’a5’ order by c2,c3;索引 c1 使用，但是 c2，c3用于排序，无 filesort explain select * from test03 where c1=’a1’ and c5=’a5’ order by c3,c2;索引 c1 使用，出现 filesort，我们建立索引 1234，它没有按照顺序来，3 2 颠倒了 explain select * from test03 where c1=’a1’ and c2=’a2’ order by c2,c3;索引 c1c2 使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c5=’a5’ order by c2,c3;索引 c1c2 使用，但是 c2、c3 用于排序，无 filesort explain select * from test03 where c1=’a1’ and c2=’a2’ and c5=’a5’ order by c3,c2;本例有常量 c2 的情况，和 8.2 对比，无filesort explain select * from test03 where c1=’a1’ and c4=’a4’ order by c2,c3;索引 c1 使用 explain select * from test03 where c1=’a1’ and c4=’a4’ order by c3,c2;索引 c1 使用, 出现了 Using where; Using temporary; Using filesort InnoDB 聚簇索引和非聚簇索引每个 InnoDB 表都有一个称为 「 聚簇索引 」 的特殊索引，通常情况下，这个聚簇索引就是 「 主键 」 ( primary key ) 。Innodb 使用它存储表中每一行的数据。 如果想要从查询、插入和其它数据库操作中获得最佳性能，那么我们就必须了解 InnoDB 如何使用 「 聚簇索引 」 来优化每个表的最常见检索和 DML 操作方式 当我们在一个 Innodb 表上定义了一个主键，InnoDB 会默认的使用它作为聚簇索引。 使用 InnoDB 存储引擎时，建议为每个表都添加一个主键。如果该表没有一个逻辑唯一且非空列或列集合，那么可以添加一个带有 AUTO_INCREMENT 约束的自增列作为主键，InnoDB 会自动填充该列。 如果某个 InnoDB 表并没有定义主键。那么 InnoDB 会查找第一个 「 唯一索引 」( UNIQUE Index ) ，因为唯一索引的所有键 ( key ) 都是 NOT ，因此可以用来作为聚簇索引。 如果某个 InnoDB 表既没有定义主键，也没有一个合适的唯一索引。InnoDB 会在内部生成一个名为 GEN_CLUST_INDEX 的隐式的聚簇索引 该聚簇索引的键 ( key ) 会包含一个自动为行生成的 ID 值 ( 行号 ) 。 该表中的所有行会按 InnoDB 分配给此类表中的行的 ID 排序。 行 ID 是一个 6 字节的字段，在插入新行时会单调自增。 因此，可以认为物理上的行保存顺序就是该行 ID 排序的排序顺序 聚簇索引如何加快查询速度通过聚簇索引访问行很快，因为索引搜索直接指向包含所有行数据页 ( data page )。 如果表很大，与那种索引页与数据页分离的 MyISAM 存储引擎相比， 聚簇索引体系结构通常可以节省磁盘 I/O 操作。 非聚簇索引和聚簇索引的关系非聚簇索引，通常也称之为 「 二级索引 」 ( Secondary Indexes ) 或 「 辅助索引 」 ，一般是指聚簇索引之外的所有其它的索引。 在 InnoDB 中，每个辅助索引中的每条记录都会包含该行的主键列 ( 也就是聚簇索引的键 ) ，以及为辅助索引指定的列。InnoDB 使用此主键值来搜索聚簇索引中的行。 如果主键很长，那么辅助索引就会占用更多空间，因此使用短主键是有利的，也是我们所推荐的。 聚簇索引和非聚簇索引的区别 首先，我们要认识到聚簇索引和非聚簇索引的划分依据是什么？ 答案就是 InnoDB 会使用聚簇索引来保存数据，而非聚簇索引的目的仅仅是加快查询速度 在第一点认知基础上，我们就可以知道 聚簇索引是唯一的，一个 InnoDB 表只有一个聚簇索引，而且一定会有一个聚簇索引，如果不存在，Innodb 存储引擎会自动添加一个 非聚簇所以可以有多个，而且只能由用户自己添加，InnoDB 默认并不会创建任何非聚簇索引。 非聚簇索引中一定包含了聚簇索引的列值，但反过来却不存在。 因此，使用非聚簇索引查询数据一定会用到聚簇索引，但反过来却不存在。 参考 http://cmsblogs.com/?p=5463","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"SQL语句","date":"2019-08-24T07:41:20.000Z","path":"wiki/SQL语句/","text":"SQL的各种JOIN用法下图展示了 LEFT JOIN、RIGHT JOIN、INNER JOIN、OUTER JOIN 相关的 7 种用法。 Inner JOIN SELECT &lt;select_list&gt; FROM Table_A AINNER JOIN Table_B BON A.Key = B.Key Left JOIN SELECT &lt;select_list&gt;FROM Table_A ALEFT JOIN Table_B BON A.Key = B.Key Right JOIN SELECT &lt;select_list&gt;FROM Table_A ARIGHT JOIN Table_B BON A.Key = B.Key Left Excluding JOIN SELECT &lt;select_list&gt; FROM Table_A ALEFT JOIN Table_B BON A.Key = B.KeyWHERE B.Key IS NULL Right Excluding JOIN SELECT &lt;select_list&gt;FROM Table_A ARIGHT JOIN Table_B BON A.Key = B.KeyWHERE A.Key IS NULL Outer JOIN # oracleSELECT &lt;select_list&gt;FROM Table_A AFULL OUTER JOIN Table_B BON A.Key = B.Key# MySQLSELECT &lt;select_list&gt;FROM Table_A ALEFT JOIN Table_B BON A.Key = B.KeyUNIONSELECT &lt;select_list&gt;FROM Table_A ARIGHT JOIN Table_B BON A.Key = B.Key Outer Excluding JOIN # oracleSELECT &lt;select_list&gt;FROM Table_A AFULL OUTER JOIN Table_B BON A.Key = B.KeyWHERE A.Key IS NULL OR B.Key IS NULL# mysqlSELECT &lt;select_list&gt; FROM Table_A ALEFT JOIN Table_B BON A.Key = B.KeyWHERE B.Key IS NULLUNIONSELECT &lt;select_list&gt;FROM Table_A ARIGHT JOIN Table_B BON A.Key = B.KeyWHERE A.Key IS NULL SQL 执行顺序 参考 https://www.runoob.com/w3cnote/sql-join-image-explain.html","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"Redis zset 实现简单限流","date":"2019-08-23T01:48:40.000Z","path":"wiki/Redis-zset-实现简单限流/","text":"除了控制流量，限流还有一个应用目的是用于控制用户行为，避免垃圾请求。比如在 UGC 社区，用户的发帖、回复、点赞等行为都要严格受控，一般要严格限定某行为在规定时间内允许的次数，超过了次数那就是非法行为。对非法行为，业务必须规定适当的惩处策略。 如何使用 Redis 来实现简单限流策略？首先我们来看一个常见的简单的限流策略。系统要限定用户的某个行为在指定的时间里只能允许发生 N 次，如何使用 Redis 的数据结构来实现这个限流的功能？ 我们先定义这个接口，理解了这个接口的定义，读者就应该能明白我们期望达到的功能。 // 指定用户 user_id 的某个行为 action_key，在特定的时间内 period，只允许发生一定的次数 max_countpublic boolean isActionAllowed(String userId, String actionKey, int period, int maxCount) &#123;&#125;// 调用这个接口，5 分钟之内只能发帖 10 次limiter.isActionAllowed(\"xiaoming\", \"publish\", 5 * 60, 10) 错误方案 将 userId 和 actionKey 拼成 key，在第一次请求时设置 value 为 1，设置过期时间 expire 为特定的时间 period 每次请求的时候获取 value 值，若存在则 incr 自增 1，超过 maxCount 则做限制 问题模拟分析 如上图：5 分钟之内只能发帖 10 次。 11:01 用户发帖 1 次，此时 redis 中存放数据 key 为 userId:actionKey，vlaue 为 1，过期时间 5 分钟； 11:05 用户发帖 8 次，发帖成功； 11:05 之后，key 过期时间到，被移除； 11:06 用户发帖，此时 redis 中 key 不存在，重新存放 key，发帖 8 次，发帖成功； 那么 11:05 -&gt; 11:06 时间段 2 分钟发帖 16 次，没有达到期望的功能； 正确方案这个限流需求中存在一个滑动时间窗口，想想 zset 数据结构的 score 值，是不是可以通过 score 来圈出这个时间窗口来。而且我们只需要保留这个时间窗口，窗口之外的数据都可以砍掉。那这个 zset 的 value 填什么比较合适呢？它只需要保证唯一性即可，用 uuid 会比较浪费空间，那就改用毫秒时间戳吧。 如图所示，用一个 zset 结构记录用户的行为历史，每一个行为都会作为 zset 中的一个 key 保存下来。同一个用户同一种行为用一个 zset 记录。 为节省内存，我们只需要保留时间窗口内的行为记录，同时如果用户是冷用户，滑动时间窗口内的行为是空记录，那么这个 zset 就可以从内存中移除，不再占用空间。 通过统计滑动窗口内的行为数量与阈值 max_count 进行比较就可以得出当前的行为是否允许。用代码表示如下： public class SimpleRateLimiter &#123; private Jedis jedis; public SimpleRateLimiter(Jedis jedis) &#123; this.jedis = jedis; &#125; /** * @param userId 用户 user_id * @param actionKey 某个行为 * @param period 特定的时间内，单位秒 * @param maxCount 最大允许的次数 * @return */ public boolean isActionAllowed(String userId, String actionKey, int period, int maxCount) &#123; String key = String.format(\"hist:%s:%s\", userId, actionKey); // 毫秒时间戳 long nowTs = System.currentTimeMillis(); Pipeline pipe = jedis.pipelined(); // 用了multi，也就是事务，能保证一系列指令的原子顺序执行 pipe.multi(); // 存放数据，value 和 score 都使用毫秒时间戳 pipe.zadd(key, nowTs, \"\" + nowTs); // zremrangebyscore key min max 命令用于移除有序集中，指定分数（score）区间内的所有成员 // 移除时间窗口之前的数据，剩下的都是时间窗口之内的 Response&lt;Long&gt; longResponse = pipe.zremrangeByScore(key, 0, nowTs - period * 1000); // 相当于 count()，获取时间窗口内的行为数量 Response&lt;Long&gt; count = pipe.zcard(key); // 设置 zset 过期时间，避免冷用户持续占用内存 // 过期时间应该等于时间窗口的长度，再多宽限 1s pipe.expire(key, period + 1); pipe.exec(); pipe.close(); // 比较数量是否超标 return count.get() &lt;= maxCount; &#125; public static void main(String[] args) &#123; Jedis jedis = new Jedis(); SimpleRateLimiter limiter = new SimpleRateLimiter(jedis); for (int i = 0; i &lt; 20; i++) &#123; System.out.println(limiter.isActionAllowed(\"xiaoming\", \"publish\", 5 * 60, 10)); &#125; &#125;&#125; 缺点因为它要记录时间窗口内所有的行为记录，如果这个量很大，比如限定 60s 内操作不得超过 100w 次这样的参数，它是不适合做这样的限流的，因为会消耗大量的存储空间。 参考 《Redis 深度历险：核心原理与应用实践》 作者：钱文品","tags":[{"name":"Redis","slug":"Redis","permalink":"https://yapengren.github.io/tags/Redis/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://yapengren.github.io/categories/数据库/Redis/"}]},{"title":"java死锁编程及定位分析","date":"2019-08-21T09:37:44.000Z","path":"wiki/java死锁编程及定位分析/","text":"死锁编程及定位分析是什么死锁是指两个或者两个以上的进程在执行过程中，因抢夺资源而造成的一种互相等待的现象，若无外力干涉它们将都无法推进下去，如果系统资源充足，进程的资源请求都能够得到满足，死锁出现的可能性也就很低，否则就会因争夺有限的资源而陷入死锁。 产生死锁的主要原因 系统资源不足 进程运行推进的顺序不合适 资源分配不当 代码：DeadLockDemopublic class DeadLockDemo &#123; public static void main(String[] args) &#123; String lockA = \"lockA\"; String lockB = \"lockB\"; new Thread(new HoldLockThread(lockA, lockB), \"ThreadAAA\").start(); new Thread(new HoldLockThread(lockB, lockA), \"ThreadBBB\").start(); &#125;&#125;class HoldLockThread implements Runnable &#123; private String lockA; private String lockB; public HoldLockThread(String lockA, String lockB) &#123; this.lockA = lockA; this.lockB = lockB; &#125; @Override public void run() &#123; synchronized (lockA) &#123; System.out.println(Thread.currentThread().getName() + \" 自己持有： \" + lockA + \" 尝试获得： \" + lockB); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (lockB) &#123; System.out.println(Thread.currentThread().getName() + \" 自己持有： \" + lockB + \" 尝试获得： \" + lockA); &#125; &#125; &#125;&#125; 解决 jps 命令定位进程号 jstack 找到死锁查看","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"java公平锁/非公平锁/可重入锁/递归锁/自旋锁","date":"2019-08-20T06:29:56.000Z","path":"wiki/java公平锁-非公平锁-可重入锁-递归锁-自旋锁/","text":"公平锁和非公平锁是什么公平锁：是指多个线程按照申请锁的顺序来获取锁，类似排队打饭，先来后到。 非公平锁：是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。在高并发的情况下，有可能会造成优先级反转或者饥饿现象。 两者区别公平锁/非公平锁：并发包中 ReentrantLock 的创建可以指定构造函数的 boolean 类型来得到公平锁或非公平锁，默认是非公平锁。 关于两者区别： 公平锁，就是很公平，在并发情况下，每个线程在获取锁时会查看此锁维护的等待队列，如果为空，或者当前线程是等待队列的第一个，就占有锁，否则就会加入到等待队列中，以后会按照 FIFO 的规则从队列中取到自己。 非公平锁：非公平锁比较粗鲁，上来就直接尝试占有锁，如果尝试失败，就再采取类似公平锁那种方式。 题外话Java ReentrantLock 而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。 对于 Synchronized 而言，也是一种非公平锁。 可重入锁（又名递归锁）是什么可重入锁（也就是递归锁）：指的是同一个线程外层函数获得锁之后，内层递归函数仍然能获取该锁的代码，在同一线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。 也就是说，线程可以进入任何一个它已经拥有的锁所有同步着的代码块。 ReentrantLock/Synchronized 就是一个典型的可重入锁可重入锁最大的作用是避免死锁独占锁/共享锁独占锁：指该锁一次只能被一个线程所持有。对 ReentrantLock 和 Synchronized 而言都是独占锁。 共享锁：指该锁可被多个线程所持有。 对 ReentrantReadWriteLock，其读锁是共享锁，其写锁是独占锁。读锁的共享锁可保证并发读是非常高效的，读写，写读，写写的过程是互斥的。 自旋锁自旋锁：是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下切换的消耗，缺点是循环会消耗CPU。","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"CopyOnWriteArrayList","date":"2019-08-20T06:19:07.000Z","path":"wiki/CopyOnWriteArrayList/","text":"写时复制CopyOnWrite 容器即写时复制的容器。往一个容器添加元素的时候，不直接往当前容器 Object[] 添加，而是先将当前 object[] 进行 Copy，复制出一个新的容器 Object[] newElements，然后新的容器 Object[] newElements 里添加元素，添加完元素之后，再将原容器的引用指向新的容器 setArray(newElements)这样做的好处是可以对 copyonwrite 容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以 copyonwrite容器也是一种读写分离的思想，读和写不同的容器。","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"原子类Atomiclnteger的ABA问题","date":"2019-08-20T05:56:30.000Z","path":"wiki/原子类Atomiclnteger的ABA问题/","text":"ABA问题怎么产生的CAS会导致 ABA 问题 CAS 算法实现一个重要前提需要取出内存中某时刻的数据并在当下时刻比较并替换，那么在这个时间差类会导致数据的变化。 尽管线程 one 的 CAS 操作成功，但是不代表这个过程就是没问题的。 原子引用解决ABA问题 时间戳原子引用AtomicStampledReferenceABADemo","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"CAS理解","date":"2019-08-20T05:49:03.000Z","path":"wiki/CAS理解/","text":"比较并交换CAS底层原理？如果知道，谈谈比对Unsafe的理解Unsafe Unsafe 是 CAS 的核心类，由于 Java 方法无法直接访问底层系统，需要通过本地（native）方法来访问，Unsafe 相当于一个后门，基于该类可以直接操作特定内存的数据。Unsafe 类存在于 sun.misc 包中，其内部方法操作可以像 C 的指针一样直接操作内存，因为 Java 中 CAS 操作的执行依赖于 Unsafe 类的方法。 注意 Unsafe 类中的所有方法都是 native 修饰的，也就是说 Unsafe 类中的方法都直接调用操作系统底层资源执行相应任务。 变量 valueOffset，表示该变量在内存中的偏移地址，因为 Unsafe 就是根据内存偏移地址获取数据的。 变量 value 用 volatile 修饰，保证了多线程之间的内存可见性。 CAS 是什么 unsafe.getAndAddInt 假设线程 A 和线程 B 两个线程同时执行 getAndAddInt 操作（分别跑在不同 CPU 上） AtomicInteger 里面的 value 原始值为 3，即主内存中 AtomicInteger 的 value 为 3，根据 JMM 模型，线程 A 和线程 B 各自持有一份值为 3 的 value 的副本分别到各自的工作内存。 线程 A 通过 getIntVolatile（var1，var2）拿到 value 值 3，这是线程 A 被挂起。 线程 B 也通过 getIntVolatile（var1，var2）方法获得 value 值 3，此时刚好线程 B 没有被挂起并执行 compareAndSwap 方法比较内存值也为 3，成功修改内存值为 4，线程 B 打完收工，一切OK。 这是线程 A 回复，执行 compareAndSwapInt 方法比较，发现手里的值 3 与内存值 4 不一致，说明该值已经被其他线程抢险异步修改过了，那 A 线程本次修改失败，只能重新读取重新来一遍了。 线程 A 重新获取 value 值，因为变量 value 被 volatile 修饰，所以其他线程对它的修改，线程 A 总是能够看到，线程 A 继续执行 compareAndSwapInt 进行比较替换，直到成功。 底层汇编 简单版小总结CAS（CompareAndSwap）比较当前工作内存中的值和主内存中的值，如果相同则执行规定操作，否则继续比较知道主内存和工作内存中的值一致为止。 CAS应用CAS有 3 个操作数，内存值 V，旧的预期值 A，要修改的更新值 B。当且仅当预期值 A 和内存值 V 相同时，将内存值 V 修改为 B，否则什么都不做。 CAS缺点 循环时间长开销大如果 CAS 失败，会一直进行尝试。如果 CAS 长时间一直不成功，可能会给 CPU 带来很大的开销。 只能保证一个共享变量的原子操作当对一个共享变量执行操作时，我们只能使用循环 CAS 的方式来保证原子操作，但是，对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁来保证原子性。 引出来 ABA 问题？？？通过原子引用解决 ABA 问题","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"volatile","date":"2019-08-19T09:23:18.000Z","path":"wiki/volatile/","text":"volatile 的理解volatile 是 Java 虚拟机提供的轻量级的同步机制 保证可见性各个线程对主内存中共享变量的操作都是各个线程各自拷贝到自己的工作内存进行操作后再写回主内存中的。 这就可能存在一个线程 A 修改了共享变量 X 的值但还未写回主内存时，另一个线程 B 又对准内存中同一个共享变量 X 进行操作，但此时 A 线程工作内存中共享变量 X 对线程 B 来说并不是可见，这种工作内存与主内存同步存在延迟现象就造成了可见性问题。 不保证原子性禁止指令重排volatile 实现禁止指令重排优化，从而避免多线程环境下程序出现乱序执行的现象。 先了解一个概念，内存屏障又称内存栅栏，是一个CPU指令，它的作用有两个：一是保证特定操作的执行顺序二是保证某些变量的内存可见性（利用该特性实现volatile的内存可见性） 由于编译器和处理器都能执行指令重排优化。如果在指令间插入一条 Memory Barrier 则告诉编译器和 CPU，不管什么指令都不能和这条 Memory Barrier 指令重新排序，也就是说通过插入内存屏障禁止在内存屏障前后的指令执行重排序优化。内存屏障另外一个作用是强制刷出各种CPU的缓存数据，因此任何CPU上的线程都能读取到这些数据的最新版本。 volatile使用场景单例模式 DCL 代码单例模式 volatile 分析 DCL（双端检锁）机制不一定线程安全，原因是有指令重排序的存在，加入 volatile 可以禁止指令重排。原因在于某一个线程执行到第一个检测，读取到的 instance 不为 null 时，instance 的引用对象可能没有完成初始化。指令重排只会保证串行语义的执行一致性（单线程），但并不会关心多线程间的语义一致性。所以当一条线程访问 instance 不为 null 时，由于 instance 实例未必已初始化完成，也就造成了线程安全问题。","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"java线程池知识点","date":"2019-08-19T07:33:15.000Z","path":"wiki/java线程池知识点/","text":"为什么用线程池，优势线程池主要是控制运行线程的数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务来执行。 主要特点是：线程复用、控制最大并发数、管理线程。 第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 线程池如何使用架构说明Java中的线程池是通过 Executor 框架实现的，该框架中用到了 Executor，Executors，ExecutorService，ThreadPoolExecutor 这几个类。 编码实现 Executors.newFixedThreadPool(int) 执行长期的任务，性能好很多 主要特点： 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newFixedThreadPool 创建的线程池 corePoolSize 和 maximumPoolSize 值是相等的，它使用的 LinkedBlockingQueue。 Executors.newSingleThreadExecutor() 一个任务一个任务执行的场景 主要特点： 创建一个单线程化的线程池，它只会唯一的工作线程来执行任务，保证所有任务按照指定顺序执行。 newSingleThread 将 corePoolSize 和 maximumPoolSize 都设置为1，它使用的是LinkedBlockingQueue。 Executors.newCachedThreadPool() 适用：执行很多短期异步的小程序或者负载较轻的服务器 主要特点： 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newCachedThreadPool 将 corePoolSize 设置为 0，将 maximumPoolSize 设置为 Integer.MAX_VALUE，使用的 SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过 60 秒，就销毁线程。 阿里巴巴 java 开发规范【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 说明：Executors 返回的线程池对象的弊端如下： FixedThreadPool 和 SingleThreadPool：允许的请求队列长度为 Integer.MAX_VALUE ，可能会堆积大量的请求，从而导致 OOM。 CachedThreadPool：允许的创建线程数量为 Integer.MAX_VALUE ，可能会创建大量的线程，从而导致 OOM。 线程池的几个重要参数介绍 corePoolSize 线程池中的常驻核心线程数 在创建线程池后，当有请求任务来之后，就会安排池中的线程去执行请求任务，近似理解为今日当值线程。 当线程池中的线程数目到达 corePoolSize 后，就会把到达的任务放到缓存队列当中。 maximumPoolSize 线程池能够容纳同时执行的最大线程数，此值必须大于等于1. keepAliveTime 多余的空闲线程的存活时间。当前线程池数量超过 corePoolSize 时，当空闲时间达到 keepAliveTime 值时，多余空闲线程会被销毁直到只剩下 corePoolSize 个线程为止。 默认情况下：只有当线程池中的线程数大于 corePoolSize 时 keepAliveTime 才会起作用，直到线程池中的线程数不大于 corePoolSize。 unit keepAliveTime 的单位 workQueue 任务队列，被提交但尚未被执行的任务。 threadFactory 表示生成线程池中工作线程的线程工厂，用于创建线程一般用默认的即可。 handler 拒绝策略，表示当队列满了并且工作线程大于等于线程池的最大线程数。 线程池的底层工作原理？ 在创建了线程池后，等待提交过来的任务请求。 当调用 execute() 方法添加一个请求任务时，线程池会做如下判断： 如果正在运行的线程数量 &lt; corePoolSize，那么马上创建线程运行这个任务。 如果正在运行的线程数量 &gt;= corePoolSize，那么将这个任务放入队列。 如果这个时候队列满了且正在运行的线程数量还 &lt; maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务。 如果队列满了且正在运行的线程数量 &gt;= maximumPoolSize，那么线程池会启动饱和拒绝策略来执行。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做超过一定的时间 (keepAliveTime) 时，线程池会判断： 如果当前运行的线程数 &gt; corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后它最终会收缩到 corePoolSize 的大小。 线程池的拒绝策略是什么等待队列也满了，再也塞不下新任务了，同时线程池中的 max 线程也达到了，无法继续为新的任务服务。 这时候就需要拒绝策略机制合理的处理这个问题。 JDK 内置的拒绝策略 AbortPolicy（默认）：直接抛出 RejectedExecutionException 异常阻止系统正常运行。 CallerRunsPolicy：“调用者运行”一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。 DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前任务。 DiscardPolicy：直接丢弃任务，不予任何处理也不抛出异常。如果允许任务丢失，这是最好的一种方案。 以上内置拒绝策略均实现了 RejectedExecutionHandler 接口 合理配置线程池你是如何考虑的？CPU密集型CPU 密集的意思是该任务需要大量的运算，而没有阻塞，CPU 一直全速运行。 CPU 密集型任务配置尽可能少的线程数量： 一般公式为：CPU 核数 + 1个线程的线程池。 IO密集型 由于 IO 密集型任务线程并不是一直执行任务，则应配置尽可能多的线程，如 CPU 核数 * 2 IO 密集型，即该任务需要大量的 IO，即大量的阻塞。 在单线程上运行 IO 密集型的任务会导致浪费大量的 CPU 运算能力浪费在等待。 所以 IO 密集型任务中使用多线程可以大大的加速程序运行，即使在单核 CPU 上，这种加速主要就是利用了被浪费掉的阻塞时间。 IO 密集型时，大部分线程都阻塞，故需要多配置线程数： 参考公式：CPU 核数 / (1 - 阻塞系数) 阻塞系数在 0.8-0.9 之间 比如 8 核 CPU：8 / (1 - 0.9) = 80 个线程数","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"Redis基础数据结构","date":"2019-08-18T03:26:40.000Z","path":"wiki/Redis基础数据结构/","text":"Redis 有 5 种基础数据结构，分别为：string (字符串)、list (列表)、set (集合)、hash (哈希) 和 zset (有序集合)。 string (字符串)set 添加$ set name zhangsan # 添加OK get 获取$ get name # 获取\"zhangsan\" del 删除$ del name # 删除(integer) 1 exists 是否存在$ exists name # 字符串是否存在(integer) 1 strlen 获取长度$ strlen name # 获取字符串的长度(integer) 8 getrange 获取子串$ getrange name 0 4 # 获取子串(开始和结束位置start,end)\"zhang\" mget 批量获取$ mget name1 name2 # 返回一个列表1) \"zhangsan\"2) \"lisi\" mset 批量添加$ mset name1 wangwu name2 zhaoliuOK expire 过期$ set name zhangsan$ expire name 20 # 20s后过期$ ttl name # 还有18秒的寿命，返回-2表示变量不存在，-1表示没有设置过期时间(integer) 18 setex 等价于set + expire$ setex name 5 zhangsan # 5s后过期，等价于 set + expire$ get name\"zhangsan\" setnx 如果key不存在就执行set操作$ setnx name zhangsan # 如果 name 不存在就执行 set 创建(integer) 1$ get name\"zhangsan\"$ setnx name zhangsan(integer) 0 # 因为 name 已经存在，所以 set 创建不成功 setnx + expire 分布式锁$ set book java ex 30 nx # setnx + expire 组合在一起的原子命令OK incr 递增$ set age 30OK$ incr age # 递增(integer) 31$ incrby age 5(integer) 36 decr 递减$ set age 30OK$ decr age # 递减(integer) 29$ decrby age 5(integer) 24 自增是有范围的，它的范围是 signed long 的最大最小值，超过了这个值，Redis 会报错。 $ set codehole 9223372036854775807 # Long.Max 最大值OK$ incr codehole(error) ERR increment or decrement would overflow list (列表)Redis 的列表相当于 Java 里面的 LinkedList，这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n) 当列表弹出了最后一个元素之后，该数据结构自动被删除，内存被回收。 Redis 的列表结构常用来做异步队列使用。将需要延后处理的任务结构体序列化成字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理。 右进左出：队列$ rpush books python java golang(integer) 3$ llen books(integer) 3$ lpop books\"python\"$ lpop books\"java\"$ lpop books\"golang\"$ lpop books(nil) 右进右出：栈$ rpush books python java golang(integer) 3$ rpop books\"golang\"$ rpop books\"java\"$ rpop books\"python\"$ rpop books(nil) hash (字典)Redis 的字典相当于 Java 里面的 HashMap，它是无序字典。内部实现结构上同 Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。 不同的是，Redis 的字典的值只能是字符串，另外它们 rehash 的方式不一样，因为 Java 的 HashMap 在字典很大时，rehash 是个耗时的操作，需要一次性全部 rehash。Redis 为了高性能，不能堵塞服务，所以采用了渐进式 rehash 策略。 渐进式 rehash 会在 rehash 的同时，保留新旧两个 hash 结构，查询时会同时查询两个 hash 结构，然后在后续的定时任务中以及 hash 操作指令中，循序渐进地将旧 hash 的内容一点点迁移到新的 hash 结构中。当搬迁完成了，就会使用新的hash结构取而代之。 当 hash 移除了最后一个元素之后，该数据结构自动被删除，内存被回收。 hash 结构也可以用来存储用户信息，不同于字符串一次性需要全部序列化整个对象，hash 可以对用户结构中的每个字段单独存储。这样当我们需要获取用户信息时可以进行部分获取。而以整个字符串的形式去保存用户信息的话就只能一次性全部读取。 hash 结构的存储消耗要高于单个字符串。 hset 添加# 命令行的字符串如果包含空格，要用引号括起来$ hset books java \"think in java\" # 新增(integer) 1 hget 根据key获取value$ hget books java # 根据key获取value\"think in java\" hgetall 获取全部$ hgetall books # 查询entries()，key 和 value 间隔出现1) \"java\"2) \"think in java\"3) \"golang\"4) \"concurrency in go\"5) \"python\"6) \"python cookbook\" hlen 获取长度$ hlen books # 获取长度(integer) 3 hmset 批量添加$ hmset books java \"effective java\" python \"learning python\" golang \"modern golang programming\"OK hincrby 递增可以使用 hincrby 对 hash 结构中的单个子 key 进行计数，和 incr使用方法一样。 $ hincrby user age 1(integer) 30 set (集合)Redis 的集合相当于 Java 里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值NULL。 当集合中最后一个元素移除之后，数据结构自动删除，内存被回收。 set 结构可以用来存储活动中奖的用户 ID，因为有去重功能，可以保证同一个用户不会中奖两次。 sadd 添加$ sadd books python # 新增(integer) 1$ sadd books java golang # 批量添加(integer) 2 smembers 获取$ smembers books # 获取，set是无序的1) \"java\"2) \"python\"3) \"golang\" srem 移除$ srem book java(integer) 1 sismember 查询value是否存在$ sismember books java # 查询某个value是否存在，相当于 contains(o)(integer) 1 scard 获取长度$ scard books # 获取长度相当于 count()(integer) 3 spop 弹出一个$ spop books # 弹出一个\"java\" zset (有序集合)zset 类似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重。 zset 中最后一个 value 被移除后，数据结构自动删除，内存被回收。 zset 可以用来存粉丝列表，value 值是粉丝的用户 ID，score 是关注时间。我们可以对粉丝列表按关注时间进行排序。 zset 还可以用来存储学生的成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次。 zadd 添加$ zadd books 9.0 \"think in java\"(integer) 1$ zadd books 8.9 \"java concurrency\"(integer) 1$ zadd books 8.6 \"java cookbook\"(integer) 1 zrange 按score排序列出$ zrange books 0 -1 # 按 score 排序列出，参数区间为排名范围1) \"java cookbook\"2) \"java concurrency\"3) \"think in java\" zrevrange 按score逆序列出$ zrevrange books 0 -1 # 按 score 逆序列出，参数区间为排名范围1) \"think in java\"2) \"java concurrency\"3) \"java cookbook\" zcard 获取个数，相当于 count()$ zcard books # 相当于 count()(integer) 3 zrank 排名$ zrank books \"java concurrency\" # 排名(integer) 1 zscore 获取指定value的score$ zscore books \"java concurrency\" # 获取指定value的score\"8.9000000000000004\" # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题 zrangebyscore 根据分值区间遍历$ zrangebyscore books 0 8.91 # 根据分值区间遍历 zset1) \"java cookbook\"2) \"java concurrency\"$ zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。1) \"java cookbook\"2) \"8.5999999999999996\"3) \"java concurrency\"4) \"8.9000000000000004\" zrem$ zrem books \"java concurrency\" # 删除 value(integer) 1 容器型数据结构的通用规则list / set / hash / zset 这四种数据结构是容器型数据结构，它们都有下面两条通用规则： create if not exists 如果容器不存在，那就创建一个，再进行操作。比如 rpush 操作刚开始是没有列表的，Redis 就会自动创建一个，然后再 rpush 进去新元素。 drop if no elements 如果容器里元素没有了，那么立即删除元素，释放内存。这意味着 lpop 操作到最后一个元素，列表就消失了。 过期时间Redis 所有的数据结构都可以设置过期时间，时间到了，Redis 会自动删除相应的对象。 还有一个需要特别注意的地方是如果一个字符串已经设置了过期时间，然后你调用了 set 方法修改了它，它的过期时间会消失。 $ set name zhangsan # 添加OK$ expire name 600 # 设置过期时间(integer) 1$ ttl name # 查看过期时间(integer) 597$ set name lisi # 再次添加OK$ ttl name # 查看过期时间，变成了永久有效(integer) -1","tags":[{"name":"Redis","slug":"Redis","permalink":"https://yapengren.github.io/tags/Redis/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://yapengren.github.io/categories/数据库/Redis/"}]},{"title":"HashMap源码分析","date":"2019-08-17T05:45:12.000Z","path":"wiki/HashMap源码分析/","text":"继承体系 在 Java 中，HashMap 的实现采用了（数组 + 链表 + 红黑树）的复杂结构，数组的一个元素又称作桶。 在添加元素时，会根据 hash 值算出元素在数组中的位置，如果该位置没有元素，则直接把元素放置在此处，如果该位置有元素了，则把元素以链表的形式放置在链表的尾部。 当一个链表的元素个数达到一定的数量（且数组的长度达到一定的长度）后，则把链表转化为红黑树，从而提高效率。 数组的查询效率为 O(1)，链表的查询效率是 O(k)，红黑树的查询效率是 O(log k)，k为桶中的元素个数，所以当元素数量非常多的时候，转化为红黑树能极大地提高效率。 源码分析属性/** * 默认初始容量为 16 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;/** * 最大容量为 2 的 30 次方 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 默认装载因子 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 当一个桶中的元素 ≥ 8 时进行树化 */static final int TREEIFY_THRESHOLD = 8;/** * 当一个桶中的元素 ≤ 6 时把树转换成链表 */static final int UNTREEIFY_THRESHOLD = 6;/** * 当桶的个数达到 64 的时候进行树化 */static final int MIN_TREEIFY_CAPACITY = 64;/** * 数组，又叫做桶 */transient Node&lt;K, V&gt;[] table;/** * 作为 enterSet() 的缓存 */transient Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet;/** * 元素的数量 */transient int size;/** * 修改次数，用于在迭代的时候执行快速失败策略 */transient int modCount;/** * 当桶的使用数量达到多少时进行扩容，threshold = capacity * loadFactor */int threshold;/** * 装载因子 */final float loadFactor; put(K key, V value) 方法public V put(K key, V value) &#123; // 调用 hash(key) 计算出 key 的 hash 值 return putVal(hash(key), key, value, false, true);&#125;static final int hash(Object key) &#123; int h; // 如果 key 为 null，则 hash 值为0，否则调用 key 的 hashCode() 方法 // 并让高 16 位与整个 hash 异或，这样做是为了使计算出的 hash 更分散 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果桶的数量为 0，则初始化 if ((tab = table) == null || (n = tab.length) == 0) // 调用 resize() 初始化 n = (tab = resize()).length; // (n-1) &amp; hash 计算元素在哪个桶中 // 如果这个桶中还没有元素，则把这个元素放在桶中的第一个位置 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 新建一个节点放在桶中 tab[i] = newNode(hash, key, value, null); else &#123; // 如果桶中已经有元素存在了 Node&lt;K,V&gt; e; K k; // 如果桶中第一个元素的 key 与待插入元素的 key 相同，保存到 e 中用于后续修改 value 值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) // 如果第一个元素是树节点，则调用树节点的 putTreeVal 插入元素 e = ((HashMap.TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 遍历这个桶对应的链表，binCount 用于存储链表中元素的个数 for (int binCount = 0; ; ++binCount) &#123; // 如果链表遍历完了都没有找到相同 key 的元素，则 key 对应的元素不存在，则在链表最后插入一个新节点 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 如果插入新节点后链表长度 &gt; 8，则判断是否需要树化，因为第一个元素没有加到 binCount 中，所以 -1 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; &#125; // 如果待插入的 key 在链表中找到了，则退出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 如果找到了对应 key 的元素 if (e != null) &#123; // existing mapping for key // 记录下旧值 V oldValue = e.value; // 判断是否需要替换旧值 if (!onlyIfAbsent || oldValue == null) // 替换旧值为新值 e.value = value; afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 到这里了说明没有找到元素 // 修改次数 +1 ++modCount; // 元素数量 +1，判断是否需要扩容 if (++size &gt; threshold) // 扩容 resize(); afterNodeInsertion(evict); // 没有找到元素返回 null return null;&#125; 计算 key 的 hash 值； 如果桶（数组）数量为 0，则初始化桶； 如果 key 所在的桶没有元素，则直接插入； 如果 key 所在的桶中的第一个元素的 key 与待插入的key相同，说明找到了元素，转后续流程（9）处理； 如果第一个元素是树节点，则调用树节点的 putTreeVal() 寻找元素或插入树节点； 如果不是以上三种情况，则遍历桶对应的链表查找 key 是否存在于链表中； 如果找到了对应 key 的元素，则转后续流程（9）处理； 如果没找到对应 key 的元素，则在链表最后插入一个新节点并判断是否需要树化； 如果找到了对应 key 的元素，则判断是否需要替换旧值，并直接返回旧值； 如果插入了元素，则数量加 1 并判断是否需要扩容；","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"源码","slug":"java/源码","permalink":"https://yapengren.github.io/categories/java/源码/"}]},{"title":"ArrayList源码分析","date":"2019-08-17T02:28:42.000Z","path":"wiki/ArrayList源码分析/","text":"继承体系 源码分析属性/** * 默认容量为10，也就是通过 new ArrayList() 创建时的默认容量 */private static final int DEFAULT_CAPACITY = 10;/** * 空数组 * 这种是通过 new ArrayList(0) 创建时用的空数组 */private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;/** * 空数组 * 这种是通过 new ArrayList() 创建时用的空数组 * 与 EMPTY_ELEMENTDATA 的区别是在添加第一个元素时使用这个空数组会初始化为 DEFAULT_CAPACITY（10）个元素 */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;/** * 真正存放元素的地方 * 使用 transient 关键字能不序列化这个字段 */transient Object[] elementData; // non-private to simplify nested class access/** * 真正存放元素的个数 */private int size; ArrayList(int initialCapacity) 构造方法public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; // 如果传入的初始容量 &gt; 0，新建一个数组存储元素 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; // 如果传入的初始容量 = 0，使用空数组 EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; // 如果传入的初始容量 &lt; 0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125; add(E e) 方法添加元素到末尾 public boolean add(E e) &#123; // 检查是否需要扩容 ensureCapacityInternal(size + 1); // 将元素插入到最后一位 elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; // 如果是空数组 DEFAULTCAPACITY_EMPTY_ELEMENTDATA，就初始化为默认大小 10 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; if (minCapacity - elementData.length &gt; 0) // 扩容 grow(minCapacity);&#125;/** * 扩容 * */private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; // 新容量为旧容量的 1.5 倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 如果新容量发现比需要的容量还小，则以需要的容量为准 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量已经超过最大容量了，则使用最大容量 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 以新容量拷贝出来一个新数组 elementData = Arrays.copyOf(elementData, newCapacity);&#125; 检查是否需要扩容； 如果 elementData 等于 DEFAULTCAPACITY_EMPTY_ELEMENTDATA，则初始化容量大小为 DEFAULT_CAPACITY； 新容量是老容量的1.5 倍（oldCapacity + (oldCapacity &gt;&gt; 1)），如果加了这么多容量发现比需要的容量还小，则以需要的容量为准； 创建新容量的数组并把老数组拷贝到新数组；","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"源码","slug":"java/源码","permalink":"https://yapengren.github.io/categories/java/源码/"}]},{"title":"Mybatis 知识点","date":"2019-08-14T09:20:15.000Z","path":"wiki/Mybatis-知识点/","text":"jdbc 问题 数据库连接创建、释放频繁造成系统资源浪费，从而影响系统性能。如果使用数据库连接池可解决此问题。 Sql 语句在代码中硬编码，造成代码不易维护，实际应用中 sql 变化的可能较大，sql 变动需要改变 java 代码。 使用 preparedStatement 向占有位符号传参数存在硬编码，因为 sql 语句的 where 条件不一定，可能多也可能少，修改 sql 还要修改代码，系统不易维护。 对结果集解析存在硬编码（查询列名），sql 变化导致解析代码变化，系统不易维护，如果能将数据库记录封装成 pojo 对象解析比较方便。 Mybatis 架构 mybatis配置SqlMapConfig.xml，此文件作为 mybatis 的全局配置文件，配置了 mybatis 的运行环境等信息。mapper.xml 文件即sql映射文件，文件中配置了操作数据库的 sql 语句。此文件需要在SqlMapConfig.xml 中加载。 通过 mybatis 环境等配置信息构造 SqlSessionFactory 即会话工厂。 由会话工厂创建 sqlSession 即会话，操作数据库需要通过 sqlSession 进行。 mybatis 底层自定义了 Executor 执行器接口操作数据库，Executor 接口有两个实现，一个是基本执行器、一个是缓存执行器。 Mapped Statement 也是 mybatis 一个底层封装对象，它包装了 mybatis 配置信息及 sql 映射信息等。mapper.xml 文件中一个 sql 对应一个 Mapped Statement 对象，sql 的 id 即是 Mapped statement 的id。 Mapped Statement 对 sql 执行输入参数进行定义，包括 HashMap、基本类型、pojo，Executor 通过Mapped Statement 在执行 sql前将输入的 java 对象映射至 sql 中，输入参数映射就是 jdbc 编程中对preparedStatement 设置参数。 Mapped Statement 对 sql 执行输出结果进行定义，包括 HashMap、基本类型、pojo，Executor 通过Mapped Statement 在执行 sql 后将输出结果映射至 java 对象中，输出结果映射过程相当于 jdbc 编程中对结果的解析处理过程。 #{} 和 ${} 区别 #{} 表示一个占位符号，通过 #{} 可以实现 preparedStatement 向占位符中设置值，自动进行 java 类型和 jdbc 类型转换。#{} 可以有效防止 sql 注入。#{} 可以接收简单类型值或 pojo 属性值。如果parameterType 传输单个简单类型值，#{} 括号中可以是 value 或其它名称。 ${} 表示拼接 sql 串，通过 ${} 可以将 parameterType 传入的内容拼接在 sql 中且不进行 jdbc 类型转换，${}可以接收简单类型值或 pojo 属性值，如果 parameterType 传输单个简单类型值，${} 括号中只能是 value。 parameterType 和 resultType parameterType：指定输入参数类型，mybatis 通过 ognl 从输入对象中获取参数值拼接在 sql 中。 resultType：指定输出结果类型，mybatis 将 sql 查询结果的一行记录数据映射为 resultType 指定类型的对象。如果有多条数据，则分别进行映射，并把对象放到容器List中。 Mapper 接口开发规范 Mapper.xml 文件中的 namespace 与 Mapper 接口的类路径相同 Mapper.xml 中定义的每个 statement 的 id 与 Mapper 接口方法名相同 Mapper.xml 中定义的每个 sql 的 parameterType 的类型与 Mapper 接口方法的输入参数类型相同 Mapper.xml 中定义的每个 sql 的 resultType 的类型与 Mapper 接口方法的输出参数类型相同 动态 sqlif 标签 &amp; where 标签Mapper.xml 文件 &lt;select id=\"selectAllData\" parameterType=\"CfContentLikeRecordDto\" resultType=\"CfContentLikeRecord\"&gt; SELECT * FROM cf_content &lt;!-- where标签可以自动添加where，同时处理sql语句中第一个and关键字 --&gt; &lt;WHERE&gt; &lt;if test=\"key != null and key !='' \"&gt; and `key` = #&#123;key&#125; &lt;/if&gt; &lt;if test=\"startTime != null and startTime !='' \"&gt; &lt;![CDATA[ AND create_Time &gt;= #&#123;startTime&#125;]]&gt; &lt;/if&gt; &lt;if test=\"endTime != null and endTime !='' \"&gt; &lt;![CDATA[ AND create_Time &lt;= #&#123;endTime&#125;]]&gt; &lt;/if&gt; &lt;/WHERE&gt;&lt;/select&gt; foreach 标签向 sql 传递数组或 List，mybatis 使用 foreach 解析，如下： 根据多个 id 查询用户信息 查询 sql： SELECT * FROM user WHERE id IN (1,10,24) &lt;!-- 根据ids查询用户 --&gt;&lt;select id=\"queryUserByIds\" parameterType=\"queryVo\" resultType=\"user\"&gt; SELECT * FROM `user` &lt;where&gt; &lt;!-- foreach标签，进行遍历 --&gt; &lt;!-- collection：遍历的集合，这里是 QueryVo 的 ids 属性 --&gt; &lt;!-- item：遍历的项目，可以随便写，，但是和后面的 #&#123;&#125;里面要一致 --&gt; &lt;!-- open：在前面添加的sql片段 --&gt; &lt;!-- close：在结尾处添加的sql片段 --&gt; &lt;!-- separator：指定遍历的元素之间使用的分隔符 --&gt; &lt;foreach collection=\"ids\" item=\"item\" open=\"id IN (\" close=\")\" separator=\",\"&gt; #&#123;item&#125; &lt;/foreach&gt; &lt;/where&gt;&lt;/select&gt; 关联查询数据模型 一对一查询需求：查询所有订单信息，关联查询下单用户信息。 注意：因为一个订单信息只会是一个人下的订单，所以从查询订单信息出发关联查询用户信息为一对一查询。如果从用户信息出发查询用户下的订单信息则为一对多查询，因为一个用户可以下多个订单。 使用 resultType使用 resultType，改造订单 pojo 类，此 pojo 类中包括了订单信息和用户信息 这样返回对象的时候，mybatis 自动把用户信息也注入进来了 pojo 类OrderUser 类继承类包括了 Order 类的所有字段，只需要定义用户的信息字段即可 public class OrderUser extends Order &#123; private String username; private String address;&#125; Mapper.xml&lt;!-- 查询订单，同时包含用户数据 --&gt;&lt;select id=\"queryOrderUser\" resultType=\"orderUser\"&gt; SELECT o.id, o.user_id, userId, o.number, o.createtime, o.note, u.username, u.address FROM `order` o LEFT JOIN `user` u ON o.user_id = u.id&lt;/select&gt; 使用 resultMap使用 resultMap，定义专门的 resultMap 用于映射一对一查询结果 pojo 类在 Order 类中加入 User 属性，user 属性中用于存储关联查询的用户信息，因为订单关联查询用户是一对一关系，所以这里使用单个 User 对象存储关联查询的用户信息 public class Order &#123; private int id; // 订单id private Integer userId; // 用户id private String number; // 订单号 private Date createTime; // 订单创建时间 private String note; // 备注 private User user; // 用户信息&#125; Mapper.xml&lt;resultMap type=\"order\" id=\"orderUserResultMap\"&gt; &lt;id property=\"id\" column=\"id\" /&gt; &lt;result property=\"userId\" column=\"user_id\" /&gt; &lt;result property=\"number\" column=\"number\" /&gt; &lt;result property=\"createTime\" column=\"createtime\" /&gt; &lt;result property=\"note\" column=\"note\" /&gt; &lt;!-- association ：配置一对一属性 --&gt; &lt;!-- property:order里面的User属性名 --&gt; &lt;!-- javaType:属性类型 --&gt; &lt;association property=\"user\" javaType=\"user\"&gt; &lt;!-- id:声明主键，表示user_id是关联查询对象的唯一标识--&gt; &lt;id property=\"id\" column=\"user_id\" /&gt; &lt;result property=\"username\" column=\"username\" /&gt; &lt;result property=\"address\" column=\"address\" /&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;!-- 一对一关联，查询订单，订单内部包含用户属性 --&gt;&lt;select id=\"queryOrderUserResultMap\" resultMap=\"orderUserResultMap\"&gt; SELECT o.id, o.user_id, o.number, o.createtime, o.note, u.username, u.address FROM `orders` o LEFT JOIN `user` u ON o.user_id = u.id&lt;/select&gt; 一对多查询案例：查询所有用户信息及用户关联的订单信息。 用户信息和订单信息为一对多关系。 pojo 类public class User &#123; private int id; private String username; // 用户姓名 private String sex; // 性别 private Date birthday; // 生日 private String address; // 地址 private List&lt;Order&gt; orders;&#125; Mapper.xml&lt;resultMap type=\"user\" id=\"userOrderResultMap\"&gt; &lt;id property=\"id\" column=\"id\" /&gt; &lt;result property=\"username\" column=\"username\" /&gt; &lt;result property=\"birthday\" column=\"birthday\" /&gt; &lt;result property=\"sex\" column=\"sex\" /&gt; &lt;result property=\"address\" column=\"address\" /&gt; &lt;!-- 配置一对多的关系 --&gt; &lt;collection property=\"orders\" javaType=\"list\" ofType=\"order\"&gt; &lt;!-- 配置主键，是关联Order的唯一标识 --&gt; &lt;id property=\"id\" column=\"oid\" /&gt; &lt;result property=\"number\" column=\"number\" /&gt; &lt;result property=\"createtime\" column=\"createtime\" /&gt; &lt;result property=\"note\" column=\"note\" /&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;!-- 一对多关联，查询订单同时查询该用户下的订单 --&gt;&lt;select id=\"queryUserOrder\" resultMap=\"userOrderResultMap\"&gt; SELECT u.id, u.username, u.birthday, u.sex, u.address, o.id oid, o.number, o.createtime, o.note FROM `user` u LEFT JOIN `order` o ON u.id = o.user_id&lt;/select&gt;","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://yapengren.github.io/tags/Mybatis/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"Mybatis","slug":"框架/Mybatis","permalink":"https://yapengren.github.io/categories/框架/Mybatis/"}]},{"title":"synchronized和lock区别","date":"2019-08-04T06:32:36.000Z","path":"wiki/synchronized和lock区别/","text":"原始构成 synchronized 是关键字，属于JVM层面 monitorenter（底层是通过 monitor 对象来完成，其实 wait/notify 等方法也依赖于 monitor 对象只有在同步块或者方法中才能调用 wait/notify 等方法） monitorexit Lock 是具体类（java.util.concurrent.locks.lock）是 api 层面的锁 使用方法 synchronized 不需要用户去手动释放锁，当 synchronized 代码执行完后系统会自动让线程释放对锁的占用。 ReentrantLock 则需要用户去手动释放锁，若没有主动释放锁，就有可能导致出现死锁现象。需要 lock() 和unlock() 方法配合 try/finally 语句块来完成。 等待是否可中断 synchronized 不可中断，除非抛出异常或者正常运行完成。 ReentrantLock可中断 1.设置超时方法 tryLock(long timeout,TimeUnit unit) 2.lockInterruptibly() 放代码块中，调用 interrupt() 方法可中断 加锁是否公平 synchronized 非公平锁 ReentrantLock 两者都可以，默认非公平锁，构造方法可以传入 boolean 值，true为公平锁，false为非公平锁 锁绑定多个条件 Condition synchronized 没有 ReentrantLock 用来实现分组唤醒需要唤醒的线程，可以精确唤醒，而不是像 synchronized 要么随机唤醒一个要么唤醒全部线程。 举例：锁绑定多个条件 Condition/** * * 题目：多线程之间按顺序调用，实现A-&gt;B-&gt;C三个线程启动，要求如下： * A打印5次，B打印10次，C打印15次 * 紧接着 * A打印5次，B打印10次，C打印15次 * 打印10轮 */public class SyncAndReentrantLockdemo &#123; public static void main(String[] args) &#123; ShareResource shareResource = new ShareResource(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; shareResource.print5(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; shareResource.print10(); &#125; &#125;, \"B\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; shareResource.print15(); &#125; &#125;, \"C\").start(); &#125;&#125;class ShareResource &#123; private int number = 1; private Lock lock = new ReentrantLock(); private Condition c1 = lock.newCondition(); private Condition c2 = lock.newCondition(); private Condition c3 = lock.newCondition(); public void print5() &#123; lock.lock(); try &#123; // 1判断 while (number != 1) &#123; c1.await(); &#125; // 2干活 for (int i = 0; i &lt; 5; i++) &#123; System.out.println(Thread.currentThread().getName() + \" \" + i); &#125; // 3通知 number = 2; c2.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void print10() &#123; lock.lock(); try &#123; // 1判断 while (number != 2) &#123; c2.await(); &#125; // 2干活 for (int i = 0; i &lt; 10; i++) &#123; System.out.println(Thread.currentThread().getName() + \" \" + i); &#125; // 3通知 number = 3; c3.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void print15() &#123; lock.lock(); try &#123; // 1判断 while (number != 3) &#123; c3.await(); &#125; // 2干活 for (int i = 0; i &lt; 5; i++) &#123; System.out.println(Thread.currentThread().getName() + \" \" + i); &#125; // 3通知 number = 1; c1.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"java hash 原理","date":"2019-08-04T05:23:50.000Z","path":"wiki/java-hash-原理/","text":"","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"源码","slug":"java/源码","permalink":"https://yapengren.github.io/categories/java/源码/"}]},{"title":"让我们同步进阶","date":"2019-08-04T05:18:18.000Z","path":"wiki/让我们同步进阶/","text":"友情链接高志龙的博客 圈子美团技术团队 阿里中间件团队博客 有赞技术团队 廖雪峰的Git教程 spring boot &amp; cloudSpring Cloud 程序猿DD","tags":[],"categories":[]},{"title":"FastDFS流程图","date":"2019-08-04T03:34:10.000Z","path":"wiki/FastDFS流程图/","text":"FastDFS 执行流程FastDFS 服务端有三个角色：跟踪服务器（Tracker Server）、存储服务器（Storage Server）和客户端（Client）。 Tracker Server：跟踪服务器，主要做调度工作，起负载均衡的作用。在内存记录集群中所有存储组和存储服务器的状态信息，是客户端和数据服务器交互的枢纽。相比 GFS 中的 Master 更为精简，不记录文件索引信息，占用的内存量很少。 Storage Server：存储服务器（又称存储节点或数据服务器），文件和文件属性（Meta Data）都保存到存储服务器上。Storage Server 直接利用 OS 的文件系统调用管理文件。 Client：客户端，作为业务请求的发起方，通过专有接口，使用 TCP/IP 协议与跟踪器服务器或存储节点进行数据交互。FastDFS 向使用者提供基本文件访问接口，如 upload、download、append、delete 等，以客户端库的方式提供给用户使用。 通过一张图来看一下 FastDFS 的运行机制： Tracker 相当于 FastDFS 的大脑，不论是上传还是下载都是通过 Tracker 来分配资源；客户端一般可以使用 Ngnix 等静态服务器来调用或者做一部分的缓存；存储服务器内部分为卷（或者叫做组），卷与卷之间是平行的关系，可以根据资源的使用情况随时增加，卷内服务器文件相互同步备份，以达到容灾的目的。 上传机制首先客户端请求 Tracker 服务获取到存储服务器的 IP 地址和端口，然后客户端根据返回的 IP 地址和端口号请求上传文件，存储服务器接收到请求后生产文件，并且将文件内容写入磁盘并返回给客户端 file_id、路径信息、文件名等信息，客户端保存相关信息上传完毕。 下载机制客户端带上文件名信息请求 Tracker 服务获取到存储服务器的 IP 地址和端口，然后客户端根据返回的 IP 地址和端口号请求下载文件，存储服务器接收到请求后返回文件给客户端。","tags":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://yapengren.github.io/tags/FastDFS/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"FastDFS","slug":"框架/FastDFS","permalink":"https://yapengren.github.io/categories/框架/FastDFS/"}]},{"title":"springMVC流程图","date":"2019-08-04T03:30:24.000Z","path":"wiki/springMVC流程图/","text":"springMVC执行流程 用户发送请求至前端控制器 DispatcherServlet 前端控制器 DispatcherServlet 收到请求调用处理器映射器HandlerMapping。 处理器映射器 HandlerMapping 根据请求 url 找到具体的处理器，生成处理器对象及处理器拦截器一并返回给前端控制器DispatcherServlet。 前端控制器 DispatcherServlet 通过处理器适配器 HandlerAdapter 调用处理器 执行处理器，执行业务逻辑(Controller，也叫后端控制器)。 Controller 执行完成返回 ModelAndView 处理器适配器 HandlerAdapter 将 controller 执行结果 ModelAndView 返回给前端控制器 DispatcherServlet 前端控制器 DispatcherServlet 将 ModelAndView 传给视图解析器 ViewResolver 视图解析器 ViewResolver 解析后返回具体 View DispatcherServlet 对 View 进行渲染视图（即将模型数据填充至视图中）。 DispatcherServlet 响应用户","tags":[{"name":"spring","slug":"spring","permalink":"https://yapengren.github.io/tags/spring/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"spring","slug":"框架/spring","permalink":"https://yapengren.github.io/categories/框架/spring/"}]},{"title":"shiro 流程图","date":"2019-08-04T03:28:49.000Z","path":"wiki/shiro流程图/","text":"","tags":[{"name":"shiro","slug":"shiro","permalink":"https://yapengren.github.io/tags/shiro/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"shiro","slug":"框架/shiro","permalink":"https://yapengren.github.io/categories/框架/shiro/"}]},{"title":"Dubbo架构","date":"2019-08-04T03:15:00.000Z","path":"wiki/Dubbo架构/","text":"节点角色说明 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 参考http://dubbo.apache.org/zh-cn/docs/user/preface/architecture.html","tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://yapengren.github.io/tags/Dubbo/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"Dubbo","slug":"框架/Dubbo","permalink":"https://yapengren.github.io/categories/框架/Dubbo/"}]},{"title":"intellij idea修改记录","date":"2019-08-02T10:13:00.000Z","path":"wiki/intellij idea修改记录/","text":"过滤文件 Ignore files and folders *.classpath;*.gitignore;*.hprof;*.idea;*.iml;*.pyc;*.pyo;*.rbc;*.yarb;*~;.DS_Store;.git;.hg;.mvn;.svn;CVS;__pycache__;_svn;logs;mvnw;mvnw.cmd;target;vssver.scc;vssver2.scc; 显示多 Tabs 关闭拖动功能","tags":[{"name":"util","slug":"util","permalink":"https://yapengren.github.io/tags/util/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://yapengren.github.io/categories/工具/"},{"name":"IDE","slug":"工具/IDE","permalink":"https://yapengren.github.io/categories/工具/IDE/"}]},{"title":"mac服务","date":"2019-08-02T09:40:48.000Z","path":"wiki/mac服务/","text":"brew 常用命令# brew 搜索软件$ brew search nginx# brew 安装软件$ brew install nginx# brew 卸载软件$ brew uninstall nginx# brew 升级$ sudo brew update# 查看安装信息$ sudo brew info nginx # 查看已安装软件$ brew list brew 清除locksbrew install 如果没有取消自动更新，每次都要更新好久，如果这时候意外按下control+z 再次执行 brew install 会提示 Error: Another active Homebrew update process is already in progress.Please wait for it to finish or terminate it to continue. 解决办法： rm -rf /usr/local/var/homebrew/locks brew 安装node低版本使用使用 brew install node 默认是最高版本。 需要先执行 brew unlink node 来解绑 node 安装需要的版本，例如 brew install node@10 执行 brew link node@10，这一步可能会报错，按照提示命令执行就可以 执行 brew link –overwrite –force node@10 node -v 查看版本是否是想要的低版本 brew 安装nginx# 安装$ sudo brew install nginx# 启动$ sudo brew services start nginx# 重启$ sudo brew services restart nginx# 停止$ sudo brew services stop nginx# 查看$ cat /usr/local/etc/nginx/nignx.conf brew redis# 启动$ redis-server /usr/local/etc/redis-6379.conf# 停止$ redis-cli -p 6379 shutdown brew 安装 activemq# 安装$ brew install activemq# 查看版本$ activemq —version# 启动$ activemq start# 地址# http://localhost:8161 admin admin brew 安装 rabbitmq# 安装$ brew install rabbitmq# 启动$ brew services start rabbitmq# 地址# http://localhost:15672 guest guest brew 安装 mongodb参考：https://blog.csdn.net/ligh_sqh/article/details/81112428 # mongo# 启动 mongodb 客户端$ cd ~/software/adminmongo$ npm start# 客户端地址 http://localhost:1234 nacos 启动命令$ sh ～/software/nacos/bin/startup.sh -m standalone elasticsearch-head 安装和启动 -elasticsearch GUI客户端$ git clone git://github.com/mobz/elasticsearch-head.git$ cd elasticsearch-head$ npm install$ npm run start# http://localhost:9100 RocketMQhttps://rocketmq.apache.org/docs/quick-start/","tags":[{"name":"mac","slug":"mac","permalink":"https://yapengren.github.io/tags/mac/"}],"categories":[{"name":"系统","slug":"系统","permalink":"https://yapengren.github.io/categories/系统/"},{"name":"mac","slug":"系统/mac","permalink":"https://yapengren.github.io/categories/系统/mac/"}]},{"title":"token防表单重复提交","date":"2019-08-02T03:16:10.000Z","path":"wiki/token防表单重复提交/","text":"生成 token加载页面时候调用 initToken() 方法，生成 token 存入页面隐藏域中 &lt;%--隐藏域-防止表单的重复提交--%&gt;&lt;input type=&quot;hidden&quot; name=&quot;examToken&quot; id=&quot;examToken&quot;&gt;&lt;/input&gt; 表单提交表单提交的时候带上 token 方法添加注解springMVC 方法上添加注解 @TokenCheck(isCheckToken = true) 自定义注解自定义注解 TokenCheck.java // 注解会在class字节码文件中存在，在运行时可以通过反射获取到@Retention(RetentionPolicy.RUNTIME) // 定义注解的作用目标**作用范围字段、枚举的常量/方法@Target(&#123;ElementType.METHOD&#125;)// 说明该注解将被包含在javadoc中@Documentedpublic @interface TokenCheck &#123; /** * 是否需要校验 */ boolean isCheckToken() default false;&#125; 自定义注解拦截器springMVC 配置文件 spring-servlet.xml 添加 &lt;mvc:interceptors 拦截器 &lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=\"/**\" /&gt; &lt;bean class=\"com.ycx.exam.web.comm.interceptor.TokenCheckDuplicateSubmitInterceptor\"&gt;&lt;/bean&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 拦截器内容TokenCheckDuplicateSubmitInterceptor.java 在拦截器中判断TokenCheck注解isCheckToken是否为 true，如果为 true，则执行缓存校验； 先从 redis 缓存中获取到 token 集合，再从缓存中查询 token； 如果存在，则属于重复提交，返回； 如果不存在，则属于首次提交，将此token压入token集合中并将token集合放回redis中； public class TokenCheckDuplicateSubmitInterceptor extends HandlerInterceptorAdapter &#123; private static final Logger log = Logger.getLogger(TokenCheckDuplicateSubmitInterceptor.class); @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123;&#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123;&#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; WebApplicationContext context = WebApplicationContextUtils.getRequiredWebApplicationContext(request.getServletContext()); CacheService cacheService = (CacheService) context.getBean(\"cacheService\"); if (cacheService == null) &#123; log.warn(\"token拦截器校验重复提交，缓存service为空!\"); return true; &#125; if (handler instanceof HandlerMethod) &#123; HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); TokenCheck annotation = method.getAnnotation(TokenCheck.class); if (annotation != null) &#123; boolean isCheckToken = annotation.isCheckToken(); if (isCheckToken) &#123; boolean isExist = true; boolean isgetLock = getLock(cacheService, \"examTokenLock\", 1); if (isgetLock) &#123; isExist = checkTokenExist(request, cacheService); if (!isExist) &#123; log.warn(\"token拦截重复提交校验\" + method.getName() + \"重复提交!\"); &#125; &#125; cacheService.releaseLock(\"examTokenLock\"); return isExist; &#125; &#125; &#125; else &#123; try &#123; return super.preHandle(request, response, handler); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; return true; &#125; /** * @param cacheService * @param cacheKey * @param second 秒 * @return */ private boolean getLock(CacheService cacheService, String cacheKey, long second) &#123; boolean isgetLock = cacheService.getLock(cacheKey, \"1\", second); //如果没有获得锁，将默认进行三次锁的获取 if (!isgetLock) &#123; log.info(\"获得分布式锁失败，进入等待...........\"); for (int i = 0; i &lt;= 2; i++) &#123; isgetLock = cacheService.getLock(cacheKey, \"1\", second); try &#123; if (!isgetLock) &#123; Thread.sleep(1000); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (isgetLock) &#123; break; &#125; &#125; &#125; return isgetLock; &#125; /** * 缓存校验 * * @param request * @param cacheService * @return */ private boolean checkTokenExist(HttpServletRequest request, CacheService cacheService) &#123; String token = request.getParameter(\"examToken\"); if (StringUtils.isEmpty(token)) &#123; log.warn(\"token拦截器校验重复提交，页面提交过来token为空!\"); return true; &#125; token = token.trim(); LRULinkedHashMap&lt;String, String&gt; cmap = (LRULinkedHashMap&lt;String, String&gt;) cacheService.getCacheData(\"examTokenMap\"); if (cmap == null) &#123; int size = 131072; // 最近最少使用算法，linkedHashMap实现，主要是针对缓存过期策略实现 cmap = new LRULinkedHashMap&lt;String, String&gt;(size); &#125; else &#123; log.info(\"token缓存map存在，size=\" + cmap.size()); &#125; String ieExist = cmap.get(token); if (StringUtils.isEmpty(ieExist)) &#123; log.info(\"缓存不存在token=\" + token); String valu = cmap.put(token, \"1\"); cacheService.setCacheDataForType(\"examTokenMap\", cmap, 1, TimeUnit.HOURS); log.info(\"将token=\" + token + \"加入缓存成功!\"); return true; &#125; else &#123; log.info(\"将token=\" + token + \"已经存在，重复提交!\"); return false; &#125; &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"方案","slug":"java/方案","permalink":"https://yapengren.github.io/categories/java/方案/"}]}]}