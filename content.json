{"pages":[{"title":"404","date":"2019-07-19T08:41:10.000Z","path":"404.html","text":"你来到了没有知识的荒原 :("},{"title":"分类","date":"2019-08-18T03:32:37.474Z","path":"categories/index.html","text":""},{"title":"关于","date":"2019-09-20T00:53:56.238Z","path":"about/index.html","text":"Email : yp_ren@126.com GitHub：https://github.com/yapengren"},{"title":"标签","date":"2019-08-02T05:00:35.984Z","path":"tags/index.html","text":""}],"posts":[{"title":"java判断对象中属性值是否全为空","date":"2019-10-24T07:29:46.000Z","path":"wiki/java判断对象中属性值是否全为空/","text":"/** * 判断对象中属性值是否全为空 * * @param object * @return */public static boolean checkObjAllFieldsIsNull(Object object) throws IllegalAccessException &#123; if (null == object) &#123; return true; &#125; for (Field f : object.getClass().getDeclaredFields()) &#123; f.setAccessible(true); if (f.get(object) != null &amp;&amp; StringUtils.isNotBlank(f.get(object).toString())) &#123; return false; &#125; &#125; return true;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"基础","slug":"java/基础","permalink":"https://yapengren.github.io/categories/java/基础/"}]},{"title":"SpringBoot中间件","date":"2019-10-23T06:40:41.000Z","path":"wiki/SpringBoot中间件/","text":"Spring Boot 和 Redis 常用操作spring-boot-starter-data-redisSpring Boot 提供了对 Redis 集成的组件包：spring-boot-starter-data-redis，它依赖于 spring-data-redis 和 lettuce。Spring Boot 1.0 默认使⽤的是 Jedis 客户端，2.0 替换成了 Lettuce，但如果你从 Spring Boot 1.5.X 切换过来，⼏乎感受不⼤差异，这是因为 spring-boot-starter-data-redis 为我们隔离了其中的差异性。 Lettuce：是⼀个可伸缩线程安全的 Redis 客户端，多个线程可以共享同⼀个 RedisConnection，它利⽤优秀 Netty NIO 框架来⾼效地管理多个连接。 Spring Data：是 Spring 框架中的⼀个主要项⽬，⽬的是为了简化构建基于 Spring 框架应⽤的数据访问，包括⾮关系数据库、Map-Reduce 框架、云数据服务等，另外也包含对关系数据库的访问⽀持。 Spring Data Redis：是 Spring Data 项⽬中的⼀个主要模块，实现了对 Redis 客户端 API 的⾼度封装，使对 Redis 的操作更加便捷。 可以⽤以下⽅式来表达它们之间的关系： Lettuce ➡️ Spring Data Redis ➡️ Spring Data ➡️ spring-boot-starter-data-redis 相关配置引入依赖包&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--Lettuce 需要使⽤ commons-pool 2 创建 Redis 连接池--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; application 配置# Redis 数据库索引（默认为 0）spring.redis.database=0# Redis 服务器地址spring.redis.host=localhost# Redis 服务器连接端⼝spring.redis.port=6379# Redis 服务器连接密码（默认为空）spring.redis.password=# 连接池最⼤连接数（使⽤负值表示没有限制） 默认 8spring.redis.lettuce.pool.max-active=8# 连接池最⼤阻塞等待时间（使⽤负值表示没有限制） 默认 -1spring.redis.lettuce.pool.max-wait=-1# 连接池中的最⼤空闲连接 默认 8spring.redis.lettuce.pool.max-idle=8# 连接池中的最⼩空闲连接 默认 0spring.redis.lettuce.pool.min-idle=0 缓存配置在这⾥可以为 Redis 设置⼀些全局配置，⽐如配置主键的⽣产策略 KeyGenerator，如不配置会默认使⽤参数名作为主键。 @Configuration@EnableCaching//@EnableCaching 来开启缓存public class RedisConfig extends CachingConfigurerSupport &#123; @Override @Bean public KeyGenerator keyGenerator() &#123; return new KeyGenerator() &#123; @Override public Object generate(Object target, Method method, Object... params) &#123; StringBuilder sb = new StringBuilder(); sb.append(target.getClass().getName()); sb.append(method.getName()); for (Object obj : params) &#123; sb.append(obj.toString()); &#125; return sb.toString(); &#125; &#125;; &#125;&#125; 测试使用@SpringBootTestpublic class TestRedisTemplate &#123; @Autowired private RedisTemplate redisTemplate; @Test public void testString() &#123; redisTemplate.opsForValue().set(\"name\", \"renyapeng\"); &#125;&#125;","tags":[{"name":"springBoot","slug":"springBoot","permalink":"https://yapengren.github.io/tags/springBoot/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"springBoot","slug":"框架/springBoot","permalink":"https://yapengren.github.io/categories/框架/springBoot/"}]},{"title":"mac系统快捷键","date":"2019-10-05T02:58:45.000Z","path":"wiki/mac系统快捷键/","text":"快捷键^ A Home ^ E End ^ H 删除 ^ D Delete ^ K 删除到结尾 ^ P Previous⬆️ ^ B ^ F Backward⬅️ Forward➡️ ^ N Next⬇️ Mac 查看端口占用及杀死进程$ lsof -i:5601COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnode 29152 renyapeng 23u IPv4 0x5ff129ce84c9eb1d 0t0 TCP localhost:esmagent (LISTEN)$ kill -9 29152[2] + 29152 killed kibana","tags":[{"name":"mac","slug":"mac","permalink":"https://yapengren.github.io/tags/mac/"}],"categories":[{"name":"系统","slug":"系统","permalink":"https://yapengren.github.io/categories/系统/"},{"name":"mac","slug":"系统/mac","permalink":"https://yapengren.github.io/categories/系统/mac/"}]},{"title":"linux 基本命令","date":"2019-09-22T05:57:36.000Z","path":"wiki/linux-基本命令/","text":"前言可以使用 man 命令来查看各个命令的使用文档，如：man cp 文件基本属性Linux中我们可以使用 ll 或者 ls –l 命令来显示一个文件的属性以及文件所属的用户和组 $ lltotal 2184-rw-r--r-- 1 renyapeng staff 846K 9 22 16:08 db.jsondrwxr-xr-x 315 renyapeng staff 9.8K 8 12 14:07 node_modulesdrwxr-xr-x 21 renyapeng staff 672B 9 22 15:28 public 每个文件的属性由左边第一部分的 10 个字符来确定（如下图） 10位字符表示： 0位：确定文件类型 1-3位：确定该文件的所有者对文件的权限 owner 4-6位：确定所有者的同组用户拥有该文件的权限 group 7-9位：确定其他用户拥有该文件的权限 others 第一个字符：代表这个文件的类型，是目录、文件，还是一个链接等等 [ d ] 目录 [ - ] 文件 [ l ] 链接文档（link file） [ b ] 可供储存的接口设备(可随机存取装置) [ c ] 串行端口设备，例如键盘、鼠标（一次性读取装置） 接下来的字符：以三个一组分成三组，用 r、w、x 三个参数的组合表示，位置不会改变 [ r ] 代表可读（read） [ w ] 代表可写（write） [ x ] 代表可执行（execute） [ - ] 没有权限 文件与目录管理mkdir 创建目录mkdir (make directory) 语法： mkdir [-p] 目录名称 参数： -m ：配置文件的权限-p ：将所需要的目录（包含上一级目录）递归创建起来 实例： $ mkdir test // 创建新目录$ mkdir test1/test2/test3/test4mkdir: test1/test2/test3: No such file or directory // 不能创建多层目录$ mkdir -p test1/test2/test3/test4 // 添加-p参数，可以创建多层目录 cp 复制文件或文件夹语法： cp [-adfilprsu] 来源档（source）目标档（destination） 参数： -a ：相当于 -pdr 的意思，至于 pdr 请参考下列说明（常用）-d ：若来源档为连结档的属性（link file），则复制连结档属性而非文件本身-f ：为强制（force）的意思，若目标文件已经存在且无法开启，则移除后再尝试一次-i ：若目标档（destination）已经存在时，在覆盖时会先询问动作的进行（常用）-l ：进行硬式连结（hard link）的连结档创建，而非复制文件本身-p ：连同文件的属性一起复制过去，而非使用默认属性（备份常用）-r ：递归持续复制，用于目录的复制行为（常用）-s ：复制成为符号连结档（symbolic link），亦即「捷径」文件-u ：若 destination 比 source 旧才升级 destination 实例： $ cp -r test1 bashrc // 复制文件夹并重命名 rm 移除文件或目录rm (remove) 语法： rm [-fir] 文件或目录 参数： -f ：就是 force 的意思，忽略不存在的文件，不会出现警告信息-i ：互动模式，在删除前会询问使用者是否动作-r ：递归删除啊！最常用在目录的删除了！这是非常危险的选项！！！ 实例： $ rm -ir test1 // 删除之前询问使用者是否动作examine files in directory test1? y mv 移动文件与目录或重命名语法： mv [-fiu] source destinationmv [options] source1 source2 source3 .... directory 参数： -f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖-i ：若目标文件（destination）已经存在时，就会询问是否覆盖-u ：若目标文件已经存在，且 source 比较新，才会升级（update） 实例： $ mv test1 test2 touch 新建文件语法： touch 实例： $ touch file // 创建一个名为“file”的新空白文件 文件内容查看Linux 系统中使用以下命令来查看文件的内容： cat 由第一行开始显示文件内容 tac 从最后一行开始显示，可以看出 tac 是 cat 的倒着写 nl 显示的时候，顺道输出行号 more 一页一页的显示文件内容 less 与 more 类似，但是可以往前翻页 head 只看头几行 tail 只看尾几行 vimvim工作模式 vim键盘图 vim常用按键系统管理psprocess kill语法： kill [-s &lt;信息名称或编号&gt;][程序] 或 kill [-l &lt;信息编号&gt;] 参数： 实例： $ kill 12345 // 杀死进程$ kill -KILL 123456 // 强制杀死进程$ kill -9 123456 // 彻底杀死进程 killall文件管理filefind搜索指定文件|文件夹 find 路径 -name 名称 删除空文件|空文件夹 find 路径 -type d -empty|xargs -n 1 rm -rf 文档编辑grep备份压缩tar 参考 https://www.runoob.com/linux/linux-tutorial.html","tags":[{"name":"linux","slug":"linux","permalink":"https://yapengren.github.io/tags/linux/"}],"categories":[{"name":"系统","slug":"系统","permalink":"https://yapengren.github.io/categories/系统/"},{"name":"linux","slug":"系统/linux","permalink":"https://yapengren.github.io/categories/系统/linux/"}]},{"title":"java.util.ConcurrentModificationException 异常详解","date":"2019-09-20T02:11:57.000Z","path":"wiki/java-util-ConcurrentModificationException-异常详解/","text":"环境：JDK 1.8.0 使用 iterator 遍历集合的同时对集合进行修改会报错 java.util.ConcurrentModificationException 问题复现抛异常代码 @Testpublic void test02() &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 1; i &lt; 11; i++) &#123; list.add(i); &#125; Iterator&lt;Integer&gt; it = list.iterator(); while (it.hasNext()) &#123; Integer next = it.next(); if (next == 5) &#123; list.remove(next); &#125; &#125;&#125; 异常信息 java.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) at java.util.ArrayList$Itr.next(ArrayList.java:859) 原因分析使用 Iterator 遍历 ArrayList， 抛出异常的是 iterator.next()，查看 ArrayList 的 Iterator 实现源码 public E next() &#123; checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i];&#125; 在 next() 方法中有一个 checkForComodification() 方法 final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException();&#125; 可以看到 checkForComodification 方法是用来判断集合的修改次数是否合法。 modCount 字段用于记录集合被修改的次数，ArrayList 增删改 (add, remove, set) 时都会自增 modCount 属性。 在创建 Iterator 的时候会将 modCount 赋值给 expectedModCount，同样记录当前集合修改的次数，初始化为集合的 modCount 值。 抛异常代码中 ArrayList 添加了 10 次所以 modCount = 10；创建 Iterator 时候 expectedModCount = 10；遍历到 next == 5 时执行了 list.remove(next)，此时 modCount = 11, expectedModCount = 10; 在执行 next 方法时，判断 modCount != expectedModCount ，导致抛出异常 java.util.ConcurrentModificationException。 为什么要这么做呢？引用一段解释 Iterator 是工作在一个独立的线程中，并且拥有一个 mutex 锁。 Iterator 被创建之后会建立一个指向原来对象的单链索引表，当原来的对象数量发生变化时，这个索引表的内容不会同步改变，所以当索引指针往后移动的时候就找不到要迭代的对象，所以按照 fail-fast 原则 Iterator 会马上抛出 java.util.ConcurrentModificationException 异常。 所以 Iterator 在工作的时候是不允许被迭代的对象被改变的。但你可以使用 Iterator 本身的方法 remove() 来删除对象， Iterator.remove() 方法会在删除当前迭代对象的同时维护索引的一致性。 解决方法使用 Iterator 本身的方法 remove() 来删除对象 正确代码 @Testpublic void test02() &#123; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 1; i &lt; 11; i++) &#123; list.add(i); &#125; Iterator&lt;Integer&gt; it = list.iterator(); while (it.hasNext()) &#123; Integer next = it.next(); if (next == 5) &#123; // 使用 Iterator 本身的方法 remove() 来删除对象 it.remove(); &#125; &#125;&#125; 参考 https://www.cnblogs.com/xujian2014/p/5846128.html https://www.cnblogs.com/snowater/p/8024776.html https://www.iteye.com/blog/lz12366-675016","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"基础","slug":"java/基础","permalink":"https://yapengren.github.io/categories/java/基础/"}]},{"title":"Mybatis 批量操作报错 BadSqlGrammarException","date":"2019-09-16T07:10:48.000Z","path":"wiki/Mybatis-批量操作报错-BadSqlGrammarException/","text":"问题Mybatis 使用&lt;foreach&gt;批量操作时，对应的 sql 文件为 &lt;update id=\"testBatchUpd\" parameterType=\"java.util.Map\"&gt; &lt;foreach collection=\"dmsProdSkuStocks\" item=\"item\" separator=\";\"&gt; UPDATE dms_prod_sku_stock SET STOCK = #&#123;item.stock&#125; WHERE SKU_RELATE_ID = #&#123;item.skuRelateId&#125; &lt;/foreach&gt;&lt;/update&gt; 调用接口执行的时候，程序就会报错 org.springframework.jdbc.BadSqlGrammarException 原因用 Mybatic 批量操作必须加上参数 &amp;allowMultiQueries=true 解决方案添加参数 &amp;allowMultiQueries=true community.jdbc.url=jdbc:mysql://172.25.28.8:3306/jr_community?createDatabaseIfNotExist=true&amp;amp;characterEncoding=utf-8&amp;amp;useUnicode=true&amp;zeroDateTimeBehavior=convertToNull&amp;allowMultiQueries=truecommunity.jdbc.username=community.jdbc.password= 参数意思是允许多查询","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://yapengren.github.io/tags/Mybatis/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"Mybatis","slug":"框架/Mybatis","permalink":"https://yapengren.github.io/categories/框架/Mybatis/"}]},{"title":"lombok 使用方法","date":"2019-09-07T08:24:59.000Z","path":"wiki/lombok-使用方法/","text":"@Data注解在类上，提供类所有属性的 getter 和 setter 方法，此外还提供了equals、canEqual、hashCode、toString 方法、无参构造方法； @Accessors(chain = true)注解在类上，使用链式设置属性； @Setter注解在属性上，为单个属性提供 setter 方法、无参构造方法； 注解在类上，为该类所有的属性提供 setter 方法、无参构造方法； @Getter注解在属性上，为单个属性提供 getter 方法、无参构造方法； 注解在类上，为该类所有的属性提供 getter 方法、无参构造方法； @Log4j注解在类上，为类提供一个 属性名为 log 的 log4j 日志对象，提供无参构造方法； @AllArgsConstructor注解在类上，为类提供一个全参构造方法，加了这个注解后，类中不提供无参构造方法； @NoArgsConstructor注解在类上，为类提供一个无参构造方法； @EqualsAndHashCode注解在类上，可以生成 equals、canEqual、hashCode 方法； @NonNull注解在属性上，会自动产生一个关于此参数的非空检查，如果参数为空，则抛出一个空指针异常，也会有一个无参构造方法； @ToString注解在类上，可以生成所有参数的 toString 方法，还会无参构造方法； @Value注解在类上，会生成全参构造方法，getter 方法，此外还提供了equals、hashCode、toString 方法； @Synchronized这个注解用在类方法或者实例方法上，效果和 synchronized 关键字相同，区别在于锁对象不同，对于类方法和实例方法，synchronized 关键字的锁对象分别是类的 class 对象和 this 对象，而 @Synchronized 的锁对象分别是 私有静态 final 对象 lock 和 私有 final 对象 lock，当然，也可以自己指定锁对象，此外也提供无参构造方法；","tags":[{"name":"spring","slug":"spring","permalink":"https://yapengren.github.io/tags/spring/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"spring","slug":"框架/spring","permalink":"https://yapengren.github.io/categories/框架/spring/"}]},{"title":"冒泡排序","date":"2019-09-03T06:34:04.000Z","path":"wiki/冒泡排序/","text":"基本介绍冒泡排序（Bubble Sort）需要重复的走访要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来，使值较大的元素逐渐从前移向后边。 算法描述冒泡排序算法的运作如下： 比较相邻的元素。如果第一个比第二个大，就交换 对每一对相邻元素做同样的工作，从开始第一对到结尾的最后一对。这步做完后，最后的元素会是最大的数 针对所有的元素重复以上的步骤，除了最后一个 持续每次对越来越少的元素重复上面的步骤 1 ～ 3，直到没有任何一对数字需要比较 代码实现private static void bubbleSort(int[] arr) &#123; // 临时变量 int temp = 0; // 标识变量，表示是否进行过交换 boolean flag = false; for (int i = 0; i &lt; arr.length - 1; i++) &#123; for (int j = 0; j &lt; arr.length - 1 - i; j++) &#123; // 如果前面的数比后面的数大，则交换 if (arr[j] &gt; arr[j + 1]) &#123; flag = true; temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; &#125; &#125; // 在一趟排序中，一次交换都没有发生过 if (!flag) &#123; break; &#125; else &#123; flag = false; &#125; &#125;&#125; 参考 http://cmsblogs.com/?p=4697","tags":[{"name":"算法","slug":"算法","permalink":"https://yapengren.github.io/tags/算法/"}],"categories":[{"name":"算法","slug":"算法","permalink":"https://yapengren.github.io/categories/算法/"},{"name":"排序","slug":"算法/排序","permalink":"https://yapengren.github.io/categories/算法/排序/"}]},{"title":"稀疏数组sparsearray","date":"2019-09-01T06:37:41.000Z","path":"wiki/稀疏数组sparsearray/","text":"需求编写的五子棋程序中，有存盘退出和续上盘的功能。 分析问题因为该二维数组的很多值是默认值 0, 因此记录了很多没有意义的数据 稀疏数组基本介绍当一个数组中大部分元素为０，或者为同一个值的数组时，可以使用稀疏数组来保存该数组。 稀疏数组处理方法 记录数组一共有几行几列，有多少个不同的值 把具有不同值的元素的行列及值记录在一个小规模的数组中，从而缩小程序的规模 稀疏数组举例说明 应用实例 使用稀疏数组，来保留类似前面的二维数组(棋盘、地图等等) 把稀疏数组存盘，并且可以重新恢复原来的二维数组数 整体思路分析 原始的二维数组 –&gt; 稀疏数组的思路 遍历原始的二维数组，得到有效数据的个数 sum 根据 sum 可以创建稀疏数组 sparseArr int[sum + 1][3] 将二维数组的有效数据存入到稀疏数组 稀疏数组 –&gt; 原始的二维数组的思路 先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如 int[][] chessArr = new int[11][11] 在读取稀疏数组后几行的数据，并赋给原始的二维数组 代码实现public class SparseArray &#123; public static void main(String[] args) &#123; // 创建原始的二维数组 11 * 11 // 0表示没有棋子，1表示黑棋，2表示蓝棋 int[][] chessArr = new int[11][11]; chessArr[1][2] = 1; chessArr[2][3] = 2; // 输出原始的二维数组 System.out.println(\"原始的二维数组----------\"); for (int[] ints : chessArr) &#123; for (int data : ints) &#123; System.out.printf(\"%d\\t\", data); &#125; System.out.println(); &#125; // 原始的二维数组 --&gt; 稀疏数组的思路 // 1、遍历二维数组，得到非 0 数据的个数 int sum = 0; for (int i = 0; i &lt; 11; i++) &#123; for (int j = 0; j &lt; 11; j++) &#123; if (chessArr[i][j] != 0) &#123; sum++; &#125; &#125; &#125; // 2、创建对应的稀疏数组 int[][] sparseArr = new int[sum + 1][3]; sparseArr[0][0] = 11; sparseArr[0][1] = 11; sparseArr[0][2] = sum; // 3、遍历二维数组，将非 0 数据存入 sparseArr // count 用于记录是第几个非 0 数据 int count = 0; for (int i = 0; i &lt; 11; i++) &#123; for (int j = 0; j &lt; 11; j++) &#123; if (chessArr[i][j] != 0) &#123; count++; sparseArr[count][0] = i; sparseArr[count][1] = j; sparseArr[count][2] = chessArr[i][j]; &#125; &#125; &#125; // 输出稀疏数组 System.out.println(); System.out.println(\"稀疏数组----------\"); for (int i = 0; i &lt; sparseArr.length; i++) &#123; System.out.printf(\"%d\\t%d\\t%d\\t\\n\", sparseArr[i][0], sparseArr[i][1], sparseArr[i][2]); &#125; System.out.println(); // 稀疏数组 --&gt; 原始的二维数组的思路 // 1. 先读取稀疏数组的第一行，根据第一行的数据，创建原始的二维数组，比如 int[][] chessArr = new int[11][11] int[][] chessArr2 = new int[sparseArr[0][0]][sparseArr[0][1]]; // 2. 在读取稀疏数组后几行的数据，并赋给原始的二维数组 for (int i = 1; i &lt; sparseArr.length; i++) &#123; chessArr2[sparseArr[i][0]][sparseArr[i][1]] = sparseArr[i][2]; &#125; // 输出恢复后的二维数组 System.out.println(); System.out.println(\"恢复后的二维数组----------\"); for (int[] ints : chessArr2) &#123; for (int data : ints) &#123; System.out.printf(\"%d\\t\", data); &#125; System.out.println(); &#125; &#125;&#125;","tags":[{"name":"算法","slug":"算法","permalink":"https://yapengren.github.io/tags/算法/"}],"categories":[{"name":"算法","slug":"算法","permalink":"https://yapengren.github.io/categories/算法/"},{"name":"数组","slug":"算法/数组","permalink":"https://yapengren.github.io/categories/算法/数组/"}]},{"title":"java 基础题","date":"2019-09-01T01:38:39.000Z","path":"wiki/java-基础题/","text":"自增变量@Testpublic void test01() &#123; int i = 1; i = i++; int j = i++; int k = i + ++i * i++; System.out.println(\"i=\" + i); System.out.println(\"j=\" + j); System.out.println(\"k=\" + k);&#125; 输出结果： i=4j=1k=11 i++：先使用再 +1 ++i：先 +1 再使用 初始化和实例初始化public class Father &#123; private int i = test(); private static int j = method(); static &#123; System.out.println(\"Father 静态代码块\"); &#125; public Father() &#123; System.out.println(\"Father 构造方法\"); &#125; &#123; System.out.println(\"Father 实例代码块\"); &#125; public int test() &#123; System.out.println(\"Father 普通方法\"); return 1; &#125; public static int method() &#123; System.out.println(\"Father 静态方法\"); return 1; &#125;&#125; public class Son extends Father&#123; private int i = test(); private static int j = method(); static &#123; System.out.println(\"Son 静态代码块\"); &#125; public Son() &#123; System.out.println(\"Son 构造方法\"); &#125; &#123; System.out.println(\"Son 实例代码块\"); &#125; @Override public int test() &#123; System.out.println(\"Son 普通方法\"); return 1; &#125; public static int method() &#123; System.out.println(\"Son 静态方法\"); return 1; &#125; public static void main(String[] args) &#123; Son s1 = new Son(); System.out.println(\"-----\"); Son s2 = new Son(); &#125;&#125; 输出结果： Father 静态方法Father 静态代码块Son 静态方法Son 静态代码块Son 普通方法Father 实例代码块Father 构造方法Son 普通方法Son 实例代码块Son 构造方法-----Son 普通方法Father 实例代码块Father 构造方法Son 普通方法Son 实例代码块Son 构造方法 方法的参数传递机制public class Exam4 &#123; public static void main(String[] args) &#123; int i = 1; String str = \"hello\"; Integer num = 200; int[] arr = &#123;1, 2, 3, 4, 5&#125;; MyData my = new MyData(); change(i, str, num, arr, my); System.out.println(\"i= \" + i); System.out.println(\"str= \" + str); System.out.println(\"num= \" + num); System.out.println(\"arr= \" + Arrays.toString(arr)); System.out.println(\"my.a= \" + my.a); &#125; public static void change(int j, String s, Integer n, int[] a, MyData m) &#123; j += 1; s += \"world\"; n += 1; a[0] += 1; m.a += 1; &#125;&#125;class MyData &#123; int a = 10;&#125; 输出结果： i= 1str= hellonum= 200arr= [2, 2, 3, 4, 5]my.a= 11","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"基础","slug":"java/基础","permalink":"https://yapengren.github.io/categories/java/基础/"}]},{"title":"数据库知识点","date":"2019-08-27T09:58:46.000Z","path":"wiki/数据库知识点/","text":"数据库三范式第一范式(1st NF－列都是不可再分)第一范式的目标是确保每列的原子性:如果每列都是不可再分的最小数据单元（也称为最小的原子单元），则满足第一范式（1NF） 第二范式(2nd NF－每个表只描述一件事情)首先满足第一范式，并且表中非主键列不存在对主键的部分依赖。 第二范式要求每个表只描述一件事情。 第三范式(3rd NF－不存在对非主键列的传递依赖)第三范式定义是，满足第二范式，并且表中的列不存在对非主键列的传递依赖。除了主键订单编号外，顾客姓名依赖于非主键顾客编号。 SQLMySQL 中FIND_IN_SET()和IN区别简析MySQL Explain详解MySQL中concat以及group_concat的使用MySQL中order by语句对null字段的排序SQL练习MySQL经典练习题及答案，常用SQL语句练习50题","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"待整理","slug":"待整理","permalink":"https://yapengren.github.io/categories/待整理/"}]},{"title":"数据库事务","date":"2019-08-27T09:20:07.000Z","path":"wiki/数据库事务/","text":"数据库事务概念什么是数据库事务事务（transaction）是指逻辑上对数据的一组操作， 这组操作要么全部成功，要么全部失败，是不可分割的一个工作单位。 数据库事务的 4 个基本性质（ACID）原子性（Atomicity）事务的原子性是指事务是一个不可分割的工作单位，这组操作要么全部成功，要么全部失败。 一致性（Consistency）在事务开始以前，被操作的数据的完整性处于一致性的状态，事务结束后，被操作的数据的完整性也必须处于一致性状态。 拿银行转账来说，一致性要求事务的执行不应改变A、B 两个账户的金额总和。如果没有这种一致性要求，转账过程中就会发生钱无中生有，或者不翼而飞的现象。事务应该把数据库从一个一致性状态转换到另外一个一致性状态。 隔离性（Isolation）事务隔离性要求系统必须保证事务不受其他并发执行的事务的影响，也即要达到这样一种效果：对于任何一对事务T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前已经结束，要么在 T1 完成之后才开始执行。这样，每个事务都感觉不到系统中有其他事务在并发地执行。 持久性（Durability）一个事务一旦成功提交，它对数据库的改变必须是永久的，即便是数据库发生故障也应该不回对其产生任何影响。 数据库事务的 4 种隔离级别Read uncommitted读未提交，顾名思义，就是一个事务可以读取另一个未提交事务的数据。 事例：老板要给程序员发工资，程序员的工资是 3.6 万/月。但是发工资时老板不小心按错了数字，按成 3.9 万/月，该钱已经打到程序员的户口，但是事务还没有提交，就在这时，程序员去查看自己这个月的工资，发现比往常多了 3 千元，以为涨工资了非常高兴。但是老板及时发现了不对，马上回滚差点就提交了的事务，将数字改成 3.6 万再提交。 分析：实际程序员这个月的工资还是 3.6 万，但是程序员看到的是 3.9 万。他看到的是老板还没提交事务时的数据，这就是脏读。 那怎么解决脏读呢？Read committed！读提交，能解决脏读问题。 Read committed读提交，顾名思义，就是一个事务要等另一个事务提交后才能读取数据。 事例：程序员拿着信用卡去享受生活（卡里当然是只有 3.6 万），当他埋单时（程序员事务开启），收费系统事先检测到他的卡里有 3.6 万，就在这个时候！！程序员的妻子要把钱全部转出充当家用，并提交。当收费系统准备扣款时，再检测卡里的金额，发现已经没钱了（第二次检测金额当然要等待妻子转出金额事务提交完）。程序员就会很郁闷，明明卡里是有钱的… 分析：这就是读提交，若有事务对数据进行更新 UPDATE 操作时，读操作事务要等待这个更新操作事务提交后才能读取数据，可以解决脏读问题。但在这个事例中，出现了一个事务范围内两个相同的查询却返回了不同数据，这就是不可重复读。 那怎么解决可能的不可重复读问题？Repeatable read ！ Repeatable read重复读，就是在开始读取数据（事务开启）时，不再允许修改操作 事例：程序员拿着信用卡去享受生活（卡里当然是只有 3.6 万），当他埋单时（事务开启，不允许其他事务的 UPDATE 修改操作），收费系统事先检测到他的卡里有 3.6 万。这个时候他的妻子不能转出金额了。接下来收费系统就可以扣款了。 分析：重复读可以解决不可重复读问题。写到这里，应该明白的一点就是，不可重复读对应的是修改，即 UPDATE 操作。但是可能还会有幻读问题。因为幻读问题对应的是插入 INSERT 操作，而不是 UPDATE 操作。 什么时候会出现幻读？ 事例：程序员某一天去消费，花了 2 千元，然后他的妻子去查看他今天的消费记录（全表扫描FTS，妻子事务开启），看到确实是花了 2 千元，就在这个时候，程序员花了 1 万买了一部电脑，即新增 INSERT 了一条消费记录，并提交。当妻子打印程序员的消费记录清单时（妻子事务提交），发现花了1.2万元，似乎出现了幻觉，这就是幻读。 那怎么解决幻读问题？Serializable！ Serializable 序列化Serializable 是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。 参考 https://blog.csdn.net/mfl0315/article/details/51981792 https://blog.csdn.net/qq_33290787/article/details/51924963","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"MySQL 索引","date":"2019-08-25T03:44:17.000Z","path":"wiki/MySQL-索引/","text":"索引优化索引失效案例 全值匹配 最佳左前缀法则。如果索引了多列，要遵守最左前缀法则，指的是查询从索引的最左前列开始并且不跳过索引中的列 不在索引列上做任何操作（计算、函数、「自动或者手动」类型转换），会导致索引失效而转向全表扫描 存储引擎不能使用索引中范围条件右边的列 尽量使用覆盖索引（只访问索引的查询「索引列和查询列一致」），减少 select * MySQL 在使用 != 或者 &lt;&gt; 的时候无法使用索引会导致全表扫描 is null，is not null 无法使用索引 like 以通配符开头 (‘%abc…’) MySQL 索引失效会变成全表扫描的操作 字符串不加单引号索引失效 少用 or，用它来连接时会索引失效 小总结假设 index(a, b, c) Where 语句 索引是否被使用 where a = 3 Y，使用到 a where a = 3 and b = 5 Y，使用到 a，b where a = 3 and b = 5 and c = 4 Y，使用到 a，b，c where b = 3 或者 where b = 3 and c = 4 或者 where c = 4 N where a = 3 and c = 5 使用到 a，但是 c 不可以，b 中间断了 where a = 3 and b &gt; 4 and c = 5 使用到 a 和 b，c 不能用在范围之后，b 断了 where a = 3 and b like ‘kk%’ and c = 4 Y，使用到 a，b，c where a = 3 and b like ‘%kk’ and c = 4 Y，只用到 a where a = 3 and b like ‘%kk%’ and c = 4 Y，只用到 a where a = 3 and b like ‘k%kk%’ and c = 4 Y，只用到 a，b，c 练习题# 创建表create table test03( id int primary key not null auto_increment, c1 char(10), c2 char(10), c3 char(10), c4 char(10), c5 char(10));# 插入数据insert into test03(c1,c2,c3,c4,c5)values(&apos;a1&apos;,&apos;a2&apos;,&apos;a3&apos;,&apos;a4&apos;,&apos;a5&apos;);insert into test03(c1,c2,c3,c4,c5)values(&apos;b1&apos;,&apos;b2&apos;,&apos;b3&apos;,&apos;b4&apos;,&apos;b5&apos;);insert into test03(c1,c2,c3,c4,c5)values(&apos;c1&apos;,&apos;c2&apos;,&apos;c3&apos;,&apos;c4&apos;,&apos;c5&apos;);insert into test03(c1,c2,c3,c4,c5)values(&apos;d1&apos;,&apos;d2&apos;,&apos;d3&apos;,&apos;d4&apos;,&apos;d5&apos;);insert into test03(c1,c2,c3,c4,c5)values(&apos;e1&apos;,&apos;e2&apos;,&apos;e3&apos;,&apos;e4&apos;,&apos;e5&apos;);# 创建索引create index idx_test03_c1234 on test03(c1,c2,c3,c4); 问题：我们创建了复合索引idx_test03_c1234 ，根据以下SQL分析索引使用情况？ explain select * from test03 where c1=’a1’ and c2=’a2’ and c3=’a3’ and c4=’a4’;索引全部使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c4=’a4’ and c3=’a3’;索引全部使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c3&gt;’a3’ and c4=’a4’;索引 c1c2c3 使用，c4 未使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c4&gt;’a4’ and c3=’a3’;索引全部使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c4=’a4’ order by c3;索引 c1c2 使用，c3 作用在排序而不是查找，c4 不使用 explain select * from test03 where c1=’a1’ and c2=’a2’ order by c3;索引 c1c2 使用 c3 作用在排序而不是查找，和 5 题一样 explain select * from test03 where c1=’a1’ and c2=’a2’ order by c4;索引 c1c2 使用 ，但出现 Using filesort explain select * from test03 where c1=’a1’ and c5=’a5’ order by c2,c3;索引 c1 使用，但是 c2，c3用于排序，无 filesort explain select * from test03 where c1=’a1’ and c5=’a5’ order by c3,c2;索引 c1 使用，出现 filesort，我们建立索引 1234，它没有按照顺序来，3 2 颠倒了 explain select * from test03 where c1=’a1’ and c2=’a2’ order by c2,c3;索引 c1c2 使用 explain select * from test03 where c1=’a1’ and c2=’a2’ and c5=’a5’ order by c2,c3;索引 c1c2 使用，但是 c2、c3 用于排序，无 filesort explain select * from test03 where c1=’a1’ and c2=’a2’ and c5=’a5’ order by c3,c2;本例有常量 c2 的情况，和 8.2 对比，无filesort explain select * from test03 where c1=’a1’ and c4=’a4’ order by c2,c3;索引 c1 使用 explain select * from test03 where c1=’a1’ and c4=’a4’ order by c3,c2;索引 c1 使用, 出现了 Using where; Using temporary; Using filesort InnoDB 聚簇索引和非聚簇索引每个 InnoDB 表都有一个称为 「 聚簇索引 」 的特殊索引，通常情况下，这个聚簇索引就是 「 主键 」 ( primary key ) 。Innodb 使用它存储表中每一行的数据。 如果想要从查询、插入和其它数据库操作中获得最佳性能，那么我们就必须了解 InnoDB 如何使用 「 聚簇索引 」 来优化每个表的最常见检索和 DML 操作方式 当我们在一个 Innodb 表上定义了一个主键，InnoDB 会默认的使用它作为聚簇索引。 使用 InnoDB 存储引擎时，建议为每个表都添加一个主键。如果该表没有一个逻辑唯一且非空列或列集合，那么可以添加一个带有 AUTO_INCREMENT 约束的自增列作为主键，InnoDB 会自动填充该列。 如果某个 InnoDB 表并没有定义主键。那么 InnoDB 会查找第一个 「 唯一索引 」( UNIQUE Index ) ，因为唯一索引的所有键 ( key ) 都是 NOT ，因此可以用来作为聚簇索引。 如果某个 InnoDB 表既没有定义主键，也没有一个合适的唯一索引。InnoDB 会在内部生成一个名为 GEN_CLUST_INDEX 的隐式的聚簇索引 该聚簇索引的键 ( key ) 会包含一个自动为行生成的 ID 值 ( 行号 ) 。 该表中的所有行会按 InnoDB 分配给此类表中的行的 ID 排序。 行 ID 是一个 6 字节的字段，在插入新行时会单调自增。 因此，可以认为物理上的行保存顺序就是该行 ID 排序的排序顺序 聚簇索引如何加快查询速度通过聚簇索引访问行很快，因为索引搜索直接指向包含所有行数据页 ( data page )。 如果表很大，与那种索引页与数据页分离的 MyISAM 存储引擎相比， 聚簇索引体系结构通常可以节省磁盘 I/O 操作。 非聚簇索引和聚簇索引的关系非聚簇索引，通常也称之为 「 二级索引 」 ( Secondary Indexes ) 或 「 辅助索引 」 ，一般是指聚簇索引之外的所有其它的索引。 在 InnoDB 中，每个辅助索引中的每条记录都会包含该行的主键列 ( 也就是聚簇索引的键 ) ，以及为辅助索引指定的列。InnoDB 使用此主键值来搜索聚簇索引中的行。 如果主键很长，那么辅助索引就会占用更多空间，因此使用短主键是有利的，也是我们所推荐的。 聚簇索引和非聚簇索引的区别 首先，我们要认识到聚簇索引和非聚簇索引的划分依据是什么？ 答案就是 InnoDB 会使用聚簇索引来保存数据，而非聚簇索引的目的仅仅是加快查询速度 在第一点认知基础上，我们就可以知道 聚簇索引是唯一的，一个 InnoDB 表只有一个聚簇索引，而且一定会有一个聚簇索引，如果不存在，Innodb 存储引擎会自动添加一个 非聚簇所以可以有多个，而且只能由用户自己添加，InnoDB 默认并不会创建任何非聚簇索引。 非聚簇索引中一定包含了聚簇索引的列值，但反过来却不存在。 因此，使用非聚簇索引查询数据一定会用到聚簇索引，但反过来却不存在。 参考 http://cmsblogs.com/?p=5463","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"SQL语句","date":"2019-08-24T07:41:20.000Z","path":"wiki/SQL语句/","text":"SQL的各种JOIN用法下图展示了 LEFT JOIN、RIGHT JOIN、INNER JOIN、OUTER JOIN 相关的 7 种用法。 Inner JOIN SELECT &lt;select_list&gt; FROM Table_A AINNER JOIN Table_B BON A.Key = B.Key Left JOIN SELECT &lt;select_list&gt;FROM Table_A ALEFT JOIN Table_B BON A.Key = B.Key Right JOIN SELECT &lt;select_list&gt;FROM Table_A ARIGHT JOIN Table_B BON A.Key = B.Key Left Excluding JOIN SELECT &lt;select_list&gt; FROM Table_A ALEFT JOIN Table_B BON A.Key = B.KeyWHERE B.Key IS NULL Right Excluding JOIN SELECT &lt;select_list&gt;FROM Table_A ARIGHT JOIN Table_B BON A.Key = B.KeyWHERE A.Key IS NULL Outer JOIN # oracleSELECT &lt;select_list&gt;FROM Table_A AFULL OUTER JOIN Table_B BON A.Key = B.Key# MySQLSELECT &lt;select_list&gt;FROM Table_A ALEFT JOIN Table_B BON A.Key = B.KeyUNIONSELECT &lt;select_list&gt;FROM Table_A ARIGHT JOIN Table_B BON A.Key = B.Key Outer Excluding JOIN # oracleSELECT &lt;select_list&gt;FROM Table_A AFULL OUTER JOIN Table_B BON A.Key = B.KeyWHERE A.Key IS NULL OR B.Key IS NULL# mysqlSELECT &lt;select_list&gt; FROM Table_A ALEFT JOIN Table_B BON A.Key = B.KeyWHERE B.Key IS NULLUNIONSELECT &lt;select_list&gt;FROM Table_A ARIGHT JOIN Table_B BON A.Key = B.KeyWHERE A.Key IS NULL SQL 执行顺序 参考 https://www.runoob.com/w3cnote/sql-join-image-explain.html","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"MySQL","slug":"数据库/MySQL","permalink":"https://yapengren.github.io/categories/数据库/MySQL/"}]},{"title":"Redis zset 实现简单限流","date":"2019-08-23T01:48:40.000Z","path":"wiki/Redis-zset-实现简单限流/","text":"除了控制流量，限流还有一个应用目的是用于控制用户行为，避免垃圾请求。比如在 UGC 社区，用户的发帖、回复、点赞等行为都要严格受控，一般要严格限定某行为在规定时间内允许的次数，超过了次数那就是非法行为。对非法行为，业务必须规定适当的惩处策略。 如何使用 Redis 来实现简单限流策略？首先我们来看一个常见的简单的限流策略。系统要限定用户的某个行为在指定的时间里只能允许发生 N 次，如何使用 Redis 的数据结构来实现这个限流的功能？ 我们先定义这个接口，理解了这个接口的定义，读者就应该能明白我们期望达到的功能。 // 指定用户 user_id 的某个行为 action_key，在特定的时间内 period，只允许发生一定的次数 max_countpublic boolean isActionAllowed(String userId, String actionKey, int period, int maxCount) &#123;&#125;// 调用这个接口，5 分钟之内只能发帖 10 次limiter.isActionAllowed(\"xiaoming\", \"publish\", 5 * 60, 10) 错误方案 将 userId 和 actionKey 拼成 key，在第一次请求时设置 value 为 1，设置过期时间 expire 为特定的时间 period 每次请求的时候获取 value 值，若存在则 incr 自增 1，超过 maxCount 则做限制 问题模拟分析 如上图：5 分钟之内只能发帖 10 次。 11:01 用户发帖 1 次，此时 redis 中存放数据 key 为 userId:actionKey，vlaue 为 1，过期时间 5 分钟； 11:05 用户发帖 8 次，发帖成功； 11:05 之后，key 过期时间到，被移除； 11:06 用户发帖，此时 redis 中 key 不存在，重新存放 key，发帖 8 次，发帖成功； 那么 11:05 -&gt; 11:06 时间段 2 分钟发帖 16 次，没有达到期望的功能； 正确方案这个限流需求中存在一个滑动时间窗口，想想 zset 数据结构的 score 值，是不是可以通过 score 来圈出这个时间窗口来。而且我们只需要保留这个时间窗口，窗口之外的数据都可以砍掉。那这个 zset 的 value 填什么比较合适呢？它只需要保证唯一性即可，用 uuid 会比较浪费空间，那就改用毫秒时间戳吧。 如图所示，用一个 zset 结构记录用户的行为历史，每一个行为都会作为 zset 中的一个 key 保存下来。同一个用户同一种行为用一个 zset 记录。 为节省内存，我们只需要保留时间窗口内的行为记录，同时如果用户是冷用户，滑动时间窗口内的行为是空记录，那么这个 zset 就可以从内存中移除，不再占用空间。 通过统计滑动窗口内的行为数量与阈值 max_count 进行比较就可以得出当前的行为是否允许。用代码表示如下： public class SimpleRateLimiter &#123; private Jedis jedis; public SimpleRateLimiter(Jedis jedis) &#123; this.jedis = jedis; &#125; /** * @param userId 用户 user_id * @param actionKey 某个行为 * @param period 特定的时间内，单位秒 * @param maxCount 最大允许的次数 * @return */ public boolean isActionAllowed(String userId, String actionKey, int period, int maxCount) &#123; String key = String.format(\"hist:%s:%s\", userId, actionKey); // 毫秒时间戳 long nowTs = System.currentTimeMillis(); Pipeline pipe = jedis.pipelined(); // 用了multi，也就是事务，能保证一系列指令的原子顺序执行 pipe.multi(); // 存放数据，value 和 score 都使用毫秒时间戳 pipe.zadd(key, nowTs, \"\" + nowTs); // zremrangebyscore key min max 命令用于移除有序集中，指定分数（score）区间内的所有成员 // 移除时间窗口之前的数据，剩下的都是时间窗口之内的 Response&lt;Long&gt; longResponse = pipe.zremrangeByScore(key, 0, nowTs - period * 1000); // 相当于 count()，获取时间窗口内的行为数量 Response&lt;Long&gt; count = pipe.zcard(key); // 设置 zset 过期时间，避免冷用户持续占用内存 // 过期时间应该等于时间窗口的长度，再多宽限 1s pipe.expire(key, period + 1); pipe.exec(); pipe.close(); // 比较数量是否超标 return count.get() &lt;= maxCount; &#125; public static void main(String[] args) &#123; Jedis jedis = new Jedis(); SimpleRateLimiter limiter = new SimpleRateLimiter(jedis); for (int i = 0; i &lt; 20; i++) &#123; System.out.println(limiter.isActionAllowed(\"xiaoming\", \"publish\", 5 * 60, 10)); &#125; &#125;&#125; 缺点因为它要记录时间窗口内所有的行为记录，如果这个量很大，比如限定 60s 内操作不得超过 100w 次这样的参数，它是不适合做这样的限流的，因为会消耗大量的存储空间。 参考 《Redis 深度历险：核心原理与应用实践》 作者：钱文品","tags":[{"name":"Redis","slug":"Redis","permalink":"https://yapengren.github.io/tags/Redis/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://yapengren.github.io/categories/数据库/Redis/"}]},{"title":"java死锁编程及定位分析","date":"2019-08-21T09:37:44.000Z","path":"wiki/java死锁编程及定位分析/","text":"死锁编程及定位分析是什么死锁是指两个或者两个以上的进程在执行过程中，因抢夺资源而造成的一种互相等待的现象，若无外力干涉它们将都无法推进下去，如果系统资源充足，进程的资源请求都能够得到满足，死锁出现的可能性也就很低，否则就会因争夺有限的资源而陷入死锁。 产生死锁的主要原因 系统资源不足 进程运行推进的顺序不合适 资源分配不当 代码：DeadLockDemopublic class DeadLockDemo &#123; public static void main(String[] args) &#123; String lockA = \"lockA\"; String lockB = \"lockB\"; new Thread(new HoldLockThread(lockA, lockB), \"ThreadAAA\").start(); new Thread(new HoldLockThread(lockB, lockA), \"ThreadBBB\").start(); &#125;&#125;class HoldLockThread implements Runnable &#123; private String lockA; private String lockB; public HoldLockThread(String lockA, String lockB) &#123; this.lockA = lockA; this.lockB = lockB; &#125; @Override public void run() &#123; synchronized (lockA) &#123; System.out.println(Thread.currentThread().getName() + \" 自己持有： \" + lockA + \" 尝试获得： \" + lockB); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (lockB) &#123; System.out.println(Thread.currentThread().getName() + \" 自己持有： \" + lockB + \" 尝试获得： \" + lockA); &#125; &#125; &#125;&#125; 解决 jps 命令定位进程号 jstack 找到死锁查看","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"java公平锁/非公平锁/可重入锁/递归锁/自旋锁谈谈你的理解？","date":"2019-08-20T06:29:56.000Z","path":"wiki/java公平锁-非公平锁-可重入锁-递归锁-自旋锁谈谈你的理解？/","text":"公平锁和非公平锁是什么公平锁：是指多个线程按照申请锁的顺序来获取锁，类似排队打饭，先来后到。 非公平锁：是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。在高并发的情况下，有可能会造成优先级反转或者饥饿现象。 两者区别公平锁/非公平锁：并发包中 ReentrantLock 的创建可以指定构造函数的 boolean 类型来得到公平锁或非公平锁，默认是非公平锁。 关于两者区别： 公平锁，就是很公平，在并发情况下，每个线程在获取锁时会查看此锁维护的等待队列，如果为空，或者当前线程是等待队列的第一个，就占有锁，否则就会加入到等待队列中，以后会按照 FIFO 的规则从队列中取到自己。 非公平锁：非公平锁比较粗鲁，上来就直接尝试占有锁，如果尝试失败，就再采取类似公平锁那种方式。 题外话Java ReentrantLock 而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。 对于 Synchronized 而言，也是一种非公平锁。 可重入锁（又名递归锁）是什么可重入锁（也就是递归锁）：指的是同一个线程外层函数获得锁之后，内层递归函数仍然能获取该锁的代码，在同一线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。 也就是说，线程可以进入任何一个它已经拥有的锁所有同步着的代码块。 ReentrantLock/Synchronized 就是一个典型的可重入锁可重入锁最大的作用是避免死锁独占锁/共享锁独占锁：指该锁一次只能被一个线程所持有。对 ReentrantLock 和 Synchronized 而言都是独占锁。 共享锁：指该锁可被多个线程所持有。 对 ReentrantReadWriteLock，其读锁是共享锁，其写锁是独占锁。读锁的共享锁可保证并发读是非常高效的，读写，写读，写写的过程是互斥的。 自旋锁自旋锁：是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下切换的消耗，缺点是循环会消耗CPU。","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"CopyOnWriteArrayList","date":"2019-08-20T06:19:07.000Z","path":"wiki/CopyOnWriteArrayList/","text":"写时复制CopyOnWrite 容器即写时复制的容器。往一个容器添加元素的时候，不直接往当前容器 Object[] 添加，而是先将当前 object[] 进行 Copy，复制出一个新的容器 Object[] newElements，然后新的容器 Object[] newElements 里添加元素，添加完元素之后，再将原容器的引用指向新的容器 setArray(newElements)这样做的好处是可以对 copyonwrite 容器进行并发的读，而不需要加锁，因为当前容器不会添加任何元素。所以 copyonwrite容器也是一种读写分离的思想，读和写不同的容器。","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"原子类Atomiclnteger的ABA问题谈谈？","date":"2019-08-20T05:56:30.000Z","path":"wiki/原子类Atomiclnteger的ABA问题谈谈？/","text":"ABA问题怎么产生的CAS会导致 ABA 问题 CAS 算法实现一个重要前提需要取出内存中某时刻的数据并在当下时刻比较并替换，那么在这个时间差类会导致数据的变化。 尽管线程 one 的 CAS 操作成功，但是不代表这个过程就是没问题的。 原子引用解决ABA问题 时间戳原子引用AtomicStampledReferenceABADemo","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"CAS理解","date":"2019-08-20T05:49:03.000Z","path":"wiki/CAS理解/","text":"比较并交换CAS底层原理？如果知道，谈谈比对Unsafe的理解Unsafe Unsafe 是 CAS 的核心类，由于 Java 方法无法直接访问底层系统，需要通过本地（native）方法来访问，Unsafe 相当于一个后门，基于该类可以直接操作特定内存的数据。Unsafe 类存在于 sun.misc 包中，其内部方法操作可以像 C 的指针一样直接操作内存，因为 Java 中 CAS 操作的执行依赖于 Unsafe 类的方法。 注意 Unsafe 类中的所有方法都是 native 修饰的，也就是说 Unsafe 类中的方法都直接调用操作系统底层资源执行相应任务。 变量 valueOffset，表示该变量在内存中的偏移地址，因为 Unsafe 就是根据内存偏移地址获取数据的。 变量 value 用 volatile 修饰，保证了多线程之间的内存可见性。 CAS 是什么 unsafe.getAndAddInt 假设线程 A 和线程 B 两个线程同时执行 getAndAddInt 操作（分别跑在不同 CPU 上） AtomicInteger 里面的 value 原始值为 3，即主内存中 AtomicInteger 的 value 为 3，根据 JMM 模型，线程 A 和线程 B 各自持有一份值为 3 的 value 的副本分别到各自的工作内存。 线程 A 通过 getIntVolatile（var1，var2）拿到 value 值 3，这是线程 A 被挂起。 线程 B 也通过 getIntVolatile（var1，var2）方法获得 value 值 3，此时刚好线程 B 没有被挂起并执行 compareAndSwap 方法比较内存值也为 3，成功修改内存值为 4，线程 B 打完收工，一切OK。 这是线程 A 回复，执行 compareAndSwapInt 方法比较，发现手里的值 3 与内存值 4 不一致，说明该值已经被其他线程抢险异步修改过了，那 A 线程本次修改失败，只能重新读取重新来一遍了。 线程 A 重新获取 value 值，因为变量 value 被 volatile 修饰，所以其他线程对它的修改，线程 A 总是能够看到，线程 A 继续执行 compareAndSwapInt 进行比较替换，直到成功。 底层汇编 简单版小总结CAS（CompareAndSwap）比较当前工作内存中的值和主内存中的值，如果相同则执行规定操作，否则继续比较知道主内存和工作内存中的值一致为止。 CAS应用CAS有 3 个操作数，内存值 V，旧的预期值 A，要修改的更新值 B。当且仅当预期值 A 和内存值 V 相同时，将内存值 V 修改为 B，否则什么都不做。 CAS缺点 循环时间长开销大如果 CAS 失败，会一直进行尝试。如果 CAS 长时间一直不成功，可能会给 CPU 带来很大的开销。 只能保证一个共享变量的原子操作当对一个共享变量执行操作时，我们只能使用循环 CAS 的方式来保证原子操作，但是，对多个共享变量操作时，循环 CAS 就无法保证操作的原子性，这个时候就可以用锁来保证原子性。 引出来 ABA 问题？？？通过原子引用解决 ABA 问题","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"谈谈你对volatile的理解","date":"2019-08-19T09:23:18.000Z","path":"wiki/谈谈你对volatile的理解/","text":"谈谈你对 volatile 的理解volatile 是 Java 虚拟机提供的轻量级的同步机制 保证可见性各个线程对主内存中共享变量的操作都是各个线程各自拷贝到自己的工作内存进行操作后再写回主内存中的。 这就可能存在一个线程 A 修改了共享变量 X 的值但还未写回主内存时，另一个线程 B 又对准内存中同一个共享变量 X 进行操作，但此时 A 线程工作内存中共享变量 X 对线程 B 来说并不是可见，这种工作内存与主内存同步存在延迟现象就造成了可见性问题。 不保证原子性禁止指令重排volatile 实现禁止指令重排优化，从而避免多线程环境下程序出现乱序执行的现象。 先了解一个概念，内存屏障又称内存栅栏，是一个CPU指令，它的作用有两个：一是保证特定操作的执行顺序二是保证某些变量的内存可见性（利用该特性实现volatile的内存可见性） 由于编译器和处理器都能执行指令重排优化。如果在指令间插入一条 Memory Barrier 则告诉编译器和 CPU，不管什么指令都不能和这条 Memory Barrier 指令重新排序，也就是说通过插入内存屏障禁止在内存屏障前后的指令执行重排序优化。内存屏障另外一个作用是强制刷出各种CPU的缓存数据，因此任何CPU上的线程都能读取到这些数据的最新版本。 你在哪些地方用过volatile？单例模式 DCL 代码单例模式 volatile 分析 DCL（双端检锁）机制不一定线程安全，原因是有指令重排序的存在，加入 volatile 可以禁止指令重排。原因在于某一个线程执行到第一个检测，读取到的 instance 不为 null 时，instance 的引用对象可能没有完成初始化。指令重排只会保证串行语义的执行一致性（单线程），但并不会关心多线程间的语义一致性。所以当一条线程访问 instance 不为 null 时，由于 instance 实例未必已初始化完成，也就造成了线程安全问题。","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"java 线程池知识点","date":"2019-08-19T07:33:15.000Z","path":"wiki/java-线程池知识点/","text":"为什么用线程池，优势线程池主要是控制运行线程的数量，处理过程中将任务放入队列，然后在线程创建后启动这些任务，如果线程数量超过了最大数量的线程排队等候，等其他线程执行完毕，再从队列中取出任务来执行。 主要特点是：线程复用、控制最大并发数、管理线程。 第一：降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。 第二：提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。 第三：提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 线程池如何使用架构说明Java中的线程池是通过 Executor 框架实现的，该框架中用到了 Executor，Executors，ExecutorService，ThreadPoolExecutor 这几个类。 编码实现 Executors.newFixedThreadPool(int) 执行长期的任务，性能好很多 主要特点： 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newFixedThreadPool 创建的线程池 corePoolSize 和 maximumPoolSize 值是相等的，它使用的 LinkedBlockingQueue。 Executors.newSingleThreadExecutor() 一个任务一个任务执行的场景 主要特点： 创建一个单线程化的线程池，它只会唯一的工作线程来执行任务，保证所有任务按照指定顺序执行。 newSingleThread 将 corePoolSize 和 maximumPoolSize 都设置为1，它使用的是LinkedBlockingQueue。 Executors.newCachedThreadPool() 适用：执行很多短期异步的小程序或者负载较轻的服务器 主要特点： 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newCachedThreadPool 将 corePoolSize 设置为 0，将 maximumPoolSize 设置为 Integer.MAX_VALUE，使用的 SynchronousQueue，也就是说来了任务就创建线程运行，当线程空闲超过 60 秒，就销毁线程。 阿里巴巴 java 开发规范【强制】线程池不允许使用 Executors 去创建，而是通过 ThreadPoolExecutor 的方式，这样的处理方式让写的同学更加明确线程池的运行规则，规避资源耗尽的风险。 说明：Executors 返回的线程池对象的弊端如下： FixedThreadPool 和 SingleThreadPool：允许的请求队列长度为 Integer.MAX_VALUE ，可能会堆积大量的请求，从而导致 OOM。 CachedThreadPool：允许的创建线程数量为 Integer.MAX_VALUE ，可能会创建大量的线程，从而导致 OOM。 线程池的几个重要参数介绍 corePoolSize 线程池中的常驻核心线程数 在创建线程池后，当有请求任务来之后，就会安排池中的线程去执行请求任务，近似理解为今日当值线程。 当线程池中的线程数目到达 corePoolSize 后，就会把到达的任务放到缓存队列当中。 maximumPoolSize 线程池能够容纳同时执行的最大线程数，此值必须大于等于1. keepAliveTime 多余的空闲线程的存活时间。当前线程池数量超过 corePoolSize 时，当空闲时间达到 keepAliveTime 值时，多余空闲线程会被销毁直到只剩下 corePoolSize 个线程为止。 默认情况下：只有当线程池中的线程数大于 corePoolSize 时 keepAliveTime 才会起作用，直到线程池中的线程数不大于 corePoolSize。 unit keepAliveTime 的单位 workQueue 任务队列，被提交但尚未被执行的任务。 threadFactory 表示生成线程池中工作线程的线程工厂，用于创建线程一般用默认的即可。 handler 拒绝策略，表示当队列满了并且工作线程大于等于线程池的最大线程数。 线程池的底层工作原理？ 在创建了线程池后，等待提交过来的任务请求。 当调用 execute() 方法添加一个请求任务时，线程池会做如下判断： 如果正在运行的线程数量 &lt; corePoolSize，那么马上创建线程运行这个任务。 如果正在运行的线程数量 &gt;= corePoolSize，那么将这个任务放入队列。 如果这个时候队列满了且正在运行的线程数量还 &lt; maximumPoolSize，那么还是要创建非核心线程立刻运行这个任务。 如果队列满了且正在运行的线程数量 &gt;= maximumPoolSize，那么线程池会启动饱和拒绝策略来执行。 当一个线程完成任务时，它会从队列中取下一个任务来执行。 当一个线程无事可做超过一定的时间 (keepAliveTime) 时，线程池会判断： 如果当前运行的线程数 &gt; corePoolSize，那么这个线程就被停掉。所以线程池的所有任务完成后它最终会收缩到 corePoolSize 的大小。 线程池的拒绝策略是什么等待队列也满了，再也塞不下新任务了，同时线程池中的 max 线程也达到了，无法继续为新的任务服务。 这时候就需要拒绝策略机制合理的处理这个问题。 JDK 内置的拒绝策略 AbortPolicy（默认）：直接抛出 RejectedExecutionException 异常阻止系统正常运行。 CallerRunsPolicy：“调用者运行”一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者，从而降低新任务的流量。 DiscardOldestPolicy：抛弃队列中等待最久的任务，然后把当前任务加入队列中尝试再次提交当前任务。 DiscardPolicy：直接丢弃任务，不予任何处理也不抛出异常。如果允许任务丢失，这是最好的一种方案。 以上内置拒绝策略均实现了 RejectedExecutionHandler 接口 合理配置线程池你是如何考虑的？CPU密集型CPU 密集的意思是该任务需要大量的运算，而没有阻塞，CPU 一直全速运行。 CPU 密集型任务配置尽可能少的线程数量： 一般公式为：CPU 核数 + 1个线程的线程池。 IO密集型 由于 IO 密集型任务线程并不是一直执行任务，则应配置尽可能多的线程，如 CPU 核数 * 2 IO 密集型，即该任务需要大量的 IO，即大量的阻塞。 在单线程上运行 IO 密集型的任务会导致浪费大量的 CPU 运算能力浪费在等待。 所以 IO 密集型任务中使用多线程可以大大的加速程序运行，即使在单核 CPU 上，这种加速主要就是利用了被浪费掉的阻塞时间。 IO 密集型时，大部分线程都阻塞，故需要多配置线程数： 参考公式：CPU 核数 / (1 - 阻塞系数) 阻塞系数在 0.8-0.9 之间 比如 8 核 CPU：8 / (1 - 0.9) = 80 个线程数","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"Redis核心原理与应用实践","date":"2019-08-18T03:26:40.000Z","path":"wiki/Redis核心原理与应用实践/","text":"基础：Redis 基础数据结构Redis 安装体验 Redis 需要使用 Linux 或者 Mac 环境，如果是 Windows 可以考虑使用虚拟机。主要方式有四种： 使用 Docker 安装。 通过 Github 源码编译。 直接安装 apt-get install(Ubuntu)、yum install(RedHat) 或者 brew install(Mac)。 如果读者懒于安装操作，也可以使用网页版的 Web Redis 直接体验。 具体操作如下： Docker 方式$ docker pull redis // 拉取 redis 镜像$ docker run --name myredis -d -p6379:6379 redis # 运行 redis 容器# 执行容器中的 redis-cli，可以直接使用命令行操作 redis$ docker exec -it myredis redis-cli Github 源码编译方式$ git clone --branch 2.8 --depth 1 git@github.com:antirez/redis.git // 下载源码$ cd redis$ make // 编译$ cd src$ ./redis-server --daemonize yes // 运行服务器，daemonize表示在后台运行$ ./redis-cli // 运行命令行 直接安装方式# mac$ brew install redis# ubuntu$ apt-get install redis# redhat$ yum install redis# 运行客户端$ redis-cli Redis 基础数据结构Redis 有 5 种基础数据结构，分别为：string (字符串)、list (列表)、set (集合)、hash (哈希) 和 zset (有序集合)。 string (字符串)字符串 string 是 Redis 最简单的数据结构。Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据。不同类型的数据结构的差异就在于 value 的结构不一样。 字符串结构使用非常广泛，一个常见的用途就是缓存用户信息。我们将用户信息结构体使用 JSON 序列化成字符串，然后将序列化后的字符串塞进 Redis 来缓存。同样，取用户信息会经过一次反序列化的过程。 Redis 的字符串是动态字符串，是可以修改的字符串，内部结构实现上类似于 Java 的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配，如图中所示，内部为当前字符串实际分配的空间 capacity 一般要高于实际字符串长度 len。当字符串长度小于 1M 时，扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间。需要注意的是字符串最大长度为 512M。 键值对$ set name zhangsan // 初始化字符串OK$ get name // 获取字符串\"zhangsan\"$ exists name // 字符串是否存在(integer) 1$ strlen name // 获取字符串的长度(integer) 8$ getrange name 0 4 // 获取子串(开始和结束位置start,end)\"zhang\"$ del name // 删除(integer) 1 批量键值对可以批量对多个字符串进行读写，节省网络耗时开销。 $ set name1 zhangsanOK$ set name2 lisiOK$ mget name1 name2 // 返回一个列表1) \"zhangsan\"2) \"lisi\"$ mset name1 wangwu name2 zhaoliuOK$ mget name1 name21) \"wangwu\"2) \"zhaoliu\" 过期和 set 命令扩展可以对 key 设置过期时间，到点自动删除，这个功能常用来控制缓存的失效时间。 $ set name zhangsan$ get name\"zhangsan\"$ expire name 20 // 20s后过期$ ttl name // 还有18秒的寿命，返回-2表示变量不存在，-1表示没有设置过期时间(integer) 18... # wait for 20s$ get name(nil)$ setex name 5 zhangsan // 5s后过期，等价于 set+expire$ get name\"zhangsan\"... # wait for 5s&gt; get name(nil)&gt; setnx name zhangsan // 如果 name 不存在就执行 set 创建(integer) 1&gt; get name\"zhangsan\"&gt; setnx name zhangsan(integer) 0 // 因为 name 已经存在，所以 set 创建不成功 计数如果 value 值是一个整数，还可以对它进行自增操作。自增是有范围的，它的范围是 signed long 的最大最小值，超过了这个值，Redis 会报错。 $ set age 30OK$ incr age(integer) 31$ incrby age 5(integer) 36$ decr age(integer) 35$ decrby age 5(integer) 30$ set codehole 9223372036854775807 // Long.MaxOK$ incr codehole(error) ERR increment or decrement would overflow 字符串是由多个字节组成，每个字节又是由 8 个 bit 组成，如此便可以将一个字符串看成很多 bit 的组合，这便是 bitmap「位图」数据结构，位图的具体使用会放到后面的章节来讲。 关于字符串的内部结构实现，请阅读第 32 节《极度深寒 —— 探索「字符串」内部》 list (列表)Redis 的列表相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)，这点让人非常意外。 当列表弹出了最后一个元素之后，该数据结构自动被删除，内存被回收。 Redis 的列表结构常用来做异步队列使用。将需要延后处理的任务结构体序列化成字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理。 右边进左边出：队列&gt; rpush books python java golang(integer) 3&gt; llen books(integer) 3&gt; lpop books\"python\"&gt; lpop books\"java\"&gt; lpop books\"golang\"&gt; lpop books(nil) 右边进右边出：栈&gt; rpush books python java golang(integer) 3&gt; rpop books\"golang\"&gt; rpop books\"java\"&gt; rpop books\"python\"&gt; rpop books(nil) 慢操作lindex 相当于 Java 链表的get(int index)方法，它需要对链表进行遍历，性能随着参数index增大而变差。 ltrim 和字面上的含义不太一样，个人觉得它叫 lretain(保留) 更合适一些，因为 ltrim 跟的两个参数start_index和end_index定义了一个区间，在这个区间内的值，ltrim 要保留，区间之外统统砍掉。我们可以通过ltrim来实现一个定长的链表，这一点非常有用。 index 可以为负数，index=-1表示倒数第一个元素，同样index=-2表示倒数第二个元素。 &gt; rpush books python java golang(integer) 3&gt; lindex books 1 # O(n) 慎用\"java\"&gt; lrange books 0 -1 # 获取所有元素，O(n) 慎用1) \"python\"2) \"java\"3) \"golang\"&gt; ltrim books 1 -1 # O(n) 慎用OK&gt; lrange books 0 -11) \"java\"2) \"golang\"&gt; ltrim books 1 0 # 这其实是清空了整个列表，因为区间范围长度为负OK&gt; llen books(integer) 0 快速列表 如果再深入一点，你会发现 Redis 底层存储的还不是一个简单的 linkedlist，而是称之为快速链表 quicklist 的一个结构。 首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 ziplist，也即是压缩列表。它将所有的元素紧挨着一起存储，分配的是一块连续的内存。当数据量比较多的时候才会改成 quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间，而且会加重内存的碎片化。比如这个列表里存的只是 int 类型的数据，结构上还需要两个额外的指针 prev 和 next 。所以 Redis 将链表和 ziplist 结合起来组成了 quicklist。也就是将多个 ziplist 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。 关于列表的内部结构实现，请阅读第 34 节《极度深寒 —— 探索「压缩列表」内部》和第 35 节《极度深寒 —— 探索「快速列表」内部》 hash (字典)Redis 的字典相当于 Java 语言里面的 HashMap，它是无序字典。内部实现结构上同 Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。 不同的是，Redis 的字典的值只能是字符串，另外它们 rehash 的方式不一样，因为 Java 的 HashMap 在字典很大时，rehash 是个耗时的操作，需要一次性全部 rehash。Redis 为了高性能，不能堵塞服务，所以采用了渐进式 rehash 策略。 渐进式 rehash 会在 rehash 的同时，保留新旧两个 hash 结构，查询时会同时查询两个 hash 结构，然后在后续的定时任务中以及 hash 操作指令中，循序渐进地将旧 hash 的内容一点点迁移到新的 hash 结构中。当搬迁完成了，就会使用新的hash结构取而代之。 当 hash 移除了最后一个元素之后，该数据结构自动被删除，内存被回收。 hash 结构也可以用来存储用户信息，不同于字符串一次性需要全部序列化整个对象，hash 可以对用户结构中的每个字段单独存储。这样当我们需要获取用户信息时可以进行部分获取。而以整个字符串的形式去保存用户信息的话就只能一次性全部读取，这样就会比较浪费网络流量。 hash 也有缺点，hash 结构的存储消耗要高于单个字符串，到底该使用 hash 还是字符串，需要根据实际情况再三权衡。 &gt; hset books java \"think in java\" # 命令行的字符串如果包含空格，要用引号括起来(integer) 1&gt; hset books golang \"concurrency in go\"(integer) 1&gt; hset books python \"python cookbook\"(integer) 1&gt; hgetall books # entries()，key 和 value 间隔出现1) \"java\"2) \"think in java\"3) \"golang\"4) \"concurrency in go\"5) \"python\"6) \"python cookbook\"&gt; hlen books(integer) 3&gt; hget books java\"think in java\"&gt; hset books golang \"learning go programming\" # 因为是更新操作，所以返回 0(integer) 0&gt; hget books golang\"learning go programming\"&gt; hmset books java \"effective java\" python \"learning python\" golang \"modern golang programming\" # 批量 setOK 同字符串对象一样，hash 结构中的单个子 key 也可以进行计数，它对应的指令是 hincrby，和 incr使用基本一样。 # 老钱又老了一岁&gt; hincrby user-laoqian age 1(integer) 30 关于字典的内部结构实现，请阅读第 33 节《极度深寒 —— 探索「字典」内部》。 set (集合)Redis 的集合相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值NULL。 当集合中最后一个元素移除之后，数据结构自动删除，内存被回收。 set 结构可以用来存储活动中奖的用户 ID，因为有去重功能，可以保证同一个用户不会中奖两次。 $ sadd books python // 增加元素(integer) 1$ sadd books python // 重复(integer) 0$ sadd books java golang(integer) 2$ smembers books // 注意顺序，和插入的并不一致，因为 set 是无序的1) \"java\"2) \"python\"3) \"golang\"$ sismember books java // 查询某个 value 是否存在，相当于 contains(o)(integer) 1$ sismember books rust(integer) 0$ scard books // 获取长度相当于 count()(integer) 3$ spop books // 弹出一个\"java\" zset (有序集合)zset 可能是 Redis 提供的最为特色的数据结构，它也是在面试中面试官最爱问的数据结构。它类似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重。它的内部实现用的是一种叫做「跳跃列表」的数据结构。 zset 中最后一个 value 被移除后，数据结构自动删除，内存被回收。 zset 可以用来存粉丝列表，value 值是粉丝的用户 ID，score 是关注时间。我们可以对粉丝列表按关注时间进行排序。 zset 还可以用来存储学生的成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次。 &gt; zadd books 9.0 \"think in java\"(integer) 1&gt; zadd books 8.9 \"java concurrency\"(integer) 1&gt; zadd books 8.6 \"java cookbook\"(integer) 1&gt; zrange books 0 -1 # 按 score 排序列出，参数区间为排名范围1) \"java cookbook\"2) \"java concurrency\"3) \"think in java\"&gt; zrevrange books 0 -1 # 按 score 逆序列出，参数区间为排名范围1) \"think in java\"2) \"java concurrency\"3) \"java cookbook\"&gt; zcard books # 相当于 count()(integer) 3&gt; zscore books \"java concurrency\" # 获取指定 value 的 score\"8.9000000000000004\" # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题&gt; zrank books \"java concurrency\" # 排名(integer) 1&gt; zrangebyscore books 0 8.91 # 根据分值区间遍历 zset1) \"java cookbook\"2) \"java concurrency\"&gt; zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。1) \"java cookbook\"2) \"8.5999999999999996\"3) \"java concurrency\"4) \"8.9000000000000004\"&gt; zrem books \"java concurrency\" # 删除 value(integer) 1&gt; zrange books 0 -11) \"java cookbook\"2) \"think in java\" 跳跃列表 zset 内部的排序功能是通过「跳跃列表」数据结构来实现的，它的结构非常特殊，也比较复杂。 因为 zset 要支持随机的插入和删除，所以它不好使用数组来表示。我们先看一个普通的链表结构。 我们需要这个链表按照 score 值进行排序。这意味着当有新元素需要插入时，要定位到特定位置的插入点，这样才可以继续保证链表是有序的。通常我们会通过二分查找来找到插入点，但是二分查找的对象必须是数组，只有数组才可以支持快速位置定位，链表做不到，那该怎么办？ 想想一个创业公司，刚开始只有几个人，团队成员之间人人平等，都是联合创始人。随着公司的成长，人数渐渐变多，团队沟通成本随之增加。这时候就会引入组长制，对团队进行划分。每个团队会有一个组长。开会的时候分团队进行，多个组长之间还会有自己的会议安排。公司规模进一步扩展，需要再增加一个层级 —— 部门，每个部门会从组长列表中推选出一个代表来作为部长。部长们之间还会有自己的高层会议安排。 跳跃列表就是类似于这种层级制，最下面一层所有的元素都会串起来。然后每隔几个元素挑选出一个代表来，再将这几个代表使用另外一级指针串起来。然后在这些代表里再挑出二级代表，再串起来。最终就形成了金字塔结构。 想想你老家在世界地图中的位置：亚洲–&gt;中国-&gt;安徽省-&gt;安庆市-&gt;枞阳县-&gt;汤沟镇-&gt;田间村-&gt;xxxx号，也是这样一个类似的结构。 「跳跃列表」之所以「跳跃」，是因为内部的元素可能「身兼数职」，比如上图中间的这个元素，同时处于 L0、L1 和 L2 层，可以快速在不同层次之间进行「跳跃」。 定位插入点时，先在顶层进行定位，然后下潜到下一级定位，一直下潜到最底层找到合适的位置，将新元素插进去。你也许会问，那新插入的元素如何才有机会「身兼数职」呢？ 跳跃列表采取一个随机策略来决定新元素可以兼职到第几层。 首先 L0 层肯定是 100% 了，L1 层只有 50% 的概率，L2 层只有 25% 的概率，L3 层只有 12.5% 的概率，一直随机到最顶层 L31 层。绝大多数元素都过不了几层，只有极少数元素可以深入到顶层。列表中的元素越多，能够深入的层次就越深，能进入到顶层的概率就会越大。 这还挺公平的，能不能进入中央不是靠拼爹，而是看运气。 关于跳跃列表的内部结构实现，请阅读第 36 节《极度深寒 —— 探索「跳跃列表」内部结构》 容器型数据结构的通用规则list/set/hash/zset 这四种数据结构是容器型数据结构，它们共享下面两条通用规则： create if not exists 如果容器不存在，那就创建一个，再进行操作。比如 rpush 操作刚开始是没有列表的，Redis 就会自动创建一个，然后再 rpush 进去新元素。 drop if no elements 如果容器里元素没有了，那么立即删除元素，释放内存。这意味着 lpop 操作到最后一个元素，列表就消失了。 过期时间Redis 所有的数据结构都可以设置过期时间，时间到了，Redis 会自动删除相应的对象。需要注意的是过期是以对象为单位，比如一个 hash 结构的过期是整个 hash 对象的过期，而不是其中的某个子 key。 还有一个需要特别注意的地方是如果一个字符串已经设置了过期时间，然后你调用了 set 方法修改了它，它的过期时间会消失。 127.0.0.1:6379&gt; set codehole yoyoOK127.0.0.1:6379&gt; expire codehole 600(integer) 1127.0.0.1:6379&gt; ttl codehole(integer) 597127.0.0.1:6379&gt; set codehole yoyoOK127.0.0.1:6379&gt; ttl codehole(integer) -1 参考 《Redis 深度历险：核心原理与应用实践》 作者：钱文品","tags":[{"name":"Redis","slug":"Redis","permalink":"https://yapengren.github.io/tags/Redis/"}],"categories":[{"name":"数据库","slug":"数据库","permalink":"https://yapengren.github.io/categories/数据库/"},{"name":"Redis","slug":"数据库/Redis","permalink":"https://yapengren.github.io/categories/数据库/Redis/"}]},{"title":"HashMap源码分析","date":"2019-08-17T05:45:12.000Z","path":"wiki/HashMap源码分析/","text":"继承体系 在 Java 中，HashMap 的实现采用了（数组 + 链表 + 红黑树）的复杂结构，数组的一个元素又称作桶。 在添加元素时，会根据 hash 值算出元素在数组中的位置，如果该位置没有元素，则直接把元素放置在此处，如果该位置有元素了，则把元素以链表的形式放置在链表的尾部。 当一个链表的元素个数达到一定的数量（且数组的长度达到一定的长度）后，则把链表转化为红黑树，从而提高效率。 数组的查询效率为 O(1)，链表的查询效率是 O(k)，红黑树的查询效率是 O(log k)，k为桶中的元素个数，所以当元素数量非常多的时候，转化为红黑树能极大地提高效率。 源码分析属性/** * 默认初始容量为 16 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4;/** * 最大容量为 2 的 30 次方 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 默认装载因子 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 当一个桶中的元素 ≥ 8 时进行树化 */static final int TREEIFY_THRESHOLD = 8;/** * 当一个桶中的元素 ≤ 6 时把树转换成链表 */static final int UNTREEIFY_THRESHOLD = 6;/** * 当桶的个数达到 64 的时候进行树化 */static final int MIN_TREEIFY_CAPACITY = 64;/** * 数组，又叫做桶 */transient Node&lt;K, V&gt;[] table;/** * 作为 enterSet() 的缓存 */transient Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet;/** * 元素的数量 */transient int size;/** * 修改次数，用于在迭代的时候执行快速失败策略 */transient int modCount;/** * 当桶的使用数量达到多少时进行扩容，threshold = capacity * loadFactor */int threshold;/** * 装载因子 */final float loadFactor; put(K key, V value) 方法public V put(K key, V value) &#123; // 调用 hash(key) 计算出 key 的 hash 值 return putVal(hash(key), key, value, false, true);&#125;static final int hash(Object key) &#123; int h; // 如果 key 为 null，则 hash 值为0，否则调用 key 的 hashCode() 方法 // 并让高 16 位与整个 hash 异或，这样做是为了使计算出的 hash 更分散 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 如果桶的数量为 0，则初始化 if ((tab = table) == null || (n = tab.length) == 0) // 调用 resize() 初始化 n = (tab = resize()).length; // (n-1) &amp; hash 计算元素在哪个桶中 // 如果这个桶中还没有元素，则把这个元素放在桶中的第一个位置 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 新建一个节点放在桶中 tab[i] = newNode(hash, key, value, null); else &#123; // 如果桶中已经有元素存在了 Node&lt;K,V&gt; e; K k; // 如果桶中第一个元素的 key 与待插入元素的 key 相同，保存到 e 中用于后续修改 value 值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) // 如果第一个元素是树节点，则调用树节点的 putTreeVal 插入元素 e = ((HashMap.TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; // 遍历这个桶对应的链表，binCount 用于存储链表中元素的个数 for (int binCount = 0; ; ++binCount) &#123; // 如果链表遍历完了都没有找到相同 key 的元素，则 key 对应的元素不存在，则在链表最后插入一个新节点 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 如果插入新节点后链表长度 &gt; 8，则判断是否需要树化，因为第一个元素没有加到 binCount 中，所以 -1 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; &#125; // 如果待插入的 key 在链表中找到了，则退出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 如果找到了对应 key 的元素 if (e != null) &#123; // existing mapping for key // 记录下旧值 V oldValue = e.value; // 判断是否需要替换旧值 if (!onlyIfAbsent || oldValue == null) // 替换旧值为新值 e.value = value; afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 到这里了说明没有找到元素 // 修改次数 +1 ++modCount; // 元素数量 +1，判断是否需要扩容 if (++size &gt; threshold) // 扩容 resize(); afterNodeInsertion(evict); // 没有找到元素返回 null return null;&#125; 计算 key 的 hash 值； 如果桶（数组）数量为 0，则初始化桶； 如果 key 所在的桶没有元素，则直接插入； 如果 key 所在的桶中的第一个元素的 key 与待插入的key相同，说明找到了元素，转后续流程（9）处理； 如果第一个元素是树节点，则调用树节点的 putTreeVal() 寻找元素或插入树节点； 如果不是以上三种情况，则遍历桶对应的链表查找 key 是否存在于链表中； 如果找到了对应 key 的元素，则转后续流程（9）处理； 如果没找到对应 key 的元素，则在链表最后插入一个新节点并判断是否需要树化； 如果找到了对应 key 的元素，则判断是否需要替换旧值，并直接返回旧值； 如果插入了元素，则数量加 1 并判断是否需要扩容；","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"源码","slug":"java/源码","permalink":"https://yapengren.github.io/categories/java/源码/"}]},{"title":"ArrayList源码分析","date":"2019-08-17T02:28:42.000Z","path":"wiki/ArrayList源码分析/","text":"继承体系 源码分析属性/** * 默认容量为10，也就是通过 new ArrayList() 创建时的默认容量 */private static final int DEFAULT_CAPACITY = 10;/** * 空数组 * 这种是通过 new ArrayList(0) 创建时用的空数组 */private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;/** * 空数组 * 这种是通过 new ArrayList() 创建时用的空数组 * 与 EMPTY_ELEMENTDATA 的区别是在添加第一个元素时使用这个空数组会初始化为 DEFAULT_CAPACITY（10）个元素 */private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;/** * 真正存放元素的地方 * 使用 transient 关键字能不序列化这个字段 */transient Object[] elementData; // non-private to simplify nested class access/** * 真正存放元素的个数 */private int size; ArrayList(int initialCapacity) 构造方法public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; // 如果传入的初始容量 &gt; 0，新建一个数组存储元素 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; // 如果传入的初始容量 = 0，使用空数组 EMPTY_ELEMENTDATA this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; // 如果传入的初始容量 &lt; 0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); &#125;&#125; add(E e) 方法添加元素到末尾 public boolean add(E e) &#123; // 检查是否需要扩容 ensureCapacityInternal(size + 1); // 将元素插入到最后一位 elementData[size++] = e; return true;&#125;private void ensureCapacityInternal(int minCapacity) &#123; ensureExplicitCapacity(calculateCapacity(elementData, minCapacity));&#125;private static int calculateCapacity(Object[] elementData, int minCapacity) &#123; // 如果是空数组 DEFAULTCAPACITY_EMPTY_ELEMENTDATA，就初始化为默认大小 10 if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; return Math.max(DEFAULT_CAPACITY, minCapacity); &#125; return minCapacity;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; if (minCapacity - elementData.length &gt; 0) // 扩容 grow(minCapacity);&#125;/** * 扩容 * */private void grow(int minCapacity) &#123; int oldCapacity = elementData.length; // 新容量为旧容量的 1.5 倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 如果新容量发现比需要的容量还小，则以需要的容量为准 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量已经超过最大容量了，则使用最大容量 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 以新容量拷贝出来一个新数组 elementData = Arrays.copyOf(elementData, newCapacity);&#125; 检查是否需要扩容； 如果 elementData 等于 DEFAULTCAPACITY_EMPTY_ELEMENTDATA，则初始化容量大小为 DEFAULT_CAPACITY； 新容量是老容量的1.5 倍（oldCapacity + (oldCapacity &gt;&gt; 1)），如果加了这么多容量发现比需要的容量还小，则以需要的容量为准； 创建新容量的数组并把老数组拷贝到新数组；","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"源码","slug":"java/源码","permalink":"https://yapengren.github.io/categories/java/源码/"}]},{"title":"Mybatis 知识点","date":"2019-08-14T09:20:15.000Z","path":"wiki/Mybatis-知识点/","text":"jdbc 问题 数据库连接创建、释放频繁造成系统资源浪费，从而影响系统性能。如果使用数据库连接池可解决此问题。 Sql 语句在代码中硬编码，造成代码不易维护，实际应用中 sql 变化的可能较大，sql 变动需要改变 java 代码。 使用 preparedStatement 向占有位符号传参数存在硬编码，因为 sql 语句的 where 条件不一定，可能多也可能少，修改 sql 还要修改代码，系统不易维护。 对结果集解析存在硬编码（查询列名），sql 变化导致解析代码变化，系统不易维护，如果能将数据库记录封装成 pojo 对象解析比较方便。 Mybatis 架构 mybatis配置SqlMapConfig.xml，此文件作为 mybatis 的全局配置文件，配置了 mybatis 的运行环境等信息。mapper.xml 文件即sql映射文件，文件中配置了操作数据库的 sql 语句。此文件需要在SqlMapConfig.xml 中加载。 通过 mybatis 环境等配置信息构造 SqlSessionFactory 即会话工厂。 由会话工厂创建 sqlSession 即会话，操作数据库需要通过 sqlSession 进行。 mybatis 底层自定义了 Executor 执行器接口操作数据库，Executor 接口有两个实现，一个是基本执行器、一个是缓存执行器。 Mapped Statement 也是 mybatis 一个底层封装对象，它包装了 mybatis 配置信息及 sql 映射信息等。mapper.xml 文件中一个 sql 对应一个 Mapped Statement 对象，sql 的 id 即是 Mapped statement 的id。 Mapped Statement 对 sql 执行输入参数进行定义，包括 HashMap、基本类型、pojo，Executor 通过Mapped Statement 在执行 sql前将输入的 java 对象映射至 sql 中，输入参数映射就是 jdbc 编程中对preparedStatement 设置参数。 Mapped Statement 对 sql 执行输出结果进行定义，包括 HashMap、基本类型、pojo，Executor 通过Mapped Statement 在执行 sql 后将输出结果映射至 java 对象中，输出结果映射过程相当于 jdbc 编程中对结果的解析处理过程。 #{} 和 ${} 区别 #{} 表示一个占位符号，通过 #{} 可以实现 preparedStatement 向占位符中设置值，自动进行 java 类型和 jdbc 类型转换。#{} 可以有效防止 sql 注入。#{} 可以接收简单类型值或 pojo 属性值。如果parameterType 传输单个简单类型值，#{} 括号中可以是 value 或其它名称。 ${} 表示拼接 sql 串，通过 ${} 可以将 parameterType 传入的内容拼接在 sql 中且不进行 jdbc 类型转换，${}可以接收简单类型值或 pojo 属性值，如果 parameterType 传输单个简单类型值，${} 括号中只能是 value。 parameterType 和 resultType parameterType：指定输入参数类型，mybatis 通过 ognl 从输入对象中获取参数值拼接在 sql 中。 resultType：指定输出结果类型，mybatis 将 sql 查询结果的一行记录数据映射为 resultType 指定类型的对象。如果有多条数据，则分别进行映射，并把对象放到容器List中。 Mapper 接口开发规范 Mapper.xml 文件中的 namespace 与 Mapper 接口的类路径相同 Mapper.xml 中定义的每个 statement 的 id 与 Mapper 接口方法名相同 Mapper.xml 中定义的每个 sql 的 parameterType 的类型与 Mapper 接口方法的输入参数类型相同 Mapper.xml 中定义的每个 sql 的 resultType 的类型与 Mapper 接口方法的输出参数类型相同 动态 sqlif 标签 &amp; where 标签Mapper.xml 文件 &lt;select id=\"selectAllData\" parameterType=\"CfContentLikeRecordDto\" resultType=\"CfContentLikeRecord\"&gt; SELECT * FROM cf_content &lt;!-- where标签可以自动添加where，同时处理sql语句中第一个and关键字 --&gt; &lt;WHERE&gt; &lt;if test=\"key != null and key !='' \"&gt; and `key` = #&#123;key&#125; &lt;/if&gt; &lt;if test=\"startTime != null and startTime !='' \"&gt; &lt;![CDATA[ AND create_Time &gt;= #&#123;startTime&#125;]]&gt; &lt;/if&gt; &lt;if test=\"endTime != null and endTime !='' \"&gt; &lt;![CDATA[ AND create_Time &lt;= #&#123;endTime&#125;]]&gt; &lt;/if&gt; &lt;/WHERE&gt;&lt;/select&gt; foreach 标签向 sql 传递数组或 List，mybatis 使用 foreach 解析，如下： 根据多个 id 查询用户信息 查询 sql： SELECT * FROM user WHERE id IN (1,10,24) &lt;!-- 根据ids查询用户 --&gt;&lt;select id=\"queryUserByIds\" parameterType=\"queryVo\" resultType=\"user\"&gt; SELECT * FROM `user` &lt;where&gt; &lt;!-- foreach标签，进行遍历 --&gt; &lt;!-- collection：遍历的集合，这里是 QueryVo 的 ids 属性 --&gt; &lt;!-- item：遍历的项目，可以随便写，，但是和后面的 #&#123;&#125;里面要一致 --&gt; &lt;!-- open：在前面添加的sql片段 --&gt; &lt;!-- close：在结尾处添加的sql片段 --&gt; &lt;!-- separator：指定遍历的元素之间使用的分隔符 --&gt; &lt;foreach collection=\"ids\" item=\"item\" open=\"id IN (\" close=\")\" separator=\",\"&gt; #&#123;item&#125; &lt;/foreach&gt; &lt;/where&gt;&lt;/select&gt; 关联查询数据模型 一对一查询需求：查询所有订单信息，关联查询下单用户信息。 注意：因为一个订单信息只会是一个人下的订单，所以从查询订单信息出发关联查询用户信息为一对一查询。如果从用户信息出发查询用户下的订单信息则为一对多查询，因为一个用户可以下多个订单。 使用 resultType使用 resultType，改造订单 pojo 类，此 pojo 类中包括了订单信息和用户信息 这样返回对象的时候，mybatis 自动把用户信息也注入进来了 pojo 类OrderUser 类继承类包括了 Order 类的所有字段，只需要定义用户的信息字段即可 public class OrderUser extends Order &#123; private String username; private String address;&#125; Mapper.xml&lt;!-- 查询订单，同时包含用户数据 --&gt;&lt;select id=\"queryOrderUser\" resultType=\"orderUser\"&gt; SELECT o.id, o.user_id, userId, o.number, o.createtime, o.note, u.username, u.address FROM `order` o LEFT JOIN `user` u ON o.user_id = u.id&lt;/select&gt; 使用 resultMap使用 resultMap，定义专门的 resultMap 用于映射一对一查询结果 pojo 类在 Order 类中加入 User 属性，user 属性中用于存储关联查询的用户信息，因为订单关联查询用户是一对一关系，所以这里使用单个 User 对象存储关联查询的用户信息 public class Order &#123; private int id; // 订单id private Integer userId; // 用户id private String number; // 订单号 private Date createTime; // 订单创建时间 private String note; // 备注 private User user; // 用户信息&#125; Mapper.xml&lt;resultMap type=\"order\" id=\"orderUserResultMap\"&gt; &lt;id property=\"id\" column=\"id\" /&gt; &lt;result property=\"userId\" column=\"user_id\" /&gt; &lt;result property=\"number\" column=\"number\" /&gt; &lt;result property=\"createTime\" column=\"createtime\" /&gt; &lt;result property=\"note\" column=\"note\" /&gt; &lt;!-- association ：配置一对一属性 --&gt; &lt;!-- property:order里面的User属性名 --&gt; &lt;!-- javaType:属性类型 --&gt; &lt;association property=\"user\" javaType=\"user\"&gt; &lt;!-- id:声明主键，表示user_id是关联查询对象的唯一标识--&gt; &lt;id property=\"id\" column=\"user_id\" /&gt; &lt;result property=\"username\" column=\"username\" /&gt; &lt;result property=\"address\" column=\"address\" /&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;!-- 一对一关联，查询订单，订单内部包含用户属性 --&gt;&lt;select id=\"queryOrderUserResultMap\" resultMap=\"orderUserResultMap\"&gt; SELECT o.id, o.user_id, o.number, o.createtime, o.note, u.username, u.address FROM `orders` o LEFT JOIN `user` u ON o.user_id = u.id&lt;/select&gt; 一对多查询案例：查询所有用户信息及用户关联的订单信息。 用户信息和订单信息为一对多关系。 pojo 类public class User &#123; private int id; private String username; // 用户姓名 private String sex; // 性别 private Date birthday; // 生日 private String address; // 地址 private List&lt;Order&gt; orders;&#125; Mapper.xml&lt;resultMap type=\"user\" id=\"userOrderResultMap\"&gt; &lt;id property=\"id\" column=\"id\" /&gt; &lt;result property=\"username\" column=\"username\" /&gt; &lt;result property=\"birthday\" column=\"birthday\" /&gt; &lt;result property=\"sex\" column=\"sex\" /&gt; &lt;result property=\"address\" column=\"address\" /&gt; &lt;!-- 配置一对多的关系 --&gt; &lt;collection property=\"orders\" javaType=\"list\" ofType=\"order\"&gt; &lt;!-- 配置主键，是关联Order的唯一标识 --&gt; &lt;id property=\"id\" column=\"oid\" /&gt; &lt;result property=\"number\" column=\"number\" /&gt; &lt;result property=\"createtime\" column=\"createtime\" /&gt; &lt;result property=\"note\" column=\"note\" /&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;!-- 一对多关联，查询订单同时查询该用户下的订单 --&gt;&lt;select id=\"queryUserOrder\" resultMap=\"userOrderResultMap\"&gt; SELECT u.id, u.username, u.birthday, u.sex, u.address, o.id oid, o.number, o.createtime, o.note FROM `user` u LEFT JOIN `order` o ON u.id = o.user_id&lt;/select&gt;","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"https://yapengren.github.io/tags/Mybatis/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"Mybatis","slug":"框架/Mybatis","permalink":"https://yapengren.github.io/categories/框架/Mybatis/"}]},{"title":"synchronized 和 lock 有什么区别？用新的lock有什么好处？举例说说","date":"2019-08-04T06:32:36.000Z","path":"wiki/synchronized-和-lock-有什么区别？用新的lock有什么好处？举例说说/","text":"原始构成 synchronized 是关键字，属于JVM层面 monitorenter（底层是通过 monitor 对象来完成，其实 wait/notify 等方法也依赖于 monitor 对象只有在同步块或者方法中才能调用 wait/notify 等方法） monitorexit Lock 是具体类（java.util.concurrent.locks.lock）是 api 层面的锁 使用方法 synchronized 不需要用户去手动释放锁，当 synchronized 代码执行完后系统会自动让线程释放对锁的占用。 ReentrantLock 则需要用户去手动释放锁，若没有主动释放锁，就有可能导致出现死锁现象。需要 lock() 和unlock() 方法配合 try/finally 语句块来完成。 等待是否可中断 synchronized 不可中断，除非抛出异常或者正常运行完成。 ReentrantLock可中断 1.设置超时方法 tryLock(long timeout,TimeUnit unit) 2.lockInterruptibly() 放代码块中，调用 interrupt() 方法可中断 加锁是否公平 synchronized 非公平锁 ReentrantLock 两者都可以，默认非公平锁，构造方法可以传入 boolean 值，true为公平锁，false为非公平锁 锁绑定多个条件 Condition synchronized 没有 ReentrantLock 用来实现分组唤醒需要唤醒的线程，可以精确唤醒，而不是像 synchronized 要么随机唤醒一个要么唤醒全部线程。 举例：锁绑定多个条件 Condition/** * * 题目：多线程之间按顺序调用，实现A-&gt;B-&gt;C三个线程启动，要求如下： * A打印5次，B打印10次，C打印15次 * 紧接着 * A打印5次，B打印10次，C打印15次 * 打印10轮 */public class SyncAndReentrantLockdemo &#123; public static void main(String[] args) &#123; ShareResource shareResource = new ShareResource(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; shareResource.print5(); &#125; &#125;, \"A\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; shareResource.print10(); &#125; &#125;, \"B\").start(); new Thread(() -&gt; &#123; for (int i = 0; i &lt; 10; i++) &#123; shareResource.print15(); &#125; &#125;, \"C\").start(); &#125;&#125;class ShareResource &#123; private int number = 1; private Lock lock = new ReentrantLock(); private Condition c1 = lock.newCondition(); private Condition c2 = lock.newCondition(); private Condition c3 = lock.newCondition(); public void print5() &#123; lock.lock(); try &#123; // 1判断 while (number != 1) &#123; c1.await(); &#125; // 2干活 for (int i = 0; i &lt; 5; i++) &#123; System.out.println(Thread.currentThread().getName() + \" \" + i); &#125; // 3通知 number = 2; c2.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void print10() &#123; lock.lock(); try &#123; // 1判断 while (number != 2) &#123; c2.await(); &#125; // 2干活 for (int i = 0; i &lt; 10; i++) &#123; System.out.println(Thread.currentThread().getName() + \" \" + i); &#125; // 3通知 number = 3; c3.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void print15() &#123; lock.lock(); try &#123; // 1判断 while (number != 3) &#123; c3.await(); &#125; // 2干活 for (int i = 0; i &lt; 5; i++) &#123; System.out.println(Thread.currentThread().getName() + \" \" + i); &#125; // 3通知 number = 1; c1.signal(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"并发","slug":"java/并发","permalink":"https://yapengren.github.io/categories/java/并发/"}]},{"title":"java hash 原理","date":"2019-08-04T05:23:50.000Z","path":"wiki/java-hash-原理/","text":"","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"源码","slug":"java/源码","permalink":"https://yapengren.github.io/categories/java/源码/"}]},{"title":"让我们同步进阶","date":"2019-08-04T05:18:18.000Z","path":"wiki/让我们同步进阶/","text":"友情链接高志龙的博客 知识点互联网java工程师进阶知识完全扫盲 美团技术团队 廖雪峰的Git教程 spring boot &amp; cloudSpring Cloud 程序猿DD","tags":[],"categories":[]},{"title":"Redis 知识点","date":"2019-08-04T05:12:21.000Z","path":"wiki/Redis-知识点/","text":"Redis用过哪些数据数据，以及Redis底层怎么实现string, hash, list, set, zset Redis缓存穿透，缓存雪崩如何使用Redis来实现分布式锁Redis的并发竞争问题如何解决Redis持久化的几种方式，优缺点是什么，怎么实现的aof 文件太大会怎么样Redis的缓存失效策略 &amp; 最大缓存策略Redis集群，高可用，原理Redis缓存分片Redis的数据淘汰策略","tags":[{"name":"Redis","slug":"Redis","permalink":"https://yapengren.github.io/tags/Redis/"}],"categories":[{"name":"待整理","slug":"待整理","permalink":"https://yapengren.github.io/categories/待整理/"}]},{"title":"Spring 知识点","date":"2019-08-04T05:01:51.000Z","path":"wiki/Spring-知识点/","text":"BeanFactory 和 FactoryBean？Spring IOC 的理解，其初始化过程？BeanFactory 和 ApplicationContext？Spring Bean 的生命周期，如何被管理的？Spring Bean 的加载过程是怎样的？如果要你实现Spring AOP，请问怎么实现？如果要你实现Spring IOC，你会注意哪些问题？Spring 是如何管理事务的，事务管理机制？全面分析 Spring 的编程式事务管理及声明式事务管理Spring 的不同事务传播行为有哪些，干什么用的？Spring 中用到了那些设计模式？Spring MVC 的工作原理？Spring 循环注入的原理？Spring AOP的理解，各个术语，他们是怎么相互工作的？Spring 如何保证 Controller 并发的安全？","tags":[{"name":"spring","slug":"spring","permalink":"https://yapengren.github.io/tags/spring/"}],"categories":[{"name":"待整理","slug":"待整理","permalink":"https://yapengren.github.io/categories/待整理/"}]},{"title":"FastDFS流程图","date":"2019-08-04T03:34:10.000Z","path":"wiki/FastDFS流程图/","text":"FastDFS 执行流程FastDFS 服务端有三个角色：跟踪服务器（Tracker Server）、存储服务器（Storage Server）和客户端（Client）。 Tracker Server：跟踪服务器，主要做调度工作，起负载均衡的作用。在内存记录集群中所有存储组和存储服务器的状态信息，是客户端和数据服务器交互的枢纽。相比 GFS 中的 Master 更为精简，不记录文件索引信息，占用的内存量很少。 Storage Server：存储服务器（又称存储节点或数据服务器），文件和文件属性（Meta Data）都保存到存储服务器上。Storage Server 直接利用 OS 的文件系统调用管理文件。 Client：客户端，作为业务请求的发起方，通过专有接口，使用 TCP/IP 协议与跟踪器服务器或存储节点进行数据交互。FastDFS 向使用者提供基本文件访问接口，如 upload、download、append、delete 等，以客户端库的方式提供给用户使用。 通过一张图来看一下 FastDFS 的运行机制： Tracker 相当于 FastDFS 的大脑，不论是上传还是下载都是通过 Tracker 来分配资源；客户端一般可以使用 Ngnix 等静态服务器来调用或者做一部分的缓存；存储服务器内部分为卷（或者叫做组），卷与卷之间是平行的关系，可以根据资源的使用情况随时增加，卷内服务器文件相互同步备份，以达到容灾的目的。 上传机制首先客户端请求 Tracker 服务获取到存储服务器的 IP 地址和端口，然后客户端根据返回的 IP 地址和端口号请求上传文件，存储服务器接收到请求后生产文件，并且将文件内容写入磁盘并返回给客户端 file_id、路径信息、文件名等信息，客户端保存相关信息上传完毕。 下载机制客户端带上文件名信息请求 Tracker 服务获取到存储服务器的 IP 地址和端口，然后客户端根据返回的 IP 地址和端口号请求下载文件，存储服务器接收到请求后返回文件给客户端。","tags":[{"name":"FastDFS","slug":"FastDFS","permalink":"https://yapengren.github.io/tags/FastDFS/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"FastDFS","slug":"框架/FastDFS","permalink":"https://yapengren.github.io/categories/框架/FastDFS/"}]},{"title":"springMVC 流程图","date":"2019-08-04T03:30:24.000Z","path":"wiki/springMVC-流程图/","text":"springMVC执行流程 用户发送请求至前端控制器 DispatcherServlet 前端控制器 DispatcherServlet 收到请求调用处理器映射器HandlerMapping。 处理器映射器 HandlerMapping 根据请求 url 找到具体的处理器，生成处理器对象及处理器拦截器一并返回给前端控制器DispatcherServlet。 前端控制器 DispatcherServlet 通过处理器适配器 HandlerAdapter 调用处理器 执行处理器，执行业务逻辑(Controller，也叫后端控制器)。 Controller 执行完成返回 ModelAndView 处理器适配器 HandlerAdapter 将 controller 执行结果 ModelAndView 返回给前端控制器 DispatcherServlet 前端控制器 DispatcherServlet 将 ModelAndView 传给视图解析器 ViewResolver 视图解析器 ViewResolver 解析后返回具体 View DispatcherServlet 对 View 进行渲染视图（即将模型数据填充至视图中）。 DispatcherServlet 响应用户","tags":[{"name":"spring","slug":"spring","permalink":"https://yapengren.github.io/tags/spring/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"spring","slug":"框架/spring","permalink":"https://yapengren.github.io/categories/框架/spring/"}]},{"title":"shiro 流程图","date":"2019-08-04T03:28:49.000Z","path":"wiki/shiro-流程图/","text":"","tags":[{"name":"shiro","slug":"shiro","permalink":"https://yapengren.github.io/tags/shiro/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"shiro","slug":"框架/shiro","permalink":"https://yapengren.github.io/categories/框架/shiro/"}]},{"title":"Dubbo架构","date":"2019-08-04T03:15:00.000Z","path":"wiki/Dubbo架构/","text":"节点角色说明 节点 角色说明 Provider 暴露服务的服务提供方 Consumer 调用远程服务的服务消费方 Registry 服务注册与发现的注册中心 Monitor 统计服务的调用次数和调用时间的监控中心 Container 服务运行容器 调用关系说明 服务容器负责启动，加载，运行服务提供者。 服务提供者在启动时，向注册中心注册自己提供的服务。 服务消费者在启动时，向注册中心订阅自己所需的服务。 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 参考http://dubbo.apache.org/zh-cn/docs/user/preface/architecture.html","tags":[{"name":"Dubbo","slug":"Dubbo","permalink":"https://yapengren.github.io/tags/Dubbo/"}],"categories":[{"name":"框架","slug":"框架","permalink":"https://yapengren.github.io/categories/框架/"},{"name":"Dubbo","slug":"框架/Dubbo","permalink":"https://yapengren.github.io/categories/框架/Dubbo/"}]},{"title":"Java 基础知识点","date":"2019-08-03T10:01:30.000Z","path":"wiki/Java-基础知识点/","text":"List 和 Set 的区别HashSet 是如何保证不重复的HashMap 是线程安全的吗，为什么不是线程安全的（最好画图说明多线程环境下不安全）HashMap 的扩容过程HashMap 1.7 与 1.8 的 区别，说明 1.8 做了哪些优化，如何优化的？final finally finalize强引用 、软引用、 弱引用、虚引用Java反射Arrays.sort 实现原理和 Collection 实现原理LinkedHashMap的应用cloneable接口实现原理异常分类以及处理机制wait和sleep的区别数组在内存中如何分配","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"待整理","slug":"待整理","permalink":"https://yapengren.github.io/categories/待整理/"}]},{"title":"MySQL知识点","date":"2019-08-03T09:36:37.000Z","path":"wiki/MySQL知识点/","text":"SQLMySQL 中FIND_IN_SET()和IN区别简析MySQL Explain详解MySQL中concat以及group_concat的使用MySQL中order by语句对null字段的排序SQL练习MySQL经典练习题及答案，常用SQL语句练习50题","tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://yapengren.github.io/tags/MySQL/"}],"categories":[{"name":"待整理","slug":"待整理","permalink":"https://yapengren.github.io/categories/待整理/"}]},{"title":"intellij idea修改记录","date":"2019-08-02T10:13:00.000Z","path":"wiki/intellij idea修改记录/","text":"过滤文件 Ignore files and folders *.classpath;*.gitignore;*.hprof;*.idea;*.iml;*.pyc;*.pyo;*.rbc;*.yarb;*~;.DS_Store;.git;.hg;.mvn;.svn;CVS;__pycache__;_svn;logs;mvnw;mvnw.cmd;target;vssver.scc;vssver2.scc; 显示多 Tabs 关闭拖动功能","tags":[{"name":"util","slug":"util","permalink":"https://yapengren.github.io/tags/util/"}],"categories":[{"name":"工具","slug":"工具","permalink":"https://yapengren.github.io/categories/工具/"},{"name":"IDE","slug":"工具/IDE","permalink":"https://yapengren.github.io/categories/工具/IDE/"}]},{"title":"mac brew 服务","date":"2019-08-02T09:40:48.000Z","path":"wiki/mac-brew-服务/","text":"brew 常用命令# brew 搜索软件$ brew search nginx# brew 安装软件$ brew install nginx# brew 卸载软件$ brew uninstall nginx# brew 升级$ sudo brew update# 查看安装信息$ sudo brew info nginx # 查看已安装软件$ brew list brew 清除locksbrew install 如果没有取消自动更新，每次都要更新好久，如果这时候意外按下control+z 再次执行 brew install 会提示 Error: Another active Homebrew update process is already in progress.Please wait for it to finish or terminate it to continue. 解决办法： rm -rf /usr/local/var/homebrew/locks brew 安装nginx# 安装$ sudo brew install nginx# 启动$ sudo brew services start nginx# 重启$ sudo brew services restart nginx# 停止$ sudo brew services stop nginx# 查看$ cat /usr/local/etc/nginx/nignx.conf brew redis# 启动$ redis-server /usr/local/etc/redis-6379.conf# 停止$ redis-cli -p 6379 shutdown brew 安装 activemq# 安装$ brew install activemq# 查看版本$ activemq —version# 启动$ activemq start# 地址# http://localhost:8161 admin admin brew 安装 rabbitmq# 安装$ brew install rabbitmq# 启动$ brew services start rabbitmq# 地址# http://localhost:15672 guest guest brew 安装 mongodb参考：https://blog.csdn.net/ligh_sqh/article/details/81112428 # mongo# 启动 mongodb 客户端$ cd ~/software/adminmongo$ npm start# 客户端地址 http://localhost:1234 nacos 启动命令$ sh ～/software/nacos/bin/startup.sh -m standalone elasticsearch-head 安装和启动 -elasticsearch GUI客户端$ git clone git://github.com/mobz/elasticsearch-head.git$ cd elasticsearch-head$ npm install$ npm run start# http://localhost:9100","tags":[{"name":"mac","slug":"mac","permalink":"https://yapengren.github.io/tags/mac/"}],"categories":[{"name":"系统","slug":"系统","permalink":"https://yapengren.github.io/categories/系统/"},{"name":"mac","slug":"系统/mac","permalink":"https://yapengren.github.io/categories/系统/mac/"}]},{"title":"token 防表单重复提交","date":"2019-08-02T03:16:10.000Z","path":"wiki/token-防表单重复提交/","text":"生成 token加载页面时候调用 initToken() 方法，生成 token 存入页面隐藏域中 &lt;%--隐藏域-防止表单的重复提交--%&gt;&lt;input type=&quot;hidden&quot; name=&quot;examToken&quot; id=&quot;examToken&quot;&gt;&lt;/input&gt; 表单提交表单提交的时候带上 token 方法添加注解springMVC 方法上添加注解 @TokenCheck(isCheckToken = true) 自定义注解自定义注解 TokenCheck.java // 注解会在class字节码文件中存在，在运行时可以通过反射获取到@Retention(RetentionPolicy.RUNTIME) // 定义注解的作用目标**作用范围字段、枚举的常量/方法@Target(&#123;ElementType.METHOD&#125;)// 说明该注解将被包含在javadoc中@Documentedpublic @interface TokenCheck &#123; /** * 是否需要校验 */ boolean isCheckToken() default false;&#125; 自定义注解拦截器springMVC 配置文件 spring-servlet.xml 添加 &lt;mvc:interceptors 拦截器 &lt;mvc:interceptors&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=\"/**\" /&gt; &lt;bean class=\"com.ycx.exam.web.comm.interceptor.TokenCheckDuplicateSubmitInterceptor\"&gt;&lt;/bean&gt; &lt;/mvc:interceptor&gt;&lt;/mvc:interceptors&gt; 拦截器内容TokenCheckDuplicateSubmitInterceptor.java 在拦截器中判断TokenCheck注解isCheckToken是否为 true，如果为 true，则执行缓存校验； 先从 redis 缓存中获取到 token 集合，再从缓存中查询 token； 如果存在，则属于重复提交，返回； 如果不存在，则属于首次提交，将此token压入token集合中并将token集合放回redis中； public class TokenCheckDuplicateSubmitInterceptor extends HandlerInterceptorAdapter &#123; private static final Logger log = Logger.getLogger(TokenCheckDuplicateSubmitInterceptor.class); @Override public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123;&#125; @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception &#123;&#125; @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) &#123; WebApplicationContext context = WebApplicationContextUtils.getRequiredWebApplicationContext(request.getServletContext()); CacheService cacheService = (CacheService) context.getBean(\"cacheService\"); if (cacheService == null) &#123; log.warn(\"token拦截器校验重复提交，缓存service为空!\"); return true; &#125; if (handler instanceof HandlerMethod) &#123; HandlerMethod handlerMethod = (HandlerMethod) handler; Method method = handlerMethod.getMethod(); TokenCheck annotation = method.getAnnotation(TokenCheck.class); if (annotation != null) &#123; boolean isCheckToken = annotation.isCheckToken(); if (isCheckToken) &#123; boolean isExist = true; boolean isgetLock = getLock(cacheService, \"examTokenLock\", 1); if (isgetLock) &#123; isExist = checkTokenExist(request, cacheService); if (!isExist) &#123; log.warn(\"token拦截重复提交校验\" + method.getName() + \"重复提交!\"); &#125; &#125; cacheService.releaseLock(\"examTokenLock\"); return isExist; &#125; &#125; &#125; else &#123; try &#123; return super.preHandle(request, response, handler); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; return true; &#125; /** * @param cacheService * @param cacheKey * @param second 秒 * @return */ private boolean getLock(CacheService cacheService, String cacheKey, long second) &#123; boolean isgetLock = cacheService.getLock(cacheKey, \"1\", second); //如果没有获得锁，将默认进行三次锁的获取 if (!isgetLock) &#123; log.info(\"获得分布式锁失败，进入等待...........\"); for (int i = 0; i &lt;= 2; i++) &#123; isgetLock = cacheService.getLock(cacheKey, \"1\", second); try &#123; if (!isgetLock) &#123; Thread.sleep(1000); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; if (isgetLock) &#123; break; &#125; &#125; &#125; return isgetLock; &#125; /** * 缓存校验 * * @param request * @param cacheService * @return */ private boolean checkTokenExist(HttpServletRequest request, CacheService cacheService) &#123; String token = request.getParameter(\"examToken\"); if (StringUtils.isEmpty(token)) &#123; log.warn(\"token拦截器校验重复提交，页面提交过来token为空!\"); return true; &#125; token = token.trim(); LRULinkedHashMap&lt;String, String&gt; cmap = (LRULinkedHashMap&lt;String, String&gt;) cacheService.getCacheData(\"examTokenMap\"); if (cmap == null) &#123; int size = 131072; // 最近最少使用算法，linkedHashMap实现，主要是针对缓存过期策略实现 cmap = new LRULinkedHashMap&lt;String, String&gt;(size); &#125; else &#123; log.info(\"token缓存map存在，size=\" + cmap.size()); &#125; String ieExist = cmap.get(token); if (StringUtils.isEmpty(ieExist)) &#123; log.info(\"缓存不存在token=\" + token); String valu = cmap.put(token, \"1\"); cacheService.setCacheDataForType(\"examTokenMap\", cmap, 1, TimeUnit.HOURS); log.info(\"将token=\" + token + \"加入缓存成功!\"); return true; &#125; else &#123; log.info(\"将token=\" + token + \"已经存在，重复提交!\"); return false; &#125; &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/tags/java/"}],"categories":[{"name":"java","slug":"java","permalink":"https://yapengren.github.io/categories/java/"},{"name":"方案","slug":"java/方案","permalink":"https://yapengren.github.io/categories/java/方案/"}]}]}